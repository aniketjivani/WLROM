{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074be34a-048a-410d-8fff-8db0c51b785c",
   "metadata": {},
   "source": [
    "Using 120 runs from CR2192, train a CNN accepting $k$ timesteps of a simulation over $k$ channels to predict the Arrival Time of the CME (simulations only). The goal is to test if using the full image (simulation or observation) instead of summary statistics is as good a predictor of arrival times or arrival time error. Extensions? \n",
    "\n",
    "- Other quantities at 1 au\n",
    "\n",
    "- Parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834edb7c-d6a9-4e61-881e-a383f1ffbb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajivani/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158ff1d1-63f0-4282-becc-e70d49087784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b328d414-e56d-4718-8e9e-ea006c551e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAHnRFWHRUaXRsZQBTT0hPIExBU0NPIEMzX3IgY29sb3JtYXDMCW5EAAAAJHRFWHREZXNjcmlwdGlvbgBTT0hPIExBU0NPIEMzX3IgY29sb3JtYXAGtYcrAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My43LjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcme6yd4QAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ5RvQTcAAAHFSURBVHic7dZBbsIwFEDB397/zKSLQisR3DQtEos3s0EksWNjCb23bdu2mZnL58dcv+6+L6/P7fo8dfzq+VePX847/xt/ff35c5jb9T+Ov3v/dnn8/P3n7J57vP/VfLO7vxi/mGd2z51b/3a5n+fc+vf7OFj/4T7mx+eOz+Hg/UfncPL3++/5v3r9vz7/68Eu1z8/3z+aZ+Z371nN8z3+3Lpv99fjD9b99b/xx3Xv/r/OrfvV+z9/fs/d/3r8uXW/DwCQIwAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABH0A+Gm+nfb+AtsAAAAASUVORK5CYII=",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>SOHO LASCO C3_r</strong> </div><div class=\"cmap\"><img alt=\"SOHO LASCO C3_r colormap\" title=\"SOHO LASCO C3_r\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAHnRFWHRUaXRsZQBTT0hPIExBU0NPIEMzX3IgY29sb3JtYXDMCW5EAAAAJHRFWHREZXNjcmlwdGlvbgBTT0hPIExBU0NPIEMzX3IgY29sb3JtYXAGtYcrAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My43LjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcme6yd4QAAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ5RvQTcAAAHFSURBVHic7dZBbsIwFEDB397/zKSLQisR3DQtEos3s0EksWNjCb23bdu2mZnL58dcv+6+L6/P7fo8dfzq+VePX847/xt/ff35c5jb9T+Ov3v/dnn8/P3n7J57vP/VfLO7vxi/mGd2z51b/3a5n+fc+vf7OFj/4T7mx+eOz+Hg/UfncPL3++/5v3r9vz7/68Eu1z8/3z+aZ+Z371nN8z3+3Lpv99fjD9b99b/xx3Xv/r/OrfvV+z9/fs/d/3r8uXW/DwCQIwAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABH0A+Gm+nfb+AtsAAAAASUVORK5CYII=\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#ffffffff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #ffffffff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#000000ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #000000ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.LinearSegmentedColormap at 0x7f8332580760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sunpy.visualization import colormaps as cm\n",
    "\n",
    "lc3_reg = cm.cmlist['soholasco3']\n",
    "lc3_reg\n",
    "\n",
    "lc3 = cm.cmlist['soholasco3'].reversed()\n",
    "lc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832b9372-f4dd-4cf8-b594-eeed4fd31020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "from random import SystemRandom\n",
    "\n",
    "from rich.progress import track\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cnn_utils as cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eed930a-01de-48cc-86ec-7416cf86c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849373fb-2020-4627-ab5c-7c68af928baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bace7614-dfb3-445f-a523-3f3ac56438ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de55ec3-3646-4e40-a7fa-354e1092f7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b46d72-ede2-4ca8-b7ff-fa069ab04e8e",
   "metadata": {},
   "source": [
    "Filter out runs which arrive very late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a16fca-d716-4810-9eca-5b2e2b64db28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SIMID</th>\n",
       "      <th>shift</th>\n",
       "      <th>Tshock_sim</th>\n",
       "      <th>Tshock_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>116</td>\n",
       "      <td>7.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>117</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>118</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>119</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>120</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SIMID  shift  Tshock_sim  Tshock_obs\n",
       "0        1  -18.0        70.0          52\n",
       "2        3  -15.0        67.0          52\n",
       "3        4   -7.0        59.0          52\n",
       "4        5  -22.0        74.0          52\n",
       "6        7   -4.0        56.0          52\n",
       "..     ...    ...         ...         ...\n",
       "114    116    7.0        45.0          52\n",
       "115    117   19.0        33.0          52\n",
       "116    118   16.0        36.0          52\n",
       "117    119    5.0        47.0          52\n",
       "118    120   20.0        32.0          52\n",
       "\n",
       "[113 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_times_data = pd.read_csv(\"./data_surrogate/UShift_CME3_120runs.csv\")\n",
    "arr_times_data = arr_times_data.dropna()\n",
    "\n",
    "arr_times_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d429455-cdd9-4df3-ae1f-0e58651ab975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sim_ids = arr_times_data['SIMID'].to_numpy()\n",
    "data_arr_time = arr_times_data['Tshock_sim'].to_numpy()\n",
    "data_shifts = arr_times_data['shift'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "017f5dd7-4933-4a9f-a0ed-1c11394ee206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split sim_ids, and arr_time into train and test.\n",
    "# Based on this load param file for sims and index that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c40190-5f17-4608-a322-9d0d3f391a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "num_sim = len(data_sim_ids)\n",
    "print(num_sim)\n",
    "\n",
    "# train_size = np.int64(num_sim * 0.6)\n",
    "# valid_size = np.int64(num_sim * 0.2)\n",
    "# test_size = num_sim - train_size - valid_size\n",
    "# print(train_size, valid_size, test_size)\n",
    "\n",
    "train_frac, valid_frac, test_frac = 0.6, 0.2, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe6c9e7-4a70-4886-b779-6a02784a069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(2024)\n",
    "train_valid_test = random_split(data_sim_ids, \n",
    "                                [train_frac, valid_frac, test_frac],\n",
    "                                # [train_size, valid_size, test_size],\n",
    "                                generator=generator1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39759b81-9aed-4f7a-aa7c-49e102418501",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.array(train_valid_test[0].indices)\n",
    "valid_idx = np.array(train_valid_test[1].indices)\n",
    "test_idx = np.array(train_valid_test[2].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86819306-a278-4e11-995d-8a6d6e967b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68,), (23,), (22,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx.shape, valid_idx.shape,test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5f881a-5234-417e-8216-6759c2b36a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idx = np.concatenate((train_idx, valid_idx, test_idx), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25abe788-a67b-478a-a163-d09323b9d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 112)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_idx.min(), all_idx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a872d1fb-c8ac-4fc8-b689-f02c102feea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70., 67., 59., 74., 56., 88., 59., 85., 73., 86., 89., 52., 53.,\n",
       "       78., 69., 67., 55., 61., 54., 59., 74., 71., 67., 51., 73., 56.,\n",
       "       63., 51., 77., 62., 63., 72., 51., 49., 50., 74., 46., 71., 55.,\n",
       "       61., 55., 67., 46., 65., 69., 56., 54., 51., 53., 64., 49., 44.,\n",
       "       56., 50., 55., 47., 54., 73., 46., 43., 53., 66., 44., 38., 45.,\n",
       "       54., 46., 46., 58., 50., 53., 47., 56., 43., 52., 39., 50., 63.,\n",
       "       55., 40., 41., 56., 44., 38., 47., 62., 41., 40., 47., 38., 58.,\n",
       "       39., 44., 37., 45., 49., 42., 39., 47., 35., 42., 33., 40., 40.,\n",
       "       41., 36., 40., 41., 45., 33., 36., 47., 32.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_arr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72a673d9-4b77-46fb-a174-f88bdfcb9f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7,  43,  63,  33,  38,  73,  66,  20,  46,  80,  32,  21,  52,\n",
       "        91, 101, 105,  35, 109,  19,  61,  82,  72,  92,  81,  90,   9,\n",
       "        85,  39,  70,  40,  22,  45,  95,  24,  12,  97,   5,  67,  84,\n",
       "        74, 110,  30,  71, 100,  37,  98, 102,  94,  57,  53,  75,  83,\n",
       "       107,  42,  68,  69,  15,  59,  26,  10, 103, 112,  14,  56,  29,\n",
       "       111,  87,  49])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ad93c84-5ca0-4b39-9a1e-e6607953e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sims = data_sim_ids[train_idx] - 1\n",
    "val_sims = data_sim_ids[valid_idx] - 1\n",
    "test_sims = data_sim_ids[test_idx] - 1\n",
    "\n",
    "train_arr = data_arr_time[train_idx]\n",
    "val_arr = data_arr_time[valid_idx]\n",
    "test_arr = data_arr_time[test_idx]\n",
    "\n",
    "pt = PowerTransformer()\n",
    "pt.fit(train_arr.reshape(-1, 1))\n",
    "\n",
    "train_arr_pt = pt.transform(train_arr.reshape(-1, 1))\n",
    "val_arr_pt = pt.transform(val_arr.reshape(-1, 1))\n",
    "test_arr_pt = pt.transform(test_arr.reshape(-1, 1))\n",
    "\n",
    "train_shifts = data_shifts[train_idx]\n",
    "val_shifts = data_shifts[valid_idx]\n",
    "test_shifts = data_shifts[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78582947-f1b3-4ad1-9efe-0ffbef9329d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7., 19., 21., 18., 16., 10.,  9.,  8.,  1.,  4.]),\n",
       " array([32. , 37.7, 43.4, 49.1, 54.8, 60.5, 66.2, 71.9, 77.6, 83.3, 89. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_arr_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ac6e44-9080-434a-a16f-b4c4430251f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.,  4.,  9.,  8.,  9., 11.,  8.,  6.,  6.,  4.]),\n",
       " array([-2.06459814, -1.65771784, -1.25083753, -0.84395722, -0.43707691,\n",
       "        -0.0301966 ,  0.37668371,  0.78356402,  1.19044432,  1.59732463,\n",
       "         2.00420494]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_arr_pt.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16288dcb-5487-4f3c-8a6b-cc4930f97e66",
   "metadata": {},
   "source": [
    "### Load and Resize data (128 x 512) to (32 x 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6ce47bc-a465-43b9-924f-2e250a75106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datafile = np.load(\"./data_surrogate/CR2192_120runs_C3_ratio_PolarTensor.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be264240-e6db-498f-9636-35e7239f3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 512, 90, 120)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datafile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1723701-bf5c-44c3-987c-0ca64cf2ba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.799298967364635, 1.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_min, raw_max = raw_datafile.min(), raw_datafile.max()\n",
    "raw_min, raw_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad9f480-e302-4364-afc1-37aec43eb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = raw_datafile[:, :, :, train_idx]\n",
    "val_data_raw = raw_datafile[:, :, :, valid_idx]\n",
    "test_data_raw = raw_datafile[:, :, :, test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53fc6192-0e55-4f5b-b756-2a451619dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_dims = (16, 64)\n",
    "\n",
    "train_data_resized = T.Resize(size=resize_dims,\n",
    "                                  antialias=True\n",
    "                                  )(torch.Tensor(train_data_raw.transpose(3, 2, 0, 1)))\n",
    "    \n",
    "val_data_resized = T.Resize(size=resize_dims,\n",
    "                            antialias=True\n",
    "                            )(torch.Tensor(val_data_raw.transpose(3, 2, 0, 1)))\n",
    "\n",
    "test_data_resized = T.Resize(size=resize_dims,\n",
    "                            antialias=True\n",
    "                            )(torch.Tensor(test_data_raw.transpose(3, 2, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e45200d-abf2-4166-80bb-4086ad80cc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.5000), tensor(0.8966))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train = torch.max(train_data_resized)\n",
    "min_train = torch.min(train_data_resized)\n",
    "\n",
    "max_train, min_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e6c5a5b-d49b-452c-9c61-e4fd9de27b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " tensor([0.0000, 0.0256, 0.0513, 0.0769, 0.1026, 0.1282, 0.1538, 0.1795, 0.2051,\n",
       "         0.2308, 0.2564, 0.2821, 0.3077, 0.3333, 0.3590, 0.3846, 0.4103, 0.4359,\n",
       "         0.4615, 0.4872, 0.5128, 0.5385, 0.5641, 0.5897, 0.6154, 0.6410, 0.6667,\n",
       "         0.6923, 0.7179, 0.7436, 0.7692, 0.7949, 0.8205, 0.8462, 0.8718, 0.8974,\n",
       "         0.9231, 0.9487, 0.9744, 1.0000]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_timesteps = np.linspace(2, 180, 90, dtype=int)\n",
    "tMinIdx = 50 # hardcoding for now, change later to accomodate variable length sequences\n",
    "tMaxIdx = len(all_timesteps) - 1\n",
    "nTimesteps = tMaxIdx - tMinIdx + 1\n",
    "\n",
    "\n",
    "\n",
    "# scale time appropriately\n",
    "tt = np.linspace(0, 1, tMaxIdx - tMinIdx + 1)\n",
    "tpredict = torch.Tensor(tt).to(device)\n",
    "\n",
    "nTimesteps, tpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e65282-361f-4c9a-a36b-d514203aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (train_data_resized[:, tMinIdx:(tMaxIdx + 1), :, :] - min_train)/(max_train - min_train)\n",
    "val_data = (val_data_resized[:, tMinIdx:(tMaxIdx + 1), :, :] - min_train)/(max_train - min_train)\n",
    "test_data = (test_data_resized[:, tMinIdx:(tMaxIdx + 1), :, :] - min_train)/(max_train - min_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d45ab255-4654-498b-9159-45b35d2e8b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([68, 40, 16, 64]),\n",
       " torch.Size([23, 40, 16, 64]),\n",
       " torch.Size([22, 40, 16, 64]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8325501-7a1c-4cdf-90d5-a0964c116955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(1.), tensor(1.))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.max(), val_data.max(), test_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c57ac-8219-408d-8238-cfae9fbd16c2",
   "metadata": {},
   "source": [
    "### Concatenate parameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9c7aa3b-4a66-460f-bd77-fdc112cd44c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BStrength</th>\n",
       "      <th>OrientationCme</th>\n",
       "      <th>ApexHeight</th>\n",
       "      <th>iHelicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0417</td>\n",
       "      <td>347.475</td>\n",
       "      <td>0.8505</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1250</td>\n",
       "      <td>330.225</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.7917</td>\n",
       "      <td>380.475</td>\n",
       "      <td>0.7482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3750</td>\n",
       "      <td>342.225</td>\n",
       "      <td>0.9123</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.2917</td>\n",
       "      <td>322.725</td>\n",
       "      <td>0.8719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>14.6250</td>\n",
       "      <td>300.225</td>\n",
       "      <td>0.6911</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>14.7083</td>\n",
       "      <td>371.475</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>14.7917</td>\n",
       "      <td>348.225</td>\n",
       "      <td>0.7743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>14.8750</td>\n",
       "      <td>312.975</td>\n",
       "      <td>0.7363</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>14.9583</td>\n",
       "      <td>340.725</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BStrength  OrientationCme  ApexHeight  iHelicity\n",
       "0       5.0417         347.475      0.8505         -1\n",
       "1       5.1250         330.225      0.7196          1\n",
       "2       6.7917         380.475      0.7482          1\n",
       "3       5.3750         342.225      0.9123         -1\n",
       "4       5.2917         322.725      0.8719          1\n",
       "..         ...             ...         ...        ...\n",
       "115    14.6250         300.225      0.6911          1\n",
       "116    14.7083         371.475      0.9337          1\n",
       "117    14.7917         348.225      0.7743          1\n",
       "118    14.8750         312.975      0.7363         -1\n",
       "119    14.9583         340.725      0.8933         -1\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_params = pd.read_csv(\"./data_surrogate/params_2192_120runs.csv\")\n",
    "raw_params = raw_params[[\"BStrength\", \"OrientationCme\", \"ApexHeight\", \"iHelicity\"]]\n",
    "raw_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66b045b0-8f44-49cb-9e03-f3549a29d1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BStrength</th>\n",
       "      <th>OrientationCme</th>\n",
       "      <th>ApexHeight</th>\n",
       "      <th>iHelicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.0417</td>\n",
       "      <td>294.975</td>\n",
       "      <td>0.6673</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.9583</td>\n",
       "      <td>384.225</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BStrength  OrientationCme  ApexHeight  iHelicity\n",
       "min     5.0417         294.975      0.6673         -1\n",
       "max    14.9583         384.225      0.9504          1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_params.agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37181c64-1836-4bcd-bf11-141770a1830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_params_np = raw_params.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57bab397-b719-4c1c-ab2c-88a0e58d226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_scaling = [15, 384, 0.95, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a603f-9676-4b00-a1f9-c4abcc967780",
   "metadata": {},
   "source": [
    "### Train-test split and normalization\n",
    "\n",
    "Use train_idx, valid_idx and test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42017e50-035a-4da6-863d-3e71d760f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params_raw = raw_params_np[train_idx, :] / param_scaling\n",
    "valid_params_raw = raw_params_np[valid_idx, :] / param_scaling\n",
    "test_params_raw = raw_params_np[test_idx, :] / param_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d38dce56-44d6-467e-830d-51bc92fa9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snaps = train_data.reshape((train_data.shape[0], train_data.shape[1], -1))\n",
    "val_snaps = val_data.reshape((val_data.shape[0], val_data.shape[1], -1))\n",
    "test_snaps = test_data.reshape((test_data.shape[0], test_data.shape[1], -1))\n",
    "\n",
    "# repeat parameter values up to number of timesteps\n",
    "train_params_ts = np.repeat(train_params_raw.reshape((train_params_raw.shape[0], train_params_raw.shape[1], 1)), nTimesteps, axis=2)\n",
    "val_params_ts = np.repeat(valid_params_raw.reshape((valid_params_raw.shape[0], valid_params_raw.shape[1], 1)), nTimesteps, axis=2)\n",
    "test_params_ts = np.repeat(test_params_raw.reshape((test_params_raw.shape[0], test_params_raw.shape[1], 1)), nTimesteps, axis=2)\n",
    "\n",
    "# concatenate snaps and params\n",
    "train_all = torch.cat((train_snaps, torch.Tensor(np.transpose(train_params_ts, (0, 2, 1)))), 2)\n",
    "val_all = torch.cat((val_snaps, torch.Tensor(np.transpose(val_params_ts, (0, 2, 1)))), 2)\n",
    "test_all = torch.cat((test_snaps, torch.Tensor(np.transpose(test_params_ts, (0, 2, 1)))), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc109883-7ffb-46ff-b8cc-d33e6d42539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([68, 40, 1024]), torch.Size([23, 40, 1024]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snaps.shape, val_snaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa16c8bc-99ff-467c-ab67-0cda9368d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr_ts = torch.Tensor(train_arr_pt.flatten())\n",
    "val_arr_ts = torch.Tensor(val_arr_pt.flatten())\n",
    "test_arr_ts = torch.Tensor(test_arr_pt.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03ba43c6-4d8f-483a-bb20-1050e21ec6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists\n",
    "train_dataset = [(train_all[i:(i+1), :, :], tpredict, train_arr_ts[i]) for i in range(train_all.shape[0])]\n",
    "val_dataset = [(val_all[i:(i+1), :, :], tpredict, val_arr_ts[i]) for i in range(val_all.shape[0])]\n",
    "test_dataset = [(test_all[i:(i+1), :, :], tpredict, test_arr_ts[i]) for i in range(test_all.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90d28b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 40, 1028]), torch.Size([1, 40, 1028]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0].shape, val_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "858d8cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 23, 22)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5477489d-45a4-4900-95af-096e43db8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_generator(iterable):\n",
    "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
    "        for i, (x, y) in enumerate(inf_generator(train_loader)):\n",
    "    \"\"\"\n",
    "    iterator = iterable.__iter__()\n",
    "    while True:\n",
    "        try:\n",
    "            yield iterator.__next__()\n",
    "        except StopIteration:\n",
    "            iterator = iterable.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1598f22c-557f-4fd3-8244-4033dbfec6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(dataloader, device=torch.device(\"cuda:0\")):\n",
    "    return dataloader.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e247046-2886-4e0b-a284-5354fab7600f",
   "metadata": {},
   "source": [
    "### Coordinate concats? \n",
    "\n",
    "https://arxiv.org/pdf/1807.03247\n",
    "Also see `coordconv_debug.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c7283-f8fc-4742-ae10-e97baa44199c",
   "metadata": {},
   "source": [
    "### Dataloaders (modify to select batch time and batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4c4eed2-452f-4db9-a857-f8f5296e030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc23685c-1939-4e77-9632-a39ac7736d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timesteps = np.linspace(2, 180, 90, dtype=int)\n",
    "tMinIdx = 46 # hardcoding for now, change later to accomodate variable length sequences\n",
    "tMaxIdx = len(all_timesteps) - 1\n",
    "nTimesteps = tMaxIdx - tMinIdx + 1\n",
    "tMaxVal = 180 # we have data up to 180 min\n",
    "time_subset = all_timesteps[tMinIdx:(tMaxIdx + 1)]\n",
    "tt = time_subset / tMaxVal\n",
    "tpredict = torch.Tensor(tt).to(device)\n",
    "\n",
    "steps_to_use = 5\n",
    "starts_to_use = 3\n",
    "\n",
    "valid_indices = list(range(nTimesteps - steps_to_use))\n",
    "\n",
    "start_idx_all = sorted(random.sample(valid_indices, starts_to_use))\n",
    "\n",
    "end_idx_all = []\n",
    "for idx in start_idx_all:\n",
    "    end_idx = min(idx + steps_to_use + 1, nTimesteps)\n",
    "    end_idx_all.append(end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9901c7a-a5a4-4c38-aa06-12822714a8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 11, 17], [10, 17, 23])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx_all, end_idx_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6934750-2d1b-4634-9091-360488d8f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_wl3(batch, \n",
    "                   # start_idx_all, \n",
    "                   # end_idx_all, \n",
    "                   device,\n",
    "                   steps_to_use=3,\n",
    "                   starts_to_use=3):\n",
    "    \"\"\"\n",
    "    Return dictionary containing:\n",
    "    Snapshots of observed data (based on start_idx_all, end_idx_all)\n",
    "    These should be of the form b x t x nx x ny (time dim is \n",
    "    channel dim, use k timesteps for k input channels)\n",
    "    Corresponding arrival times\n",
    "    \"\"\"\n",
    "    all_timesteps = np.linspace(2, 180, 90, dtype=int)\n",
    "    tMinIdx = 46 # hardcoding for now, change later to accomodate variable length sequences\n",
    "    tMaxIdx = len(all_timesteps) - 1\n",
    "    nTimesteps = tMaxIdx - tMinIdx + 1\n",
    "    tMaxVal = 180 # we have data up to 180 min\n",
    "    time_subset = all_timesteps[tMinIdx:(tMaxIdx + 1)]\n",
    "    tt = time_subset / tMaxVal\n",
    "    tpredict = torch.Tensor(tt).to(device)\n",
    "\n",
    "    \n",
    "    # starts_to_use = 4\n",
    "\n",
    "    valid_indices = list(range(nTimesteps - steps_to_use))\n",
    "\n",
    "    start_idx_all = sorted(random.sample(valid_indices, starts_to_use))\n",
    "\n",
    "    end_idx_all = []\n",
    "    for idx in start_idx_all:\n",
    "        end_idx = min(idx + steps_to_use - 1, nTimesteps)\n",
    "        end_idx_all.append(end_idx)\n",
    "    \n",
    "    all_data_dicts = []\n",
    "    nICs = len(start_idx_all)\n",
    "    for i, st, ed in zip(range(nICs), start_idx_all, end_idx_all):\n",
    "        data = torch.zeros([len(batch), ed - st + 1, batch[0][0].shape[2]]).to(device)\n",
    "        target = torch.zeros([len(batch), 1]).to(device)\n",
    "        \n",
    "        for b, (snap, ts, tarr) in enumerate(batch):\n",
    "            data[b, :, :] = snap[:, st:(ed + 1), :]\n",
    "            target[b, :] = tarr\n",
    "            \n",
    "        data_dict = {\"observed_data\": data,\n",
    "                     \"target\": target,\n",
    "                     \"start_idx\":st,\n",
    "                     \"end_idx\":ed}\n",
    "        \n",
    "        \n",
    "        all_data_dicts.append(data_dict)\n",
    "        \n",
    "        \n",
    "    return all_data_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5834f571-b856-44c5-9526-a41f8f636d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = 4,\n",
    "                              shuffle=True, \n",
    "                              collate_fn = lambda batch: collate_fn_wl3(batch, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2dd98fd3-95fd-4222-b492-a9e9ac50715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, \n",
    "                              batch_size = 4,\n",
    "                              shuffle=True, \n",
    "                              collate_fn = lambda batch: collate_fn_wl3(batch, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4deadca-1744-4243-8b45-9950c6d7a49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03b869e6-eb12-41de-b95c-b88592c55cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, \n",
    "                              batch_size = len(test_dataset),\n",
    "                              shuffle=False, \n",
    "                              collate_fn = lambda batch: collate_fn_wl3(batch, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e9a9d50-9e3b-4915-ab66-f1ceb9e0cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 64 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee2f2206-aa16-454a-9d9f-494466e76c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1028])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah1 = train_dataloader.__iter__().__next__()\n",
    "blah1[0][\"observed_data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d78ecce-a4e8-4c90-a26c-eebb7f7fe67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah[0][\"target\"], blah[1][\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c0ebf-53af-45bb-9e6a-0a1427da8904",
   "metadata": {},
   "source": [
    "### Model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cfbddbb-6af5-441a-975b-4939b98ff2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model? One hidden layer, non-linear activations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32dc3391-90ee-40a1-99fa-542d0441b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae(model, data_loader):\n",
    "    SAE, n_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        all_dicts = get_next_batch(inf_generator(data_loader))\n",
    "        for batch in all_dicts: \n",
    "            images = batch['observed_data'].to(device)\n",
    "            true_arrival = batch['target'].reshape(-1,1).to(device)\n",
    "            pred_arrival = model.predict(images)\n",
    "            SAE += torch.sum(torch.abs(pred_arrival - true_arrival))\n",
    "            n_examples += true_arrival.size(0)\n",
    "        return SAE/n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e03384b3-f836-4312-8bbf-623fd467f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_hidden_1,\n",
    "                  n_hidden_2):\n",
    "        super(WLCNN, self).__init__()\n",
    "        in_channels = 3\n",
    "\n",
    "        self.features = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=3, padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            # nn.ELU(),\n",
    "            # nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1),\n",
    "        )\n",
    "\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            nn.Linear(1280, n_hidden_1),\n",
    "            nn.BatchNorm1d(n_hidden_1),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            nn.BatchNorm1d(n_hidden_2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(n_hidden_2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ip = x[:, :, :1024].reshape(-1, 3, 16, 64)\n",
    "\n",
    "        \n",
    "        x_ft = self.features(x_ip)\n",
    "        \n",
    "        x_ft = torch.flatten(x_ft, 1)\n",
    "        xpred = self.regressor(x_ft)\n",
    "        return xpred\n",
    "\n",
    "    def predict(self, x):\n",
    "        xpred = self(x)\n",
    "        return xpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07ca46eb-441b-43d5-90f1-4e77f2095b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah = torch.rand(4, 5, 4100)\n",
    "blah = torch.rand(4, 3, 16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30eaa59d-3c2f-4bfd-8efa-53f29643f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blah[:, :, :4096].reshape(-1, 5, 32, 128).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69145a7c-2a36-4113-a554-76d99ab0e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WLCNN(n_hidden_1=16, \n",
    "              n_hidden_2=8, \n",
    "              ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0174d018-a7e7-4001-9235-c16fec552745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3561],\n",
       "        [-0.0046],\n",
       "        [-0.0244],\n",
       "        [-0.4803]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11603cc6-4e6e-4dcc-b058-a0b469abf691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fts_blah = model.features(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc551c48-7208-4692-953e-ab6b44c790be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHC(Hin, Win, padding=(1, 1), dilation=(1, 1), kernel_size=(3, 3), stride=(1, 1)):\n",
    "    \"\"\"\n",
    "    Calculate Hout and Wout when using `nn.Conv2d` on an image. Supply Hin, Win, padding, dilation,\n",
    "    kernel_size and stride to calculate.\n",
    "    \"\"\"\n",
    "    Hout = np.floor(((Hin + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1)/stride[0]) + 1)\n",
    "    Wout = np.floor(((Win + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1)/stride[1]) + 1)\n",
    "    return Hout, Wout\n",
    "\n",
    "\n",
    "def getHCTransposed(Hin, Win, \n",
    "                    padding=(1, 1), \n",
    "                    dilation=(1, 1), \n",
    "                    kernel_size=(3, 3), \n",
    "                    stride=(1, 1),\n",
    "                    output_padding=(0, 0)):\n",
    "    \"\"\"\n",
    "    Calculate Hout and Wout when using `nn.ConvTranspose2d` on an image. Supply Hin, Win, padding, dilation,\n",
    "    kernel_size and stride to calculate. Also supply output padding but we will keep it at zero so optional.\n",
    "    \"\"\"\n",
    "\n",
    "    Hout = (Hin - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "    Wout = (Win - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "    return Hout, Wout\n",
    "\n",
    "\n",
    "def getHCAdaptivePool(Hin, Win, padding=(1, 1), dilation=(1, 1), kernel_size=(3, 3), stride=(1, 1)):\n",
    "    \"\"\"\n",
    "    Calculate Hout and Wout when using `nn.AdaptiveAvgPool` on an image. Supply Hin, Win, padding, dilation,\n",
    "    kernel_size and stride to calculate.\n",
    "    \"\"\"\n",
    "    Hout = np.floor(((Hin + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1)/stride[0]) + 1)\n",
    "    Wout = np.floor(((Win + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1)/stride[1]) + 1)\n",
    "    return Hout, Wout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48430755-47d3-4550-8b54-d3936d95ef3d",
   "metadata": {},
   "source": [
    "### Continuous Model (NODE-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9639e10-9a71-408f-9437-56af9d64935b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9725eeb-ac3e-45e8-b221-0ddc13a3a43e",
   "metadata": {},
   "source": [
    "### Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25a7b7b6-ce4d-4858-8965-e64625e2cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "062c0590-de09-474b-9149-effb96eba809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 001/1000 | Batch 000/017 | Cost: 2.8426\n",
      "Iter: 001/1000 | Batch 002/017 | Cost: 0.5523\n",
      "Train Loss: 1.111 | Val Loss: 0.524\n",
      "Iter: 002/1000 | Batch 000/017 | Cost: 3.3510\n",
      "Iter: 002/1000 | Batch 002/017 | Cost: 4.6350\n",
      "Train Loss: 0.792 | Val Loss: 0.869\n",
      "Iter: 003/1000 | Batch 000/017 | Cost: 4.1885\n",
      "Iter: 003/1000 | Batch 002/017 | Cost: 2.4593\n",
      "Train Loss: 0.932 | Val Loss: 0.746\n",
      "Iter: 004/1000 | Batch 000/017 | Cost: 2.3017\n",
      "Iter: 004/1000 | Batch 002/017 | Cost: 1.9235\n",
      "Train Loss: 0.797 | Val Loss: 0.566\n",
      "Iter: 005/1000 | Batch 000/017 | Cost: 3.1959\n",
      "Iter: 005/1000 | Batch 002/017 | Cost: 4.4026\n",
      "Train Loss: 1.116 | Val Loss: 1.013\n",
      "Iter: 006/1000 | Batch 000/017 | Cost: 1.9027\n",
      "Iter: 006/1000 | Batch 002/017 | Cost: 1.9373\n",
      "Train Loss: 0.696 | Val Loss: 0.534\n",
      "Iter: 007/1000 | Batch 000/017 | Cost: 3.5901\n",
      "Iter: 007/1000 | Batch 002/017 | Cost: 4.9749\n",
      "Train Loss: 0.608 | Val Loss: 1.013\n",
      "Iter: 008/1000 | Batch 000/017 | Cost: 3.4981\n",
      "Iter: 008/1000 | Batch 002/017 | Cost: 3.0480\n",
      "Train Loss: 1.340 | Val Loss: 0.561\n",
      "Iter: 009/1000 | Batch 000/017 | Cost: 3.4573\n",
      "Iter: 009/1000 | Batch 002/017 | Cost: 4.1765\n",
      "Train Loss: 0.643 | Val Loss: 0.611\n",
      "Iter: 010/1000 | Batch 000/017 | Cost: 5.2068\n",
      "Iter: 010/1000 | Batch 002/017 | Cost: 5.1838\n",
      "Train Loss: 0.615 | Val Loss: 0.599\n",
      "Iter: 011/1000 | Batch 000/017 | Cost: 4.4909\n",
      "Iter: 011/1000 | Batch 002/017 | Cost: 5.9888\n",
      "Train Loss: 1.067 | Val Loss: 0.639\n",
      "Iter: 012/1000 | Batch 000/017 | Cost: 3.6398\n",
      "Iter: 012/1000 | Batch 002/017 | Cost: 3.9873\n",
      "Train Loss: 0.594 | Val Loss: 0.833\n",
      "Iter: 013/1000 | Batch 000/017 | Cost: 3.5439\n",
      "Iter: 013/1000 | Batch 002/017 | Cost: 3.2545\n",
      "Train Loss: 0.802 | Val Loss: 1.092\n",
      "Iter: 014/1000 | Batch 000/017 | Cost: 2.9935\n",
      "Iter: 014/1000 | Batch 002/017 | Cost: 3.3993\n",
      "Train Loss: 0.704 | Val Loss: 0.361\n",
      "Iter: 015/1000 | Batch 000/017 | Cost: 1.9985\n",
      "Iter: 015/1000 | Batch 002/017 | Cost: 1.6258\n",
      "Train Loss: 0.734 | Val Loss: 0.671\n",
      "Iter: 016/1000 | Batch 000/017 | Cost: 3.7682\n",
      "Iter: 016/1000 | Batch 002/017 | Cost: 4.7890\n",
      "Train Loss: 0.406 | Val Loss: 0.431\n",
      "Iter: 017/1000 | Batch 000/017 | Cost: 4.0659\n",
      "Iter: 017/1000 | Batch 002/017 | Cost: 3.6570\n",
      "Train Loss: 0.690 | Val Loss: 0.860\n",
      "Iter: 018/1000 | Batch 000/017 | Cost: 3.0106\n",
      "Iter: 018/1000 | Batch 002/017 | Cost: 3.9753\n",
      "Train Loss: 0.611 | Val Loss: 0.572\n",
      "Iter: 019/1000 | Batch 000/017 | Cost: 3.2968\n",
      "Iter: 019/1000 | Batch 002/017 | Cost: 2.6648\n",
      "Train Loss: 0.306 | Val Loss: 0.547\n",
      "Iter: 020/1000 | Batch 000/017 | Cost: 4.5079\n",
      "Iter: 020/1000 | Batch 002/017 | Cost: 3.0593\n",
      "Train Loss: 0.938 | Val Loss: 0.786\n",
      "Iter: 021/1000 | Batch 000/017 | Cost: 1.3412\n",
      "Iter: 021/1000 | Batch 002/017 | Cost: 2.5769\n",
      "Train Loss: 0.343 | Val Loss: 0.512\n",
      "Iter: 022/1000 | Batch 000/017 | Cost: 0.9060\n",
      "Iter: 022/1000 | Batch 002/017 | Cost: 1.9217\n",
      "Train Loss: 0.409 | Val Loss: 0.536\n",
      "Iter: 023/1000 | Batch 000/017 | Cost: 0.9913\n",
      "Iter: 023/1000 | Batch 002/017 | Cost: 1.8266\n",
      "Train Loss: 0.688 | Val Loss: 0.577\n",
      "Iter: 024/1000 | Batch 000/017 | Cost: 4.1555\n",
      "Iter: 024/1000 | Batch 002/017 | Cost: 3.3993\n",
      "Train Loss: 0.468 | Val Loss: 0.560\n",
      "Iter: 025/1000 | Batch 000/017 | Cost: 4.6789\n",
      "Iter: 025/1000 | Batch 002/017 | Cost: 4.1995\n",
      "Train Loss: 1.033 | Val Loss: 0.663\n",
      "Iter: 026/1000 | Batch 000/017 | Cost: 4.0909\n",
      "Iter: 026/1000 | Batch 002/017 | Cost: 2.5951\n",
      "Train Loss: 0.493 | Val Loss: 0.587\n",
      "Iter: 027/1000 | Batch 000/017 | Cost: 5.1468\n",
      "Iter: 027/1000 | Batch 002/017 | Cost: 5.2028\n",
      "Train Loss: 1.251 | Val Loss: 0.764\n",
      "Iter: 028/1000 | Batch 000/017 | Cost: 2.1773\n",
      "Iter: 028/1000 | Batch 002/017 | Cost: 2.4044\n",
      "Train Loss: 0.425 | Val Loss: 0.542\n",
      "Iter: 029/1000 | Batch 000/017 | Cost: 2.2032\n",
      "Iter: 029/1000 | Batch 002/017 | Cost: 2.6802\n",
      "Train Loss: 0.600 | Val Loss: 0.465\n",
      "Iter: 030/1000 | Batch 000/017 | Cost: 1.9255\n",
      "Iter: 030/1000 | Batch 002/017 | Cost: 1.2171\n",
      "Train Loss: 0.493 | Val Loss: 0.789\n",
      "Iter: 031/1000 | Batch 000/017 | Cost: 2.7136\n",
      "Iter: 031/1000 | Batch 002/017 | Cost: 3.4558\n",
      "Train Loss: 0.748 | Val Loss: 0.720\n",
      "Iter: 032/1000 | Batch 000/017 | Cost: 2.6376\n",
      "Iter: 032/1000 | Batch 002/017 | Cost: 2.5568\n",
      "Train Loss: 0.482 | Val Loss: 0.470\n",
      "Iter: 033/1000 | Batch 000/017 | Cost: 2.8109\n",
      "Iter: 033/1000 | Batch 002/017 | Cost: 3.0373\n",
      "Train Loss: 0.859 | Val Loss: 0.536\n",
      "Iter: 034/1000 | Batch 000/017 | Cost: 3.3129\n",
      "Iter: 034/1000 | Batch 002/017 | Cost: 3.9684\n",
      "Train Loss: 0.767 | Val Loss: 0.641\n",
      "Iter: 035/1000 | Batch 000/017 | Cost: 4.1051\n",
      "Iter: 035/1000 | Batch 002/017 | Cost: 2.6987\n",
      "Train Loss: 0.733 | Val Loss: 0.752\n",
      "Iter: 036/1000 | Batch 000/017 | Cost: 2.6741\n",
      "Iter: 036/1000 | Batch 002/017 | Cost: 1.3382\n",
      "Train Loss: 0.473 | Val Loss: 0.737\n",
      "Iter: 037/1000 | Batch 000/017 | Cost: 1.5489\n",
      "Iter: 037/1000 | Batch 002/017 | Cost: 2.8624\n",
      "Train Loss: 0.386 | Val Loss: 0.461\n",
      "Iter: 038/1000 | Batch 000/017 | Cost: 4.8983\n",
      "Iter: 038/1000 | Batch 002/017 | Cost: 3.8108\n",
      "Train Loss: 0.225 | Val Loss: 0.424\n",
      "Iter: 039/1000 | Batch 000/017 | Cost: 2.1670\n",
      "Iter: 039/1000 | Batch 002/017 | Cost: 1.9244\n",
      "Train Loss: 1.055 | Val Loss: 0.794\n",
      "Iter: 040/1000 | Batch 000/017 | Cost: 2.7978\n",
      "Iter: 040/1000 | Batch 002/017 | Cost: 2.7969\n",
      "Train Loss: 0.910 | Val Loss: 0.615\n",
      "Iter: 041/1000 | Batch 000/017 | Cost: 2.6291\n",
      "Iter: 041/1000 | Batch 002/017 | Cost: 3.5873\n",
      "Train Loss: 0.805 | Val Loss: 0.911\n",
      "Iter: 042/1000 | Batch 000/017 | Cost: 3.6451\n",
      "Iter: 042/1000 | Batch 002/017 | Cost: 4.1400\n",
      "Train Loss: 0.671 | Val Loss: 0.880\n",
      "Iter: 043/1000 | Batch 000/017 | Cost: 3.6109\n",
      "Iter: 043/1000 | Batch 002/017 | Cost: 2.9859\n",
      "Train Loss: 0.656 | Val Loss: 0.549\n",
      "Iter: 044/1000 | Batch 000/017 | Cost: 3.5604\n",
      "Iter: 044/1000 | Batch 002/017 | Cost: 3.8216\n",
      "Train Loss: 0.866 | Val Loss: 0.325\n",
      "Iter: 045/1000 | Batch 000/017 | Cost: 3.9412\n",
      "Iter: 045/1000 | Batch 002/017 | Cost: 3.1591\n",
      "Train Loss: 0.524 | Val Loss: 0.948\n",
      "Iter: 046/1000 | Batch 000/017 | Cost: 2.0879\n",
      "Iter: 046/1000 | Batch 002/017 | Cost: 2.3957\n",
      "Train Loss: 0.706 | Val Loss: 0.597\n",
      "Iter: 047/1000 | Batch 000/017 | Cost: 3.9772\n",
      "Iter: 047/1000 | Batch 002/017 | Cost: 3.4948\n",
      "Train Loss: 0.359 | Val Loss: 0.319\n",
      "Iter: 048/1000 | Batch 000/017 | Cost: 2.9287\n",
      "Iter: 048/1000 | Batch 002/017 | Cost: 2.6474\n",
      "Train Loss: 1.001 | Val Loss: 0.694\n",
      "Iter: 049/1000 | Batch 000/017 | Cost: 2.4182\n",
      "Iter: 049/1000 | Batch 002/017 | Cost: 3.3170\n",
      "Train Loss: 0.516 | Val Loss: 0.534\n",
      "Iter: 050/1000 | Batch 000/017 | Cost: 2.4671\n",
      "Iter: 050/1000 | Batch 002/017 | Cost: 2.7363\n",
      "Train Loss: 0.591 | Val Loss: 0.441\n",
      "Iter: 051/1000 | Batch 000/017 | Cost: 3.0776\n",
      "Iter: 051/1000 | Batch 002/017 | Cost: 3.0952\n",
      "Train Loss: 0.764 | Val Loss: 0.915\n",
      "Iter: 052/1000 | Batch 000/017 | Cost: 2.3239\n",
      "Iter: 052/1000 | Batch 002/017 | Cost: 1.0894\n",
      "Train Loss: 0.412 | Val Loss: 0.728\n",
      "Iter: 053/1000 | Batch 000/017 | Cost: 2.4408\n",
      "Iter: 053/1000 | Batch 002/017 | Cost: 2.5099\n",
      "Train Loss: 0.712 | Val Loss: 0.640\n",
      "Iter: 054/1000 | Batch 000/017 | Cost: 3.1870\n",
      "Iter: 054/1000 | Batch 002/017 | Cost: 3.0490\n",
      "Train Loss: 0.481 | Val Loss: 0.425\n",
      "Iter: 055/1000 | Batch 000/017 | Cost: 2.9617\n",
      "Iter: 055/1000 | Batch 002/017 | Cost: 4.0570\n",
      "Train Loss: 0.702 | Val Loss: 0.454\n",
      "Iter: 056/1000 | Batch 000/017 | Cost: 1.5647\n",
      "Iter: 056/1000 | Batch 002/017 | Cost: 2.3248\n",
      "Train Loss: 0.832 | Val Loss: 0.493\n",
      "Iter: 057/1000 | Batch 000/017 | Cost: 2.6739\n",
      "Iter: 057/1000 | Batch 002/017 | Cost: 2.8650\n",
      "Train Loss: 0.814 | Val Loss: 0.345\n",
      "Iter: 058/1000 | Batch 000/017 | Cost: 2.0858\n",
      "Iter: 058/1000 | Batch 002/017 | Cost: 1.7516\n",
      "Train Loss: 0.686 | Val Loss: 0.563\n",
      "Iter: 059/1000 | Batch 000/017 | Cost: 2.5159\n",
      "Iter: 059/1000 | Batch 002/017 | Cost: 3.5369\n",
      "Train Loss: 0.587 | Val Loss: 0.686\n",
      "Iter: 060/1000 | Batch 000/017 | Cost: 1.5968\n",
      "Iter: 060/1000 | Batch 002/017 | Cost: 2.2864\n",
      "Train Loss: 0.446 | Val Loss: 1.005\n",
      "Iter: 061/1000 | Batch 000/017 | Cost: 2.9456\n",
      "Iter: 061/1000 | Batch 002/017 | Cost: 3.0045\n",
      "Train Loss: 0.717 | Val Loss: 1.059\n",
      "Iter: 062/1000 | Batch 000/017 | Cost: 3.4163\n",
      "Iter: 062/1000 | Batch 002/017 | Cost: 4.2668\n",
      "Train Loss: 0.424 | Val Loss: 0.293\n",
      "Iter: 063/1000 | Batch 000/017 | Cost: 2.4567\n",
      "Iter: 063/1000 | Batch 002/017 | Cost: 1.8839\n",
      "Train Loss: 0.654 | Val Loss: 0.443\n",
      "Iter: 064/1000 | Batch 000/017 | Cost: 1.7345\n",
      "Iter: 064/1000 | Batch 002/017 | Cost: 1.6638\n",
      "Train Loss: 0.665 | Val Loss: 0.570\n",
      "Iter: 065/1000 | Batch 000/017 | Cost: 3.5171\n",
      "Iter: 065/1000 | Batch 002/017 | Cost: 4.2083\n",
      "Train Loss: 0.970 | Val Loss: 0.257\n",
      "Iter: 066/1000 | Batch 000/017 | Cost: 2.7758\n",
      "Iter: 066/1000 | Batch 002/017 | Cost: 3.2219\n",
      "Train Loss: 0.418 | Val Loss: 0.449\n",
      "Iter: 067/1000 | Batch 000/017 | Cost: 3.0589\n",
      "Iter: 067/1000 | Batch 002/017 | Cost: 2.1534\n",
      "Train Loss: 0.791 | Val Loss: 0.546\n",
      "Iter: 068/1000 | Batch 000/017 | Cost: 1.9312\n",
      "Iter: 068/1000 | Batch 002/017 | Cost: 2.3026\n",
      "Train Loss: 0.511 | Val Loss: 0.679\n",
      "Iter: 069/1000 | Batch 000/017 | Cost: 3.0504\n",
      "Iter: 069/1000 | Batch 002/017 | Cost: 2.6730\n",
      "Train Loss: 0.646 | Val Loss: 0.617\n",
      "Iter: 070/1000 | Batch 000/017 | Cost: 1.5327\n",
      "Iter: 070/1000 | Batch 002/017 | Cost: 0.8077\n",
      "Train Loss: 0.315 | Val Loss: 0.204\n",
      "Iter: 071/1000 | Batch 000/017 | Cost: 3.7585\n",
      "Iter: 071/1000 | Batch 002/017 | Cost: 3.7631\n",
      "Train Loss: 0.699 | Val Loss: 0.316\n",
      "Iter: 072/1000 | Batch 000/017 | Cost: 4.2140\n",
      "Iter: 072/1000 | Batch 002/017 | Cost: 3.9095\n",
      "Train Loss: 0.274 | Val Loss: 0.280\n",
      "Iter: 073/1000 | Batch 000/017 | Cost: 3.3168\n",
      "Iter: 073/1000 | Batch 002/017 | Cost: 2.1020\n",
      "Train Loss: 1.010 | Val Loss: 0.513\n",
      "Iter: 074/1000 | Batch 000/017 | Cost: 4.0364\n",
      "Iter: 074/1000 | Batch 002/017 | Cost: 3.3677\n",
      "Train Loss: 0.401 | Val Loss: 0.374\n",
      "Iter: 075/1000 | Batch 000/017 | Cost: 3.8079\n",
      "Iter: 075/1000 | Batch 002/017 | Cost: 4.0095\n",
      "Train Loss: 0.664 | Val Loss: 0.596\n",
      "Iter: 076/1000 | Batch 000/017 | Cost: 5.1855\n",
      "Iter: 076/1000 | Batch 002/017 | Cost: 3.9847\n",
      "Train Loss: 0.670 | Val Loss: 0.769\n",
      "Iter: 077/1000 | Batch 000/017 | Cost: 1.8491\n",
      "Iter: 077/1000 | Batch 002/017 | Cost: 1.0468\n",
      "Train Loss: 0.261 | Val Loss: 0.456\n",
      "Iter: 078/1000 | Batch 000/017 | Cost: 2.4549\n",
      "Iter: 078/1000 | Batch 002/017 | Cost: 2.6858\n",
      "Train Loss: 0.705 | Val Loss: 0.595\n",
      "Iter: 079/1000 | Batch 000/017 | Cost: 1.6012\n",
      "Iter: 079/1000 | Batch 002/017 | Cost: 1.2851\n",
      "Train Loss: 0.425 | Val Loss: 0.563\n",
      "Iter: 080/1000 | Batch 000/017 | Cost: 2.8333\n",
      "Iter: 080/1000 | Batch 002/017 | Cost: 2.5809\n",
      "Train Loss: 1.083 | Val Loss: 0.315\n",
      "Iter: 081/1000 | Batch 000/017 | Cost: 3.1693\n",
      "Iter: 081/1000 | Batch 002/017 | Cost: 3.7211\n",
      "Train Loss: 0.612 | Val Loss: 0.602\n",
      "Iter: 082/1000 | Batch 000/017 | Cost: 1.8745\n",
      "Iter: 082/1000 | Batch 002/017 | Cost: 2.9466\n",
      "Train Loss: 0.575 | Val Loss: 0.516\n",
      "Iter: 083/1000 | Batch 000/017 | Cost: 4.3591\n",
      "Iter: 083/1000 | Batch 002/017 | Cost: 3.5458\n",
      "Train Loss: 0.388 | Val Loss: 0.549\n",
      "Iter: 084/1000 | Batch 000/017 | Cost: 1.3705\n",
      "Iter: 084/1000 | Batch 002/017 | Cost: 1.5041\n",
      "Train Loss: 0.779 | Val Loss: 0.549\n",
      "Iter: 085/1000 | Batch 000/017 | Cost: 2.0545\n",
      "Iter: 085/1000 | Batch 002/017 | Cost: 4.0548\n",
      "Train Loss: 0.574 | Val Loss: 0.761\n",
      "Iter: 086/1000 | Batch 000/017 | Cost: 3.3516\n",
      "Iter: 086/1000 | Batch 002/017 | Cost: 2.8398\n",
      "Train Loss: 0.815 | Val Loss: 0.327\n",
      "Iter: 087/1000 | Batch 000/017 | Cost: 3.2556\n",
      "Iter: 087/1000 | Batch 002/017 | Cost: 2.4487\n",
      "Train Loss: 0.717 | Val Loss: 0.575\n",
      "Iter: 088/1000 | Batch 000/017 | Cost: 2.9059\n",
      "Iter: 088/1000 | Batch 002/017 | Cost: 2.4221\n",
      "Train Loss: 0.724 | Val Loss: 0.550\n",
      "Iter: 089/1000 | Batch 000/017 | Cost: 4.1074\n",
      "Iter: 089/1000 | Batch 002/017 | Cost: 4.8298\n",
      "Train Loss: 0.444 | Val Loss: 0.565\n",
      "Iter: 090/1000 | Batch 000/017 | Cost: 3.9953\n",
      "Iter: 090/1000 | Batch 002/017 | Cost: 4.0122\n",
      "Train Loss: 0.650 | Val Loss: 0.482\n",
      "Iter: 091/1000 | Batch 000/017 | Cost: 2.5176\n",
      "Iter: 091/1000 | Batch 002/017 | Cost: 2.5241\n",
      "Train Loss: 0.755 | Val Loss: 0.349\n",
      "Iter: 092/1000 | Batch 000/017 | Cost: 3.8485\n",
      "Iter: 092/1000 | Batch 002/017 | Cost: 3.1604\n",
      "Train Loss: 1.025 | Val Loss: 0.798\n",
      "Iter: 093/1000 | Batch 000/017 | Cost: 6.2350\n",
      "Iter: 093/1000 | Batch 002/017 | Cost: 4.3524\n",
      "Train Loss: 0.608 | Val Loss: 0.450\n",
      "Iter: 094/1000 | Batch 000/017 | Cost: 4.3028\n",
      "Iter: 094/1000 | Batch 002/017 | Cost: 3.8395\n",
      "Train Loss: 0.435 | Val Loss: 0.530\n",
      "Iter: 095/1000 | Batch 000/017 | Cost: 4.5400\n",
      "Iter: 095/1000 | Batch 002/017 | Cost: 3.4661\n",
      "Train Loss: 0.482 | Val Loss: 0.552\n",
      "Iter: 096/1000 | Batch 000/017 | Cost: 2.9993\n",
      "Iter: 096/1000 | Batch 002/017 | Cost: 2.8697\n",
      "Train Loss: 0.631 | Val Loss: 0.796\n",
      "Iter: 097/1000 | Batch 000/017 | Cost: 3.8044\n",
      "Iter: 097/1000 | Batch 002/017 | Cost: 2.7930\n",
      "Train Loss: 1.109 | Val Loss: 0.285\n",
      "Iter: 098/1000 | Batch 000/017 | Cost: 3.2549\n",
      "Iter: 098/1000 | Batch 002/017 | Cost: 1.9451\n",
      "Train Loss: 0.819 | Val Loss: 0.283\n",
      "Iter: 099/1000 | Batch 000/017 | Cost: 2.7294\n",
      "Iter: 099/1000 | Batch 002/017 | Cost: 2.8749\n",
      "Train Loss: 0.810 | Val Loss: 0.189\n",
      "Iter: 100/1000 | Batch 000/017 | Cost: 1.9857\n",
      "Iter: 100/1000 | Batch 002/017 | Cost: 2.4681\n",
      "Train Loss: 1.015 | Val Loss: 0.311\n",
      "Iter: 101/1000 | Batch 000/017 | Cost: 2.0603\n",
      "Iter: 101/1000 | Batch 002/017 | Cost: 2.6547\n",
      "Train Loss: 0.493 | Val Loss: 0.874\n",
      "Iter: 102/1000 | Batch 000/017 | Cost: 3.5398\n",
      "Iter: 102/1000 | Batch 002/017 | Cost: 3.0746\n",
      "Train Loss: 0.320 | Val Loss: 0.689\n",
      "Iter: 103/1000 | Batch 000/017 | Cost: 3.7948\n",
      "Iter: 103/1000 | Batch 002/017 | Cost: 5.3957\n",
      "Train Loss: 0.442 | Val Loss: 0.809\n",
      "Iter: 104/1000 | Batch 000/017 | Cost: 2.8886\n",
      "Iter: 104/1000 | Batch 002/017 | Cost: 3.1455\n",
      "Train Loss: 0.505 | Val Loss: 0.528\n",
      "Iter: 105/1000 | Batch 000/017 | Cost: 2.0300\n",
      "Iter: 105/1000 | Batch 002/017 | Cost: 1.6533\n",
      "Train Loss: 0.329 | Val Loss: 0.984\n",
      "Iter: 106/1000 | Batch 000/017 | Cost: 3.7668\n",
      "Iter: 106/1000 | Batch 002/017 | Cost: 1.8463\n",
      "Train Loss: 0.403 | Val Loss: 0.817\n",
      "Iter: 107/1000 | Batch 000/017 | Cost: 4.5895\n",
      "Iter: 107/1000 | Batch 002/017 | Cost: 4.3337\n",
      "Train Loss: 0.495 | Val Loss: 0.706\n",
      "Iter: 108/1000 | Batch 000/017 | Cost: 2.3780\n",
      "Iter: 108/1000 | Batch 002/017 | Cost: 1.5621\n",
      "Train Loss: 0.835 | Val Loss: 0.596\n",
      "Iter: 109/1000 | Batch 000/017 | Cost: 3.0681\n",
      "Iter: 109/1000 | Batch 002/017 | Cost: 2.5185\n",
      "Train Loss: 0.280 | Val Loss: 0.654\n",
      "Iter: 110/1000 | Batch 000/017 | Cost: 3.2913\n",
      "Iter: 110/1000 | Batch 002/017 | Cost: 1.3338\n",
      "Train Loss: 0.434 | Val Loss: 1.071\n",
      "Iter: 111/1000 | Batch 000/017 | Cost: 3.1777\n",
      "Iter: 111/1000 | Batch 002/017 | Cost: 2.8294\n",
      "Train Loss: 1.560 | Val Loss: 1.108\n",
      "Iter: 112/1000 | Batch 000/017 | Cost: 4.1177\n",
      "Iter: 112/1000 | Batch 002/017 | Cost: 3.6885\n",
      "Train Loss: 0.612 | Val Loss: 0.440\n",
      "Iter: 113/1000 | Batch 000/017 | Cost: 5.5119\n",
      "Iter: 113/1000 | Batch 002/017 | Cost: 4.5331\n",
      "Train Loss: 0.540 | Val Loss: 0.493\n",
      "Iter: 114/1000 | Batch 000/017 | Cost: 3.0351\n",
      "Iter: 114/1000 | Batch 002/017 | Cost: 4.1768\n",
      "Train Loss: 0.612 | Val Loss: 0.452\n",
      "Iter: 115/1000 | Batch 000/017 | Cost: 2.1035\n",
      "Iter: 115/1000 | Batch 002/017 | Cost: 1.6992\n",
      "Train Loss: 0.810 | Val Loss: 0.221\n",
      "Iter: 116/1000 | Batch 000/017 | Cost: 3.3407\n",
      "Iter: 116/1000 | Batch 002/017 | Cost: 3.3078\n",
      "Train Loss: 0.689 | Val Loss: 0.414\n",
      "Iter: 117/1000 | Batch 000/017 | Cost: 3.0334\n",
      "Iter: 117/1000 | Batch 002/017 | Cost: 1.5646\n",
      "Train Loss: 0.528 | Val Loss: 0.274\n",
      "Iter: 118/1000 | Batch 000/017 | Cost: 2.1901\n",
      "Iter: 118/1000 | Batch 002/017 | Cost: 1.9559\n",
      "Train Loss: 0.342 | Val Loss: 0.685\n",
      "Iter: 119/1000 | Batch 000/017 | Cost: 2.3221\n",
      "Iter: 119/1000 | Batch 002/017 | Cost: 2.9883\n",
      "Train Loss: 0.569 | Val Loss: 0.642\n",
      "Iter: 120/1000 | Batch 000/017 | Cost: 2.7074\n",
      "Iter: 120/1000 | Batch 002/017 | Cost: 3.2059\n",
      "Train Loss: 0.899 | Val Loss: 0.329\n",
      "Iter: 121/1000 | Batch 000/017 | Cost: 5.5701\n",
      "Iter: 121/1000 | Batch 002/017 | Cost: 3.7965\n",
      "Train Loss: 0.923 | Val Loss: 0.743\n",
      "Iter: 122/1000 | Batch 000/017 | Cost: 0.5727\n",
      "Iter: 122/1000 | Batch 002/017 | Cost: 0.8283\n",
      "Train Loss: 0.815 | Val Loss: 0.289\n",
      "Iter: 123/1000 | Batch 000/017 | Cost: 2.7336\n",
      "Iter: 123/1000 | Batch 002/017 | Cost: 1.7821\n",
      "Train Loss: 0.749 | Val Loss: 0.415\n",
      "Iter: 124/1000 | Batch 000/017 | Cost: 3.7810\n",
      "Iter: 124/1000 | Batch 002/017 | Cost: 3.7390\n",
      "Train Loss: 0.712 | Val Loss: 0.678\n",
      "Iter: 125/1000 | Batch 000/017 | Cost: 3.9711\n",
      "Iter: 125/1000 | Batch 002/017 | Cost: 3.7298\n",
      "Train Loss: 0.568 | Val Loss: 0.415\n",
      "Iter: 126/1000 | Batch 000/017 | Cost: 2.2184\n",
      "Iter: 126/1000 | Batch 002/017 | Cost: 2.1880\n",
      "Train Loss: 0.588 | Val Loss: 0.644\n",
      "Iter: 127/1000 | Batch 000/017 | Cost: 1.6980\n",
      "Iter: 127/1000 | Batch 002/017 | Cost: 1.7399\n",
      "Train Loss: 0.415 | Val Loss: 0.375\n",
      "Iter: 128/1000 | Batch 000/017 | Cost: 2.6859\n",
      "Iter: 128/1000 | Batch 002/017 | Cost: 2.1915\n",
      "Train Loss: 0.490 | Val Loss: 0.504\n",
      "Iter: 129/1000 | Batch 000/017 | Cost: 4.0388\n",
      "Iter: 129/1000 | Batch 002/017 | Cost: 4.0875\n",
      "Train Loss: 0.389 | Val Loss: 0.490\n",
      "Iter: 130/1000 | Batch 000/017 | Cost: 3.2980\n",
      "Iter: 130/1000 | Batch 002/017 | Cost: 2.4419\n",
      "Train Loss: 0.689 | Val Loss: 0.830\n",
      "Iter: 131/1000 | Batch 000/017 | Cost: 2.7434\n",
      "Iter: 131/1000 | Batch 002/017 | Cost: 3.1299\n",
      "Train Loss: 0.417 | Val Loss: 0.590\n",
      "Iter: 132/1000 | Batch 000/017 | Cost: 3.4696\n",
      "Iter: 132/1000 | Batch 002/017 | Cost: 2.9868\n",
      "Train Loss: 0.641 | Val Loss: 0.200\n",
      "Iter: 133/1000 | Batch 000/017 | Cost: 1.8325\n",
      "Iter: 133/1000 | Batch 002/017 | Cost: 2.4997\n",
      "Train Loss: 0.667 | Val Loss: 0.227\n",
      "Iter: 134/1000 | Batch 000/017 | Cost: 2.0383\n",
      "Iter: 134/1000 | Batch 002/017 | Cost: 1.4785\n",
      "Train Loss: 0.927 | Val Loss: 0.358\n",
      "Iter: 135/1000 | Batch 000/017 | Cost: 2.8575\n",
      "Iter: 135/1000 | Batch 002/017 | Cost: 3.5483\n",
      "Train Loss: 0.631 | Val Loss: 0.478\n",
      "Iter: 136/1000 | Batch 000/017 | Cost: 2.2557\n",
      "Iter: 136/1000 | Batch 002/017 | Cost: 2.3211\n",
      "Train Loss: 0.398 | Val Loss: 0.599\n",
      "Iter: 137/1000 | Batch 000/017 | Cost: 1.8612\n",
      "Iter: 137/1000 | Batch 002/017 | Cost: 2.6385\n",
      "Train Loss: 0.654 | Val Loss: 0.549\n",
      "Iter: 138/1000 | Batch 000/017 | Cost: 2.1923\n",
      "Iter: 138/1000 | Batch 002/017 | Cost: 2.5626\n",
      "Train Loss: 0.768 | Val Loss: 0.321\n",
      "Iter: 139/1000 | Batch 000/017 | Cost: 1.9557\n",
      "Iter: 139/1000 | Batch 002/017 | Cost: 2.5101\n",
      "Train Loss: 0.668 | Val Loss: 0.585\n",
      "Iter: 140/1000 | Batch 000/017 | Cost: 1.9884\n",
      "Iter: 140/1000 | Batch 002/017 | Cost: 2.7381\n",
      "Train Loss: 0.698 | Val Loss: 0.447\n",
      "Iter: 141/1000 | Batch 000/017 | Cost: 1.5737\n",
      "Iter: 141/1000 | Batch 002/017 | Cost: 2.5742\n",
      "Train Loss: 0.521 | Val Loss: 0.301\n",
      "Iter: 142/1000 | Batch 000/017 | Cost: 3.7836\n",
      "Iter: 142/1000 | Batch 002/017 | Cost: 1.3640\n",
      "Train Loss: 0.244 | Val Loss: 0.791\n",
      "Iter: 143/1000 | Batch 000/017 | Cost: 3.0492\n",
      "Iter: 143/1000 | Batch 002/017 | Cost: 3.4218\n",
      "Train Loss: 0.603 | Val Loss: 0.227\n",
      "Iter: 144/1000 | Batch 000/017 | Cost: 1.6422\n",
      "Iter: 144/1000 | Batch 002/017 | Cost: 0.8054\n",
      "Train Loss: 0.582 | Val Loss: 0.550\n",
      "Iter: 145/1000 | Batch 000/017 | Cost: 2.1584\n",
      "Iter: 145/1000 | Batch 002/017 | Cost: 3.0313\n",
      "Train Loss: 0.466 | Val Loss: 0.553\n",
      "Iter: 146/1000 | Batch 000/017 | Cost: 4.5241\n",
      "Iter: 146/1000 | Batch 002/017 | Cost: 3.6209\n",
      "Train Loss: 0.664 | Val Loss: 0.758\n",
      "Iter: 147/1000 | Batch 000/017 | Cost: 2.5312\n",
      "Iter: 147/1000 | Batch 002/017 | Cost: 2.1140\n",
      "Train Loss: 0.647 | Val Loss: 0.306\n",
      "Iter: 148/1000 | Batch 000/017 | Cost: 2.8641\n",
      "Iter: 148/1000 | Batch 002/017 | Cost: 2.3326\n",
      "Train Loss: 1.030 | Val Loss: 0.198\n",
      "Iter: 149/1000 | Batch 000/017 | Cost: 2.3554\n",
      "Iter: 149/1000 | Batch 002/017 | Cost: 1.4772\n",
      "Train Loss: 0.541 | Val Loss: 0.667\n",
      "Iter: 150/1000 | Batch 000/017 | Cost: 3.3976\n",
      "Iter: 150/1000 | Batch 002/017 | Cost: 2.9192\n",
      "Train Loss: 0.240 | Val Loss: 0.553\n",
      "Iter: 151/1000 | Batch 000/017 | Cost: 2.8556\n",
      "Iter: 151/1000 | Batch 002/017 | Cost: 2.4855\n",
      "Train Loss: 0.475 | Val Loss: 0.614\n",
      "Iter: 152/1000 | Batch 000/017 | Cost: 2.0762\n",
      "Iter: 152/1000 | Batch 002/017 | Cost: 3.0612\n",
      "Train Loss: 0.490 | Val Loss: 0.388\n",
      "Iter: 153/1000 | Batch 000/017 | Cost: 2.1402\n",
      "Iter: 153/1000 | Batch 002/017 | Cost: 2.4627\n",
      "Train Loss: 0.456 | Val Loss: 0.458\n",
      "Iter: 154/1000 | Batch 000/017 | Cost: 2.2822\n",
      "Iter: 154/1000 | Batch 002/017 | Cost: 2.2011\n",
      "Train Loss: 1.248 | Val Loss: 0.872\n",
      "Iter: 155/1000 | Batch 000/017 | Cost: 3.1933\n",
      "Iter: 155/1000 | Batch 002/017 | Cost: 3.4638\n",
      "Train Loss: 0.896 | Val Loss: 0.740\n",
      "Iter: 156/1000 | Batch 000/017 | Cost: 6.6460\n",
      "Iter: 156/1000 | Batch 002/017 | Cost: 5.1649\n",
      "Train Loss: 1.412 | Val Loss: 0.852\n",
      "Iter: 157/1000 | Batch 000/017 | Cost: 1.1547\n",
      "Iter: 157/1000 | Batch 002/017 | Cost: 1.8152\n",
      "Train Loss: 0.870 | Val Loss: 0.553\n",
      "Iter: 158/1000 | Batch 000/017 | Cost: 4.3481\n",
      "Iter: 158/1000 | Batch 002/017 | Cost: 3.9101\n",
      "Train Loss: 0.439 | Val Loss: 0.695\n",
      "Iter: 159/1000 | Batch 000/017 | Cost: 6.5082\n",
      "Iter: 159/1000 | Batch 002/017 | Cost: 5.2955\n",
      "Train Loss: 0.480 | Val Loss: 0.382\n",
      "Iter: 160/1000 | Batch 000/017 | Cost: 1.4013\n",
      "Iter: 160/1000 | Batch 002/017 | Cost: 1.7426\n",
      "Train Loss: 0.974 | Val Loss: 0.532\n",
      "Iter: 161/1000 | Batch 000/017 | Cost: 2.2798\n",
      "Iter: 161/1000 | Batch 002/017 | Cost: 2.7223\n",
      "Train Loss: 0.705 | Val Loss: 0.510\n",
      "Iter: 162/1000 | Batch 000/017 | Cost: 6.0015\n",
      "Iter: 162/1000 | Batch 002/017 | Cost: 6.4594\n",
      "Train Loss: 0.636 | Val Loss: 0.510\n",
      "Iter: 163/1000 | Batch 000/017 | Cost: 2.8469\n",
      "Iter: 163/1000 | Batch 002/017 | Cost: 2.8899\n",
      "Train Loss: 0.937 | Val Loss: 0.760\n",
      "Iter: 164/1000 | Batch 000/017 | Cost: 3.2515\n",
      "Iter: 164/1000 | Batch 002/017 | Cost: 2.9843\n",
      "Train Loss: 0.484 | Val Loss: 0.498\n",
      "Iter: 165/1000 | Batch 000/017 | Cost: 5.5265\n",
      "Iter: 165/1000 | Batch 002/017 | Cost: 4.4153\n",
      "Train Loss: 0.554 | Val Loss: 0.425\n",
      "Iter: 166/1000 | Batch 000/017 | Cost: 3.9521\n",
      "Iter: 166/1000 | Batch 002/017 | Cost: 3.4718\n",
      "Train Loss: 0.353 | Val Loss: 0.233\n",
      "Iter: 167/1000 | Batch 000/017 | Cost: 2.1434\n",
      "Iter: 167/1000 | Batch 002/017 | Cost: 1.4026\n",
      "Train Loss: 0.709 | Val Loss: 0.445\n",
      "Iter: 168/1000 | Batch 000/017 | Cost: 2.8459\n",
      "Iter: 168/1000 | Batch 002/017 | Cost: 2.0963\n",
      "Train Loss: 0.374 | Val Loss: 0.449\n",
      "Iter: 169/1000 | Batch 000/017 | Cost: 3.3892\n",
      "Iter: 169/1000 | Batch 002/017 | Cost: 2.2635\n",
      "Train Loss: 0.529 | Val Loss: 0.220\n",
      "Iter: 170/1000 | Batch 000/017 | Cost: 1.9970\n",
      "Iter: 170/1000 | Batch 002/017 | Cost: 2.0136\n",
      "Train Loss: 0.629 | Val Loss: 0.233\n",
      "Iter: 171/1000 | Batch 000/017 | Cost: 2.7574\n",
      "Iter: 171/1000 | Batch 002/017 | Cost: 2.6760\n",
      "Train Loss: 0.416 | Val Loss: 0.664\n",
      "Iter: 172/1000 | Batch 000/017 | Cost: 3.2153\n",
      "Iter: 172/1000 | Batch 002/017 | Cost: 4.4985\n",
      "Train Loss: 0.556 | Val Loss: 0.501\n",
      "Iter: 173/1000 | Batch 000/017 | Cost: 3.3276\n",
      "Iter: 173/1000 | Batch 002/017 | Cost: 3.1193\n",
      "Train Loss: 0.575 | Val Loss: 0.858\n",
      "Iter: 174/1000 | Batch 000/017 | Cost: 2.9078\n",
      "Iter: 174/1000 | Batch 002/017 | Cost: 3.3840\n",
      "Train Loss: 0.856 | Val Loss: 0.249\n",
      "Iter: 175/1000 | Batch 000/017 | Cost: 3.2024\n",
      "Iter: 175/1000 | Batch 002/017 | Cost: 3.0662\n",
      "Train Loss: 0.639 | Val Loss: 0.621\n",
      "Iter: 176/1000 | Batch 000/017 | Cost: 2.4292\n",
      "Iter: 176/1000 | Batch 002/017 | Cost: 2.2772\n",
      "Train Loss: 0.589 | Val Loss: 0.398\n",
      "Iter: 177/1000 | Batch 000/017 | Cost: 4.6664\n",
      "Iter: 177/1000 | Batch 002/017 | Cost: 4.3708\n",
      "Train Loss: 0.574 | Val Loss: 0.825\n",
      "Iter: 178/1000 | Batch 000/017 | Cost: 3.3218\n",
      "Iter: 178/1000 | Batch 002/017 | Cost: 4.1821\n",
      "Train Loss: 0.684 | Val Loss: 0.544\n",
      "Iter: 179/1000 | Batch 000/017 | Cost: 4.3492\n",
      "Iter: 179/1000 | Batch 002/017 | Cost: 3.6441\n",
      "Train Loss: 0.475 | Val Loss: 0.668\n",
      "Iter: 180/1000 | Batch 000/017 | Cost: 1.7516\n",
      "Iter: 180/1000 | Batch 002/017 | Cost: 1.5682\n",
      "Train Loss: 0.728 | Val Loss: 0.690\n",
      "Iter: 181/1000 | Batch 000/017 | Cost: 2.3308\n",
      "Iter: 181/1000 | Batch 002/017 | Cost: 1.8450\n",
      "Train Loss: 0.560 | Val Loss: 0.170\n",
      "Iter: 182/1000 | Batch 000/017 | Cost: 2.7208\n",
      "Iter: 182/1000 | Batch 002/017 | Cost: 3.4573\n",
      "Train Loss: 0.364 | Val Loss: 0.710\n",
      "Iter: 183/1000 | Batch 000/017 | Cost: 3.1576\n",
      "Iter: 183/1000 | Batch 002/017 | Cost: 3.4973\n",
      "Train Loss: 0.556 | Val Loss: 0.262\n",
      "Iter: 184/1000 | Batch 000/017 | Cost: 1.7561\n",
      "Iter: 184/1000 | Batch 002/017 | Cost: 1.4696\n",
      "Train Loss: 0.330 | Val Loss: 0.673\n",
      "Iter: 185/1000 | Batch 000/017 | Cost: 1.9012\n",
      "Iter: 185/1000 | Batch 002/017 | Cost: 1.4974\n",
      "Train Loss: 0.342 | Val Loss: 0.788\n",
      "Iter: 186/1000 | Batch 000/017 | Cost: 1.4651\n",
      "Iter: 186/1000 | Batch 002/017 | Cost: 1.5862\n",
      "Train Loss: 0.773 | Val Loss: 0.677\n",
      "Iter: 187/1000 | Batch 000/017 | Cost: 3.7005\n",
      "Iter: 187/1000 | Batch 002/017 | Cost: 4.3446\n",
      "Train Loss: 0.834 | Val Loss: 0.551\n",
      "Iter: 188/1000 | Batch 000/017 | Cost: 3.1422\n",
      "Iter: 188/1000 | Batch 002/017 | Cost: 4.0048\n",
      "Train Loss: 0.844 | Val Loss: 0.559\n",
      "Iter: 189/1000 | Batch 000/017 | Cost: 4.4731\n",
      "Iter: 189/1000 | Batch 002/017 | Cost: 4.2275\n",
      "Train Loss: 0.817 | Val Loss: 0.849\n",
      "Iter: 190/1000 | Batch 000/017 | Cost: 1.5770\n",
      "Iter: 190/1000 | Batch 002/017 | Cost: 1.0631\n",
      "Train Loss: 0.929 | Val Loss: 0.481\n",
      "Iter: 191/1000 | Batch 000/017 | Cost: 4.1655\n",
      "Iter: 191/1000 | Batch 002/017 | Cost: 3.6430\n",
      "Train Loss: 0.535 | Val Loss: 0.802\n",
      "Iter: 192/1000 | Batch 000/017 | Cost: 4.3168\n",
      "Iter: 192/1000 | Batch 002/017 | Cost: 4.6942\n",
      "Train Loss: 1.192 | Val Loss: 1.087\n",
      "Iter: 193/1000 | Batch 000/017 | Cost: 3.5414\n",
      "Iter: 193/1000 | Batch 002/017 | Cost: 3.3105\n",
      "Train Loss: 0.391 | Val Loss: 0.847\n",
      "Iter: 194/1000 | Batch 000/017 | Cost: 1.6418\n",
      "Iter: 194/1000 | Batch 002/017 | Cost: 2.2183\n",
      "Train Loss: 0.613 | Val Loss: 0.621\n",
      "Iter: 195/1000 | Batch 000/017 | Cost: 3.8064\n",
      "Iter: 195/1000 | Batch 002/017 | Cost: 4.1847\n",
      "Train Loss: 0.632 | Val Loss: 1.063\n",
      "Iter: 196/1000 | Batch 000/017 | Cost: 4.3742\n",
      "Iter: 196/1000 | Batch 002/017 | Cost: 3.9632\n",
      "Train Loss: 0.685 | Val Loss: 0.452\n",
      "Iter: 197/1000 | Batch 000/017 | Cost: 1.8023\n",
      "Iter: 197/1000 | Batch 002/017 | Cost: 2.0629\n",
      "Train Loss: 0.472 | Val Loss: 0.315\n",
      "Iter: 198/1000 | Batch 000/017 | Cost: 4.0049\n",
      "Iter: 198/1000 | Batch 002/017 | Cost: 1.6852\n",
      "Train Loss: 0.305 | Val Loss: 0.369\n",
      "Iter: 199/1000 | Batch 000/017 | Cost: 3.1011\n",
      "Iter: 199/1000 | Batch 002/017 | Cost: 2.8866\n",
      "Train Loss: 0.871 | Val Loss: 0.728\n",
      "Iter: 200/1000 | Batch 000/017 | Cost: 1.5813\n",
      "Iter: 200/1000 | Batch 002/017 | Cost: 1.0597\n",
      "Train Loss: 0.457 | Val Loss: 0.432\n",
      "Iter: 201/1000 | Batch 000/017 | Cost: 1.9074\n",
      "Iter: 201/1000 | Batch 002/017 | Cost: 2.7542\n",
      "Train Loss: 0.823 | Val Loss: 0.626\n",
      "Iter: 202/1000 | Batch 000/017 | Cost: 1.8048\n",
      "Iter: 202/1000 | Batch 002/017 | Cost: 1.1483\n",
      "Train Loss: 0.865 | Val Loss: 0.900\n",
      "Iter: 203/1000 | Batch 000/017 | Cost: 2.6691\n",
      "Iter: 203/1000 | Batch 002/017 | Cost: 2.4325\n",
      "Train Loss: 0.782 | Val Loss: 0.215\n",
      "Iter: 204/1000 | Batch 000/017 | Cost: 3.1140\n",
      "Iter: 204/1000 | Batch 002/017 | Cost: 2.8923\n",
      "Train Loss: 0.475 | Val Loss: 0.524\n",
      "Iter: 205/1000 | Batch 000/017 | Cost: 2.2912\n",
      "Iter: 205/1000 | Batch 002/017 | Cost: 2.0929\n",
      "Train Loss: 0.956 | Val Loss: 0.452\n",
      "Iter: 206/1000 | Batch 000/017 | Cost: 3.6542\n",
      "Iter: 206/1000 | Batch 002/017 | Cost: 3.4000\n",
      "Train Loss: 0.572 | Val Loss: 0.310\n",
      "Iter: 207/1000 | Batch 000/017 | Cost: 1.1197\n",
      "Iter: 207/1000 | Batch 002/017 | Cost: 1.5325\n",
      "Train Loss: 0.626 | Val Loss: 0.370\n",
      "Iter: 208/1000 | Batch 000/017 | Cost: 3.7239\n",
      "Iter: 208/1000 | Batch 002/017 | Cost: 3.9060\n",
      "Train Loss: 0.690 | Val Loss: 0.501\n",
      "Iter: 209/1000 | Batch 000/017 | Cost: 2.7819\n",
      "Iter: 209/1000 | Batch 002/017 | Cost: 3.2149\n",
      "Train Loss: 0.464 | Val Loss: 0.397\n",
      "Iter: 210/1000 | Batch 000/017 | Cost: 4.4685\n",
      "Iter: 210/1000 | Batch 002/017 | Cost: 5.2983\n",
      "Train Loss: 0.522 | Val Loss: 0.653\n",
      "Iter: 211/1000 | Batch 000/017 | Cost: 3.0062\n",
      "Iter: 211/1000 | Batch 002/017 | Cost: 2.1378\n",
      "Train Loss: 0.651 | Val Loss: 0.430\n",
      "Iter: 212/1000 | Batch 000/017 | Cost: 2.6474\n",
      "Iter: 212/1000 | Batch 002/017 | Cost: 1.9647\n",
      "Train Loss: 0.597 | Val Loss: 0.708\n",
      "Iter: 213/1000 | Batch 000/017 | Cost: 2.1839\n",
      "Iter: 213/1000 | Batch 002/017 | Cost: 2.7961\n",
      "Train Loss: 0.650 | Val Loss: 0.324\n",
      "Iter: 214/1000 | Batch 000/017 | Cost: 1.7219\n",
      "Iter: 214/1000 | Batch 002/017 | Cost: 2.0234\n",
      "Train Loss: 1.000 | Val Loss: 0.552\n",
      "Iter: 215/1000 | Batch 000/017 | Cost: 1.9985\n",
      "Iter: 215/1000 | Batch 002/017 | Cost: 1.7915\n",
      "Train Loss: 0.599 | Val Loss: 0.339\n",
      "Iter: 216/1000 | Batch 000/017 | Cost: 1.9419\n",
      "Iter: 216/1000 | Batch 002/017 | Cost: 2.3737\n",
      "Train Loss: 0.333 | Val Loss: 0.524\n",
      "Iter: 217/1000 | Batch 000/017 | Cost: 2.0715\n",
      "Iter: 217/1000 | Batch 002/017 | Cost: 2.5792\n",
      "Train Loss: 0.711 | Val Loss: 0.507\n",
      "Iter: 218/1000 | Batch 000/017 | Cost: 2.2692\n",
      "Iter: 218/1000 | Batch 002/017 | Cost: 1.9542\n",
      "Train Loss: 1.003 | Val Loss: 0.477\n",
      "Iter: 219/1000 | Batch 000/017 | Cost: 2.8831\n",
      "Iter: 219/1000 | Batch 002/017 | Cost: 2.4558\n",
      "Train Loss: 1.000 | Val Loss: 0.169\n",
      "Iter: 220/1000 | Batch 000/017 | Cost: 2.0180\n",
      "Iter: 220/1000 | Batch 002/017 | Cost: 1.8006\n",
      "Train Loss: 0.559 | Val Loss: 0.698\n",
      "Iter: 221/1000 | Batch 000/017 | Cost: 2.3012\n",
      "Iter: 221/1000 | Batch 002/017 | Cost: 1.7114\n",
      "Train Loss: 0.693 | Val Loss: 0.487\n",
      "Iter: 222/1000 | Batch 000/017 | Cost: 4.0259\n",
      "Iter: 222/1000 | Batch 002/017 | Cost: 3.6558\n",
      "Train Loss: 0.758 | Val Loss: 0.173\n",
      "Iter: 223/1000 | Batch 000/017 | Cost: 2.6929\n",
      "Iter: 223/1000 | Batch 002/017 | Cost: 2.5957\n",
      "Train Loss: 0.318 | Val Loss: 0.449\n",
      "Iter: 224/1000 | Batch 000/017 | Cost: 3.1959\n",
      "Iter: 224/1000 | Batch 002/017 | Cost: 3.9819\n",
      "Train Loss: 0.250 | Val Loss: 0.661\n",
      "Iter: 225/1000 | Batch 000/017 | Cost: 2.6495\n",
      "Iter: 225/1000 | Batch 002/017 | Cost: 3.3467\n",
      "Train Loss: 0.840 | Val Loss: 0.342\n",
      "Iter: 226/1000 | Batch 000/017 | Cost: 2.3159\n",
      "Iter: 226/1000 | Batch 002/017 | Cost: 1.1379\n",
      "Train Loss: 0.489 | Val Loss: 0.649\n",
      "Iter: 227/1000 | Batch 000/017 | Cost: 1.6935\n",
      "Iter: 227/1000 | Batch 002/017 | Cost: 1.4174\n",
      "Train Loss: 0.576 | Val Loss: 0.441\n",
      "Iter: 228/1000 | Batch 000/017 | Cost: 3.7791\n",
      "Iter: 228/1000 | Batch 002/017 | Cost: 2.7567\n",
      "Train Loss: 0.464 | Val Loss: 0.564\n",
      "Iter: 229/1000 | Batch 000/017 | Cost: 1.4622\n",
      "Iter: 229/1000 | Batch 002/017 | Cost: 1.8905\n",
      "Train Loss: 0.358 | Val Loss: 0.730\n",
      "Iter: 230/1000 | Batch 000/017 | Cost: 2.3792\n",
      "Iter: 230/1000 | Batch 002/017 | Cost: 1.3759\n",
      "Train Loss: 0.422 | Val Loss: 0.369\n",
      "Iter: 231/1000 | Batch 000/017 | Cost: 2.8692\n",
      "Iter: 231/1000 | Batch 002/017 | Cost: 2.7718\n",
      "Train Loss: 0.789 | Val Loss: 0.979\n",
      "Iter: 232/1000 | Batch 000/017 | Cost: 3.2418\n",
      "Iter: 232/1000 | Batch 002/017 | Cost: 3.4099\n",
      "Train Loss: 0.915 | Val Loss: 0.653\n",
      "Iter: 233/1000 | Batch 000/017 | Cost: 4.0761\n",
      "Iter: 233/1000 | Batch 002/017 | Cost: 4.5175\n",
      "Train Loss: 0.757 | Val Loss: 0.873\n",
      "Iter: 234/1000 | Batch 000/017 | Cost: 2.8257\n",
      "Iter: 234/1000 | Batch 002/017 | Cost: 3.4041\n",
      "Train Loss: 0.572 | Val Loss: 0.554\n",
      "Iter: 235/1000 | Batch 000/017 | Cost: 1.8855\n",
      "Iter: 235/1000 | Batch 002/017 | Cost: 2.0526\n",
      "Train Loss: 0.288 | Val Loss: 0.816\n",
      "Iter: 236/1000 | Batch 000/017 | Cost: 2.7827\n",
      "Iter: 236/1000 | Batch 002/017 | Cost: 3.5114\n",
      "Train Loss: 0.451 | Val Loss: 0.327\n",
      "Iter: 237/1000 | Batch 000/017 | Cost: 4.6634\n",
      "Iter: 237/1000 | Batch 002/017 | Cost: 3.3962\n",
      "Train Loss: 0.444 | Val Loss: 0.650\n",
      "Iter: 238/1000 | Batch 000/017 | Cost: 3.1078\n",
      "Iter: 238/1000 | Batch 002/017 | Cost: 2.9754\n",
      "Train Loss: 0.264 | Val Loss: 0.734\n",
      "Iter: 239/1000 | Batch 000/017 | Cost: 4.0947\n",
      "Iter: 239/1000 | Batch 002/017 | Cost: 4.1486\n",
      "Train Loss: 0.984 | Val Loss: 0.619\n",
      "Iter: 240/1000 | Batch 000/017 | Cost: 2.5243\n",
      "Iter: 240/1000 | Batch 002/017 | Cost: 1.5170\n",
      "Train Loss: 0.525 | Val Loss: 0.702\n",
      "Iter: 241/1000 | Batch 000/017 | Cost: 1.8897\n",
      "Iter: 241/1000 | Batch 002/017 | Cost: 1.2338\n",
      "Train Loss: 0.518 | Val Loss: 0.371\n",
      "Iter: 242/1000 | Batch 000/017 | Cost: 4.6841\n",
      "Iter: 242/1000 | Batch 002/017 | Cost: 4.1840\n",
      "Train Loss: 0.438 | Val Loss: 0.261\n",
      "Iter: 243/1000 | Batch 000/017 | Cost: 4.6906\n",
      "Iter: 243/1000 | Batch 002/017 | Cost: 4.2907\n",
      "Train Loss: 0.657 | Val Loss: 0.603\n",
      "Iter: 244/1000 | Batch 000/017 | Cost: 1.3595\n",
      "Iter: 244/1000 | Batch 002/017 | Cost: 1.0792\n",
      "Train Loss: 0.366 | Val Loss: 0.283\n",
      "Iter: 245/1000 | Batch 000/017 | Cost: 2.8703\n",
      "Iter: 245/1000 | Batch 002/017 | Cost: 2.9807\n",
      "Train Loss: 0.668 | Val Loss: 0.280\n",
      "Iter: 246/1000 | Batch 000/017 | Cost: 2.5957\n",
      "Iter: 246/1000 | Batch 002/017 | Cost: 2.0254\n",
      "Train Loss: 0.678 | Val Loss: 0.545\n",
      "Iter: 247/1000 | Batch 000/017 | Cost: 2.8409\n",
      "Iter: 247/1000 | Batch 002/017 | Cost: 3.5721\n",
      "Train Loss: 0.700 | Val Loss: 0.679\n",
      "Iter: 248/1000 | Batch 000/017 | Cost: 3.3808\n",
      "Iter: 248/1000 | Batch 002/017 | Cost: 3.4134\n",
      "Train Loss: 0.801 | Val Loss: 0.222\n",
      "Iter: 249/1000 | Batch 000/017 | Cost: 2.7451\n",
      "Iter: 249/1000 | Batch 002/017 | Cost: 1.3044\n",
      "Train Loss: 1.054 | Val Loss: 0.349\n",
      "Iter: 250/1000 | Batch 000/017 | Cost: 1.8429\n",
      "Iter: 250/1000 | Batch 002/017 | Cost: 1.7885\n",
      "Train Loss: 0.992 | Val Loss: 0.326\n",
      "Iter: 251/1000 | Batch 000/017 | Cost: 1.9878\n",
      "Iter: 251/1000 | Batch 002/017 | Cost: 2.7262\n",
      "Train Loss: 0.432 | Val Loss: 0.320\n",
      "Iter: 252/1000 | Batch 000/017 | Cost: 0.8916\n",
      "Iter: 252/1000 | Batch 002/017 | Cost: 1.3659\n",
      "Train Loss: 0.434 | Val Loss: 0.736\n",
      "Iter: 253/1000 | Batch 000/017 | Cost: 1.0346\n",
      "Iter: 253/1000 | Batch 002/017 | Cost: 1.8154\n",
      "Train Loss: 0.546 | Val Loss: 0.699\n",
      "Iter: 254/1000 | Batch 000/017 | Cost: 3.4146\n",
      "Iter: 254/1000 | Batch 002/017 | Cost: 3.4358\n",
      "Train Loss: 0.748 | Val Loss: 0.441\n",
      "Iter: 255/1000 | Batch 000/017 | Cost: 2.6736\n",
      "Iter: 255/1000 | Batch 002/017 | Cost: 3.2527\n",
      "Train Loss: 0.895 | Val Loss: 0.831\n",
      "Iter: 256/1000 | Batch 000/017 | Cost: 2.7915\n",
      "Iter: 256/1000 | Batch 002/017 | Cost: 2.3818\n",
      "Train Loss: 0.792 | Val Loss: 0.637\n",
      "Iter: 257/1000 | Batch 000/017 | Cost: 3.2584\n",
      "Iter: 257/1000 | Batch 002/017 | Cost: 3.7503\n",
      "Train Loss: 0.847 | Val Loss: 0.573\n",
      "Iter: 258/1000 | Batch 000/017 | Cost: 3.0739\n",
      "Iter: 258/1000 | Batch 002/017 | Cost: 2.6597\n",
      "Train Loss: 0.535 | Val Loss: 0.347\n",
      "Iter: 259/1000 | Batch 000/017 | Cost: 2.8998\n",
      "Iter: 259/1000 | Batch 002/017 | Cost: 2.1193\n",
      "Train Loss: 0.629 | Val Loss: 0.702\n",
      "Iter: 260/1000 | Batch 000/017 | Cost: 2.3164\n",
      "Iter: 260/1000 | Batch 002/017 | Cost: 2.4602\n",
      "Train Loss: 0.498 | Val Loss: 0.290\n",
      "Iter: 261/1000 | Batch 000/017 | Cost: 3.7702\n",
      "Iter: 261/1000 | Batch 002/017 | Cost: 2.9975\n",
      "Train Loss: 1.081 | Val Loss: 0.596\n",
      "Iter: 262/1000 | Batch 000/017 | Cost: 2.3079\n",
      "Iter: 262/1000 | Batch 002/017 | Cost: 1.8828\n",
      "Train Loss: 1.119 | Val Loss: 0.610\n",
      "Iter: 263/1000 | Batch 000/017 | Cost: 2.1414\n",
      "Iter: 263/1000 | Batch 002/017 | Cost: 2.0586\n",
      "Train Loss: 0.510 | Val Loss: 0.669\n",
      "Iter: 264/1000 | Batch 000/017 | Cost: 1.8145\n",
      "Iter: 264/1000 | Batch 002/017 | Cost: 1.8611\n",
      "Train Loss: 0.641 | Val Loss: 0.625\n",
      "Iter: 265/1000 | Batch 000/017 | Cost: 2.5521\n",
      "Iter: 265/1000 | Batch 002/017 | Cost: 2.5593\n",
      "Train Loss: 0.743 | Val Loss: 0.602\n",
      "Iter: 266/1000 | Batch 000/017 | Cost: 2.4490\n",
      "Iter: 266/1000 | Batch 002/017 | Cost: 2.4771\n",
      "Train Loss: 0.551 | Val Loss: 0.783\n",
      "Iter: 267/1000 | Batch 000/017 | Cost: 4.1078\n",
      "Iter: 267/1000 | Batch 002/017 | Cost: 4.1292\n",
      "Train Loss: 0.594 | Val Loss: 0.324\n",
      "Iter: 268/1000 | Batch 000/017 | Cost: 2.4302\n",
      "Iter: 268/1000 | Batch 002/017 | Cost: 2.3190\n",
      "Train Loss: 0.933 | Val Loss: 0.353\n",
      "Iter: 269/1000 | Batch 000/017 | Cost: 5.3343\n",
      "Iter: 269/1000 | Batch 002/017 | Cost: 5.3388\n",
      "Train Loss: 0.486 | Val Loss: 0.761\n",
      "Iter: 270/1000 | Batch 000/017 | Cost: 3.5734\n",
      "Iter: 270/1000 | Batch 002/017 | Cost: 2.7743\n",
      "Train Loss: 0.616 | Val Loss: 0.410\n",
      "Iter: 271/1000 | Batch 000/017 | Cost: 2.5500\n",
      "Iter: 271/1000 | Batch 002/017 | Cost: 1.8470\n",
      "Train Loss: 0.546 | Val Loss: 0.502\n",
      "Iter: 272/1000 | Batch 000/017 | Cost: 3.1113\n",
      "Iter: 272/1000 | Batch 002/017 | Cost: 1.8559\n",
      "Train Loss: 0.595 | Val Loss: 0.462\n",
      "Iter: 273/1000 | Batch 000/017 | Cost: 1.7702\n",
      "Iter: 273/1000 | Batch 002/017 | Cost: 2.2157\n",
      "Train Loss: 0.712 | Val Loss: 0.587\n",
      "Iter: 274/1000 | Batch 000/017 | Cost: 3.8812\n",
      "Iter: 274/1000 | Batch 002/017 | Cost: 4.2798\n",
      "Train Loss: 0.767 | Val Loss: 0.502\n",
      "Iter: 275/1000 | Batch 000/017 | Cost: 3.7705\n",
      "Iter: 275/1000 | Batch 002/017 | Cost: 3.8787\n",
      "Train Loss: 0.520 | Val Loss: 0.587\n",
      "Iter: 276/1000 | Batch 000/017 | Cost: 2.5899\n",
      "Iter: 276/1000 | Batch 002/017 | Cost: 2.5204\n",
      "Train Loss: 0.668 | Val Loss: 0.627\n",
      "Iter: 277/1000 | Batch 000/017 | Cost: 2.4757\n",
      "Iter: 277/1000 | Batch 002/017 | Cost: 1.8047\n",
      "Train Loss: 0.541 | Val Loss: 0.734\n",
      "Iter: 278/1000 | Batch 000/017 | Cost: 3.3874\n",
      "Iter: 278/1000 | Batch 002/017 | Cost: 2.9666\n",
      "Train Loss: 0.448 | Val Loss: 0.379\n",
      "Iter: 279/1000 | Batch 000/017 | Cost: 2.8037\n",
      "Iter: 279/1000 | Batch 002/017 | Cost: 2.3973\n",
      "Train Loss: 0.563 | Val Loss: 0.362\n",
      "Iter: 280/1000 | Batch 000/017 | Cost: 2.7487\n",
      "Iter: 280/1000 | Batch 002/017 | Cost: 2.0819\n",
      "Train Loss: 0.577 | Val Loss: 0.708\n",
      "Iter: 281/1000 | Batch 000/017 | Cost: 2.1392\n",
      "Iter: 281/1000 | Batch 002/017 | Cost: 1.5097\n",
      "Train Loss: 1.035 | Val Loss: 0.428\n",
      "Iter: 282/1000 | Batch 000/017 | Cost: 1.9222\n",
      "Iter: 282/1000 | Batch 002/017 | Cost: 2.7662\n",
      "Train Loss: 0.513 | Val Loss: 0.542\n",
      "Iter: 283/1000 | Batch 000/017 | Cost: 2.1811\n",
      "Iter: 283/1000 | Batch 002/017 | Cost: 0.7758\n",
      "Train Loss: 0.295 | Val Loss: 0.759\n",
      "Iter: 284/1000 | Batch 000/017 | Cost: 3.0663\n",
      "Iter: 284/1000 | Batch 002/017 | Cost: 3.8205\n",
      "Train Loss: 0.337 | Val Loss: 0.894\n",
      "Iter: 285/1000 | Batch 000/017 | Cost: 4.5373\n",
      "Iter: 285/1000 | Batch 002/017 | Cost: 4.0368\n",
      "Train Loss: 0.483 | Val Loss: 0.512\n",
      "Iter: 286/1000 | Batch 000/017 | Cost: 1.2559\n",
      "Iter: 286/1000 | Batch 002/017 | Cost: 0.8660\n",
      "Train Loss: 0.756 | Val Loss: 0.641\n",
      "Iter: 287/1000 | Batch 000/017 | Cost: 1.9938\n",
      "Iter: 287/1000 | Batch 002/017 | Cost: 1.6437\n",
      "Train Loss: 1.116 | Val Loss: 0.800\n",
      "Iter: 288/1000 | Batch 000/017 | Cost: 0.8328\n",
      "Iter: 288/1000 | Batch 002/017 | Cost: 2.1473\n",
      "Train Loss: 0.788 | Val Loss: 0.845\n",
      "Iter: 289/1000 | Batch 000/017 | Cost: 3.3854\n",
      "Iter: 289/1000 | Batch 002/017 | Cost: 1.9339\n",
      "Train Loss: 0.691 | Val Loss: 0.867\n",
      "Iter: 290/1000 | Batch 000/017 | Cost: 3.2003\n",
      "Iter: 290/1000 | Batch 002/017 | Cost: 4.0534\n",
      "Train Loss: 0.817 | Val Loss: 0.818\n",
      "Iter: 291/1000 | Batch 000/017 | Cost: 2.1134\n",
      "Iter: 291/1000 | Batch 002/017 | Cost: 0.9647\n",
      "Train Loss: 0.285 | Val Loss: 0.261\n",
      "Iter: 292/1000 | Batch 000/017 | Cost: 2.0657\n",
      "Iter: 292/1000 | Batch 002/017 | Cost: 2.8836\n",
      "Train Loss: 0.603 | Val Loss: 0.641\n",
      "Iter: 293/1000 | Batch 000/017 | Cost: 2.7167\n",
      "Iter: 293/1000 | Batch 002/017 | Cost: 3.6827\n",
      "Train Loss: 0.538 | Val Loss: 0.620\n",
      "Iter: 294/1000 | Batch 000/017 | Cost: 3.2613\n",
      "Iter: 294/1000 | Batch 002/017 | Cost: 4.4286\n",
      "Train Loss: 0.315 | Val Loss: 0.687\n",
      "Iter: 295/1000 | Batch 000/017 | Cost: 2.6091\n",
      "Iter: 295/1000 | Batch 002/017 | Cost: 2.0037\n",
      "Train Loss: 1.016 | Val Loss: 0.675\n",
      "Iter: 296/1000 | Batch 000/017 | Cost: 2.8102\n",
      "Iter: 296/1000 | Batch 002/017 | Cost: 3.4166\n",
      "Train Loss: 0.765 | Val Loss: 0.689\n",
      "Iter: 297/1000 | Batch 000/017 | Cost: 1.7147\n",
      "Iter: 297/1000 | Batch 002/017 | Cost: 1.0226\n",
      "Train Loss: 0.756 | Val Loss: 0.415\n",
      "Iter: 298/1000 | Batch 000/017 | Cost: 4.2495\n",
      "Iter: 298/1000 | Batch 002/017 | Cost: 3.2479\n",
      "Train Loss: 0.654 | Val Loss: 0.525\n",
      "Iter: 299/1000 | Batch 000/017 | Cost: 2.7080\n",
      "Iter: 299/1000 | Batch 002/017 | Cost: 1.9459\n",
      "Train Loss: 0.578 | Val Loss: 0.200\n",
      "Iter: 300/1000 | Batch 000/017 | Cost: 5.5327\n",
      "Iter: 300/1000 | Batch 002/017 | Cost: 5.2381\n",
      "Train Loss: 0.963 | Val Loss: 0.405\n",
      "Iter: 301/1000 | Batch 000/017 | Cost: 6.1084\n",
      "Iter: 301/1000 | Batch 002/017 | Cost: 5.5693\n",
      "Train Loss: 0.322 | Val Loss: 0.315\n",
      "Iter: 302/1000 | Batch 000/017 | Cost: 3.1510\n",
      "Iter: 302/1000 | Batch 002/017 | Cost: 2.4461\n",
      "Train Loss: 0.539 | Val Loss: 0.574\n",
      "Iter: 303/1000 | Batch 000/017 | Cost: 3.7560\n",
      "Iter: 303/1000 | Batch 002/017 | Cost: 2.7858\n",
      "Train Loss: 0.855 | Val Loss: 0.156\n",
      "Iter: 304/1000 | Batch 000/017 | Cost: 2.2800\n",
      "Iter: 304/1000 | Batch 002/017 | Cost: 2.2826\n",
      "Train Loss: 0.391 | Val Loss: 0.326\n",
      "Iter: 305/1000 | Batch 000/017 | Cost: 4.2211\n",
      "Iter: 305/1000 | Batch 002/017 | Cost: 4.1932\n",
      "Train Loss: 0.526 | Val Loss: 0.172\n",
      "Iter: 306/1000 | Batch 000/017 | Cost: 2.9851\n",
      "Iter: 306/1000 | Batch 002/017 | Cost: 3.5183\n",
      "Train Loss: 0.773 | Val Loss: 0.314\n",
      "Iter: 307/1000 | Batch 000/017 | Cost: 3.9537\n",
      "Iter: 307/1000 | Batch 002/017 | Cost: 4.2306\n",
      "Train Loss: 0.688 | Val Loss: 0.543\n",
      "Iter: 308/1000 | Batch 000/017 | Cost: 3.7054\n",
      "Iter: 308/1000 | Batch 002/017 | Cost: 2.9263\n",
      "Train Loss: 0.802 | Val Loss: 0.517\n",
      "Iter: 309/1000 | Batch 000/017 | Cost: 2.4262\n",
      "Iter: 309/1000 | Batch 002/017 | Cost: 2.6052\n",
      "Train Loss: 0.465 | Val Loss: 0.209\n",
      "Iter: 310/1000 | Batch 000/017 | Cost: 3.6981\n",
      "Iter: 310/1000 | Batch 002/017 | Cost: 3.1027\n",
      "Train Loss: 0.622 | Val Loss: 0.311\n",
      "Iter: 311/1000 | Batch 000/017 | Cost: 2.0916\n",
      "Iter: 311/1000 | Batch 002/017 | Cost: 2.2576\n",
      "Train Loss: 0.963 | Val Loss: 0.587\n",
      "Iter: 312/1000 | Batch 000/017 | Cost: 3.1290\n",
      "Iter: 312/1000 | Batch 002/017 | Cost: 2.7825\n",
      "Train Loss: 0.408 | Val Loss: 0.605\n",
      "Iter: 313/1000 | Batch 000/017 | Cost: 2.9634\n",
      "Iter: 313/1000 | Batch 002/017 | Cost: 2.2122\n",
      "Train Loss: 0.544 | Val Loss: 0.168\n",
      "Iter: 314/1000 | Batch 000/017 | Cost: 4.3806\n",
      "Iter: 314/1000 | Batch 002/017 | Cost: 3.9459\n",
      "Train Loss: 0.461 | Val Loss: 0.833\n",
      "Iter: 315/1000 | Batch 000/017 | Cost: 1.7727\n",
      "Iter: 315/1000 | Batch 002/017 | Cost: 2.4039\n",
      "Train Loss: 0.532 | Val Loss: 0.827\n",
      "Iter: 316/1000 | Batch 000/017 | Cost: 5.4521\n",
      "Iter: 316/1000 | Batch 002/017 | Cost: 6.0669\n",
      "Train Loss: 0.533 | Val Loss: 0.576\n",
      "Iter: 317/1000 | Batch 000/017 | Cost: 3.8629\n",
      "Iter: 317/1000 | Batch 002/017 | Cost: 3.7994\n",
      "Train Loss: 0.856 | Val Loss: 0.588\n",
      "Iter: 318/1000 | Batch 000/017 | Cost: 3.1270\n",
      "Iter: 318/1000 | Batch 002/017 | Cost: 3.0099\n",
      "Train Loss: 0.831 | Val Loss: 0.579\n",
      "Iter: 319/1000 | Batch 000/017 | Cost: 1.8314\n",
      "Iter: 319/1000 | Batch 002/017 | Cost: 1.8080\n",
      "Train Loss: 0.414 | Val Loss: 0.402\n",
      "Iter: 320/1000 | Batch 000/017 | Cost: 4.7586\n",
      "Iter: 320/1000 | Batch 002/017 | Cost: 4.8800\n",
      "Train Loss: 0.784 | Val Loss: 0.270\n",
      "Iter: 321/1000 | Batch 000/017 | Cost: 3.1648\n",
      "Iter: 321/1000 | Batch 002/017 | Cost: 3.6303\n",
      "Train Loss: 0.662 | Val Loss: 0.457\n",
      "Iter: 322/1000 | Batch 000/017 | Cost: 1.8848\n",
      "Iter: 322/1000 | Batch 002/017 | Cost: 1.6712\n",
      "Train Loss: 0.915 | Val Loss: 0.709\n",
      "Iter: 323/1000 | Batch 000/017 | Cost: 3.2992\n",
      "Iter: 323/1000 | Batch 002/017 | Cost: 2.9946\n",
      "Train Loss: 0.669 | Val Loss: 0.329\n",
      "Iter: 324/1000 | Batch 000/017 | Cost: 1.8852\n",
      "Iter: 324/1000 | Batch 002/017 | Cost: 1.9825\n",
      "Train Loss: 0.596 | Val Loss: 0.670\n",
      "Iter: 325/1000 | Batch 000/017 | Cost: 1.9209\n",
      "Iter: 325/1000 | Batch 002/017 | Cost: 2.1695\n",
      "Train Loss: 1.041 | Val Loss: 0.512\n",
      "Iter: 326/1000 | Batch 000/017 | Cost: 2.8482\n",
      "Iter: 326/1000 | Batch 002/017 | Cost: 2.1035\n",
      "Train Loss: 0.383 | Val Loss: 0.710\n",
      "Iter: 327/1000 | Batch 000/017 | Cost: 2.4756\n",
      "Iter: 327/1000 | Batch 002/017 | Cost: 1.9252\n",
      "Train Loss: 0.537 | Val Loss: 0.580\n",
      "Iter: 328/1000 | Batch 000/017 | Cost: 2.6947\n",
      "Iter: 328/1000 | Batch 002/017 | Cost: 2.1099\n",
      "Train Loss: 0.849 | Val Loss: 0.567\n",
      "Iter: 329/1000 | Batch 000/017 | Cost: 3.1747\n",
      "Iter: 329/1000 | Batch 002/017 | Cost: 3.5175\n",
      "Train Loss: 0.421 | Val Loss: 0.640\n",
      "Iter: 330/1000 | Batch 000/017 | Cost: 3.2227\n",
      "Iter: 330/1000 | Batch 002/017 | Cost: 2.9460\n",
      "Train Loss: 0.789 | Val Loss: 0.289\n",
      "Iter: 331/1000 | Batch 000/017 | Cost: 1.4172\n",
      "Iter: 331/1000 | Batch 002/017 | Cost: 1.9306\n",
      "Train Loss: 0.665 | Val Loss: 0.356\n",
      "Iter: 332/1000 | Batch 000/017 | Cost: 3.3381\n",
      "Iter: 332/1000 | Batch 002/017 | Cost: 4.2760\n",
      "Train Loss: 0.559 | Val Loss: 0.499\n",
      "Iter: 333/1000 | Batch 000/017 | Cost: 1.2989\n",
      "Iter: 333/1000 | Batch 002/017 | Cost: 0.7371\n",
      "Train Loss: 0.837 | Val Loss: 0.508\n",
      "Iter: 334/1000 | Batch 000/017 | Cost: 1.3446\n",
      "Iter: 334/1000 | Batch 002/017 | Cost: 2.0166\n",
      "Train Loss: 0.230 | Val Loss: 0.529\n",
      "Iter: 335/1000 | Batch 000/017 | Cost: 3.4258\n",
      "Iter: 335/1000 | Batch 002/017 | Cost: 2.8141\n",
      "Train Loss: 0.492 | Val Loss: 0.735\n",
      "Iter: 336/1000 | Batch 000/017 | Cost: 3.5323\n",
      "Iter: 336/1000 | Batch 002/017 | Cost: 3.2325\n",
      "Train Loss: 1.021 | Val Loss: 0.564\n",
      "Iter: 337/1000 | Batch 000/017 | Cost: 5.0989\n",
      "Iter: 337/1000 | Batch 002/017 | Cost: 6.0066\n",
      "Train Loss: 0.650 | Val Loss: 0.583\n",
      "Iter: 338/1000 | Batch 000/017 | Cost: 2.0911\n",
      "Iter: 338/1000 | Batch 002/017 | Cost: 3.2209\n",
      "Train Loss: 0.430 | Val Loss: 0.519\n",
      "Iter: 339/1000 | Batch 000/017 | Cost: 3.1165\n",
      "Iter: 339/1000 | Batch 002/017 | Cost: 2.5549\n",
      "Train Loss: 0.607 | Val Loss: 0.714\n",
      "Iter: 340/1000 | Batch 000/017 | Cost: 1.8411\n",
      "Iter: 340/1000 | Batch 002/017 | Cost: 2.5866\n",
      "Train Loss: 0.858 | Val Loss: 0.892\n",
      "Iter: 341/1000 | Batch 000/017 | Cost: 0.9413\n",
      "Iter: 341/1000 | Batch 002/017 | Cost: 3.2922\n",
      "Train Loss: 0.389 | Val Loss: 0.599\n",
      "Iter: 342/1000 | Batch 000/017 | Cost: 4.4620\n",
      "Iter: 342/1000 | Batch 002/017 | Cost: 3.9432\n",
      "Train Loss: 0.257 | Val Loss: 0.713\n",
      "Iter: 343/1000 | Batch 000/017 | Cost: 3.0813\n",
      "Iter: 343/1000 | Batch 002/017 | Cost: 2.8693\n",
      "Train Loss: 0.376 | Val Loss: 0.427\n",
      "Iter: 344/1000 | Batch 000/017 | Cost: 0.8203\n",
      "Iter: 344/1000 | Batch 002/017 | Cost: 1.3028\n",
      "Train Loss: 0.601 | Val Loss: 0.765\n",
      "Iter: 345/1000 | Batch 000/017 | Cost: 0.9962\n",
      "Iter: 345/1000 | Batch 002/017 | Cost: 1.8694\n",
      "Train Loss: 0.447 | Val Loss: 0.371\n",
      "Iter: 346/1000 | Batch 000/017 | Cost: 1.5273\n",
      "Iter: 346/1000 | Batch 002/017 | Cost: 2.5683\n",
      "Train Loss: 0.411 | Val Loss: 0.378\n",
      "Iter: 347/1000 | Batch 000/017 | Cost: 4.2493\n",
      "Iter: 347/1000 | Batch 002/017 | Cost: 3.7265\n",
      "Train Loss: 0.322 | Val Loss: 0.424\n",
      "Iter: 348/1000 | Batch 000/017 | Cost: 1.5464\n",
      "Iter: 348/1000 | Batch 002/017 | Cost: 2.1342\n",
      "Train Loss: 0.754 | Val Loss: 0.669\n",
      "Iter: 349/1000 | Batch 000/017 | Cost: 3.6058\n",
      "Iter: 349/1000 | Batch 002/017 | Cost: 2.4157\n",
      "Train Loss: 0.696 | Val Loss: 0.775\n",
      "Iter: 350/1000 | Batch 000/017 | Cost: 3.5352\n",
      "Iter: 350/1000 | Batch 002/017 | Cost: 3.9883\n",
      "Train Loss: 0.782 | Val Loss: 0.613\n",
      "Iter: 351/1000 | Batch 000/017 | Cost: 2.2018\n",
      "Iter: 351/1000 | Batch 002/017 | Cost: 1.7336\n",
      "Train Loss: 0.553 | Val Loss: 0.668\n",
      "Iter: 352/1000 | Batch 000/017 | Cost: 1.8553\n",
      "Iter: 352/1000 | Batch 002/017 | Cost: 2.2377\n",
      "Train Loss: 0.424 | Val Loss: 0.382\n",
      "Iter: 353/1000 | Batch 000/017 | Cost: 2.7457\n",
      "Iter: 353/1000 | Batch 002/017 | Cost: 2.4381\n",
      "Train Loss: 0.552 | Val Loss: 0.981\n",
      "Iter: 354/1000 | Batch 000/017 | Cost: 2.4293\n",
      "Iter: 354/1000 | Batch 002/017 | Cost: 2.3365\n",
      "Train Loss: 0.834 | Val Loss: 0.701\n",
      "Iter: 355/1000 | Batch 000/017 | Cost: 2.8204\n",
      "Iter: 355/1000 | Batch 002/017 | Cost: 2.3724\n",
      "Train Loss: 0.716 | Val Loss: 0.398\n",
      "Iter: 356/1000 | Batch 000/017 | Cost: 3.1817\n",
      "Iter: 356/1000 | Batch 002/017 | Cost: 3.2258\n",
      "Train Loss: 0.560 | Val Loss: 0.714\n",
      "Iter: 357/1000 | Batch 000/017 | Cost: 3.2472\n",
      "Iter: 357/1000 | Batch 002/017 | Cost: 3.0346\n",
      "Train Loss: 0.442 | Val Loss: 0.652\n",
      "Iter: 358/1000 | Batch 000/017 | Cost: 3.1083\n",
      "Iter: 358/1000 | Batch 002/017 | Cost: 2.6172\n",
      "Train Loss: 0.388 | Val Loss: 0.441\n",
      "Iter: 359/1000 | Batch 000/017 | Cost: 3.3342\n",
      "Iter: 359/1000 | Batch 002/017 | Cost: 4.7264\n",
      "Train Loss: 0.498 | Val Loss: 0.277\n",
      "Iter: 360/1000 | Batch 000/017 | Cost: 4.2321\n",
      "Iter: 360/1000 | Batch 002/017 | Cost: 3.6429\n",
      "Train Loss: 0.416 | Val Loss: 0.828\n",
      "Iter: 361/1000 | Batch 000/017 | Cost: 2.7152\n",
      "Iter: 361/1000 | Batch 002/017 | Cost: 1.8305\n",
      "Train Loss: 0.597 | Val Loss: 0.589\n",
      "Iter: 362/1000 | Batch 000/017 | Cost: 2.2630\n",
      "Iter: 362/1000 | Batch 002/017 | Cost: 1.1447\n",
      "Train Loss: 0.637 | Val Loss: 0.592\n",
      "Iter: 363/1000 | Batch 000/017 | Cost: 0.8084\n",
      "Iter: 363/1000 | Batch 002/017 | Cost: 1.2837\n",
      "Train Loss: 0.535 | Val Loss: 0.586\n",
      "Iter: 364/1000 | Batch 000/017 | Cost: 4.3809\n",
      "Iter: 364/1000 | Batch 002/017 | Cost: 4.4094\n",
      "Train Loss: 0.124 | Val Loss: 0.313\n",
      "Iter: 365/1000 | Batch 000/017 | Cost: 3.4703\n",
      "Iter: 365/1000 | Batch 002/017 | Cost: 4.2192\n",
      "Train Loss: 0.504 | Val Loss: 0.866\n",
      "Iter: 366/1000 | Batch 000/017 | Cost: 3.2850\n",
      "Iter: 366/1000 | Batch 002/017 | Cost: 2.1842\n",
      "Train Loss: 0.511 | Val Loss: 0.585\n",
      "Iter: 367/1000 | Batch 000/017 | Cost: 2.5436\n",
      "Iter: 367/1000 | Batch 002/017 | Cost: 2.0678\n",
      "Train Loss: 0.856 | Val Loss: 0.908\n",
      "Iter: 368/1000 | Batch 000/017 | Cost: 3.3741\n",
      "Iter: 368/1000 | Batch 002/017 | Cost: 2.9620\n",
      "Train Loss: 0.692 | Val Loss: 0.363\n",
      "Iter: 369/1000 | Batch 000/017 | Cost: 4.7363\n",
      "Iter: 369/1000 | Batch 002/017 | Cost: 4.9749\n",
      "Train Loss: 0.322 | Val Loss: 0.674\n",
      "Iter: 370/1000 | Batch 000/017 | Cost: 2.9440\n",
      "Iter: 370/1000 | Batch 002/017 | Cost: 2.4628\n",
      "Train Loss: 0.569 | Val Loss: 0.597\n",
      "Iter: 371/1000 | Batch 000/017 | Cost: 2.0081\n",
      "Iter: 371/1000 | Batch 002/017 | Cost: 2.2072\n",
      "Train Loss: 0.287 | Val Loss: 0.927\n",
      "Iter: 372/1000 | Batch 000/017 | Cost: 4.6536\n",
      "Iter: 372/1000 | Batch 002/017 | Cost: 4.1405\n",
      "Train Loss: 0.392 | Val Loss: 0.928\n",
      "Iter: 373/1000 | Batch 000/017 | Cost: 3.8045\n",
      "Iter: 373/1000 | Batch 002/017 | Cost: 3.7945\n",
      "Train Loss: 0.783 | Val Loss: 0.532\n",
      "Iter: 374/1000 | Batch 000/017 | Cost: 4.1411\n",
      "Iter: 374/1000 | Batch 002/017 | Cost: 2.6294\n",
      "Train Loss: 0.415 | Val Loss: 0.465\n",
      "Iter: 375/1000 | Batch 000/017 | Cost: 4.6290\n",
      "Iter: 375/1000 | Batch 002/017 | Cost: 3.4413\n",
      "Train Loss: 0.670 | Val Loss: 0.217\n",
      "Iter: 376/1000 | Batch 000/017 | Cost: 3.1812\n",
      "Iter: 376/1000 | Batch 002/017 | Cost: 2.6148\n",
      "Train Loss: 0.589 | Val Loss: 0.229\n",
      "Iter: 377/1000 | Batch 000/017 | Cost: 3.1566\n",
      "Iter: 377/1000 | Batch 002/017 | Cost: 2.3526\n",
      "Train Loss: 0.815 | Val Loss: 0.733\n",
      "Iter: 378/1000 | Batch 000/017 | Cost: 0.9380\n",
      "Iter: 378/1000 | Batch 002/017 | Cost: 1.8689\n",
      "Train Loss: 0.815 | Val Loss: 0.416\n",
      "Iter: 379/1000 | Batch 000/017 | Cost: 1.8728\n",
      "Iter: 379/1000 | Batch 002/017 | Cost: 1.9618\n",
      "Train Loss: 0.595 | Val Loss: 0.335\n",
      "Iter: 380/1000 | Batch 000/017 | Cost: 3.3544\n",
      "Iter: 380/1000 | Batch 002/017 | Cost: 3.1847\n",
      "Train Loss: 0.385 | Val Loss: 0.810\n",
      "Iter: 381/1000 | Batch 000/017 | Cost: 4.3136\n",
      "Iter: 381/1000 | Batch 002/017 | Cost: 4.0008\n",
      "Train Loss: 0.829 | Val Loss: 0.612\n",
      "Iter: 382/1000 | Batch 000/017 | Cost: 1.9373\n",
      "Iter: 382/1000 | Batch 002/017 | Cost: 3.0803\n",
      "Train Loss: 0.508 | Val Loss: 0.746\n",
      "Iter: 383/1000 | Batch 000/017 | Cost: 1.4346\n",
      "Iter: 383/1000 | Batch 002/017 | Cost: 2.3268\n",
      "Train Loss: 0.371 | Val Loss: 0.605\n",
      "Iter: 384/1000 | Batch 000/017 | Cost: 3.7526\n",
      "Iter: 384/1000 | Batch 002/017 | Cost: 2.8954\n",
      "Train Loss: 0.395 | Val Loss: 0.577\n",
      "Iter: 385/1000 | Batch 000/017 | Cost: 2.5529\n",
      "Iter: 385/1000 | Batch 002/017 | Cost: 3.0048\n",
      "Train Loss: 0.406 | Val Loss: 0.634\n",
      "Iter: 386/1000 | Batch 000/017 | Cost: 2.6732\n",
      "Iter: 386/1000 | Batch 002/017 | Cost: 2.9016\n",
      "Train Loss: 0.779 | Val Loss: 0.653\n",
      "Iter: 387/1000 | Batch 000/017 | Cost: 2.4557\n",
      "Iter: 387/1000 | Batch 002/017 | Cost: 2.6999\n",
      "Train Loss: 0.488 | Val Loss: 0.815\n",
      "Iter: 388/1000 | Batch 000/017 | Cost: 1.0211\n",
      "Iter: 388/1000 | Batch 002/017 | Cost: 1.3349\n",
      "Train Loss: 0.691 | Val Loss: 0.885\n",
      "Iter: 389/1000 | Batch 000/017 | Cost: 4.0220\n",
      "Iter: 389/1000 | Batch 002/017 | Cost: 3.4179\n",
      "Train Loss: 0.889 | Val Loss: 1.072\n",
      "Iter: 390/1000 | Batch 000/017 | Cost: 4.0809\n",
      "Iter: 390/1000 | Batch 002/017 | Cost: 2.6955\n",
      "Train Loss: 0.830 | Val Loss: 0.927\n",
      "Iter: 391/1000 | Batch 000/017 | Cost: 2.8914\n",
      "Iter: 391/1000 | Batch 002/017 | Cost: 3.5002\n",
      "Train Loss: 0.819 | Val Loss: 0.618\n",
      "Iter: 392/1000 | Batch 000/017 | Cost: 2.1125\n",
      "Iter: 392/1000 | Batch 002/017 | Cost: 1.4851\n",
      "Train Loss: 0.411 | Val Loss: 0.656\n",
      "Iter: 393/1000 | Batch 000/017 | Cost: 2.3880\n",
      "Iter: 393/1000 | Batch 002/017 | Cost: 2.7912\n",
      "Train Loss: 0.823 | Val Loss: 0.453\n",
      "Iter: 394/1000 | Batch 000/017 | Cost: 3.8447\n",
      "Iter: 394/1000 | Batch 002/017 | Cost: 3.6520\n",
      "Train Loss: 0.610 | Val Loss: 0.585\n",
      "Iter: 395/1000 | Batch 000/017 | Cost: 2.2842\n",
      "Iter: 395/1000 | Batch 002/017 | Cost: 1.3968\n",
      "Train Loss: 0.600 | Val Loss: 0.624\n",
      "Iter: 396/1000 | Batch 000/017 | Cost: 2.3382\n",
      "Iter: 396/1000 | Batch 002/017 | Cost: 2.9487\n",
      "Train Loss: 0.458 | Val Loss: 0.821\n",
      "Iter: 397/1000 | Batch 000/017 | Cost: 2.8682\n",
      "Iter: 397/1000 | Batch 002/017 | Cost: 2.8993\n",
      "Train Loss: 0.755 | Val Loss: 0.344\n",
      "Iter: 398/1000 | Batch 000/017 | Cost: 1.7899\n",
      "Iter: 398/1000 | Batch 002/017 | Cost: 2.4818\n",
      "Train Loss: 0.653 | Val Loss: 0.577\n",
      "Iter: 399/1000 | Batch 000/017 | Cost: 3.2138\n",
      "Iter: 399/1000 | Batch 002/017 | Cost: 2.3676\n",
      "Train Loss: 0.614 | Val Loss: 0.409\n",
      "Iter: 400/1000 | Batch 000/017 | Cost: 3.0288\n",
      "Iter: 400/1000 | Batch 002/017 | Cost: 2.0923\n",
      "Train Loss: 0.700 | Val Loss: 0.596\n",
      "Iter: 401/1000 | Batch 000/017 | Cost: 3.4742\n",
      "Iter: 401/1000 | Batch 002/017 | Cost: 4.3522\n",
      "Train Loss: 0.606 | Val Loss: 0.650\n",
      "Iter: 402/1000 | Batch 000/017 | Cost: 2.6009\n",
      "Iter: 402/1000 | Batch 002/017 | Cost: 3.7642\n",
      "Train Loss: 0.698 | Val Loss: 0.607\n",
      "Iter: 403/1000 | Batch 000/017 | Cost: 4.3078\n",
      "Iter: 403/1000 | Batch 002/017 | Cost: 4.9573\n",
      "Train Loss: 0.619 | Val Loss: 0.358\n",
      "Iter: 404/1000 | Batch 000/017 | Cost: 3.3253\n",
      "Iter: 404/1000 | Batch 002/017 | Cost: 2.7600\n",
      "Train Loss: 0.423 | Val Loss: 0.580\n",
      "Iter: 405/1000 | Batch 000/017 | Cost: 3.5735\n",
      "Iter: 405/1000 | Batch 002/017 | Cost: 2.1408\n",
      "Train Loss: 0.756 | Val Loss: 0.538\n",
      "Iter: 406/1000 | Batch 000/017 | Cost: 3.6514\n",
      "Iter: 406/1000 | Batch 002/017 | Cost: 2.4192\n",
      "Train Loss: 0.673 | Val Loss: 0.329\n",
      "Iter: 407/1000 | Batch 000/017 | Cost: 2.6350\n",
      "Iter: 407/1000 | Batch 002/017 | Cost: 3.5499\n",
      "Train Loss: 0.740 | Val Loss: 0.861\n",
      "Iter: 408/1000 | Batch 000/017 | Cost: 2.8557\n",
      "Iter: 408/1000 | Batch 002/017 | Cost: 2.8106\n",
      "Train Loss: 0.488 | Val Loss: 0.630\n",
      "Iter: 409/1000 | Batch 000/017 | Cost: 4.2364\n",
      "Iter: 409/1000 | Batch 002/017 | Cost: 3.8137\n",
      "Train Loss: 1.032 | Val Loss: 0.430\n",
      "Iter: 410/1000 | Batch 000/017 | Cost: 1.4887\n",
      "Iter: 410/1000 | Batch 002/017 | Cost: 1.2023\n",
      "Train Loss: 1.035 | Val Loss: 0.718\n",
      "Iter: 411/1000 | Batch 000/017 | Cost: 0.5905\n",
      "Iter: 411/1000 | Batch 002/017 | Cost: 1.0632\n",
      "Train Loss: 1.015 | Val Loss: 0.707\n",
      "Iter: 412/1000 | Batch 000/017 | Cost: 3.6577\n",
      "Iter: 412/1000 | Batch 002/017 | Cost: 3.5712\n",
      "Train Loss: 0.916 | Val Loss: 0.849\n",
      "Iter: 413/1000 | Batch 000/017 | Cost: 2.0490\n",
      "Iter: 413/1000 | Batch 002/017 | Cost: 2.9753\n",
      "Train Loss: 1.280 | Val Loss: 0.436\n",
      "Iter: 414/1000 | Batch 000/017 | Cost: 2.8962\n",
      "Iter: 414/1000 | Batch 002/017 | Cost: 2.1192\n",
      "Train Loss: 0.740 | Val Loss: 0.623\n",
      "Iter: 415/1000 | Batch 000/017 | Cost: 2.1411\n",
      "Iter: 415/1000 | Batch 002/017 | Cost: 1.0679\n",
      "Train Loss: 0.570 | Val Loss: 0.269\n",
      "Iter: 416/1000 | Batch 000/017 | Cost: 1.3220\n",
      "Iter: 416/1000 | Batch 002/017 | Cost: 1.1099\n",
      "Train Loss: 0.440 | Val Loss: 0.557\n",
      "Iter: 417/1000 | Batch 000/017 | Cost: 3.2671\n",
      "Iter: 417/1000 | Batch 002/017 | Cost: 3.5521\n",
      "Train Loss: 0.740 | Val Loss: 0.507\n",
      "Iter: 418/1000 | Batch 000/017 | Cost: 1.4280\n",
      "Iter: 418/1000 | Batch 002/017 | Cost: 1.0304\n",
      "Train Loss: 0.630 | Val Loss: 0.335\n",
      "Iter: 419/1000 | Batch 000/017 | Cost: 4.0219\n",
      "Iter: 419/1000 | Batch 002/017 | Cost: 4.2729\n",
      "Train Loss: 0.625 | Val Loss: 0.476\n",
      "Iter: 420/1000 | Batch 000/017 | Cost: 1.5229\n",
      "Iter: 420/1000 | Batch 002/017 | Cost: 1.4523\n",
      "Train Loss: 0.488 | Val Loss: 0.931\n",
      "Iter: 421/1000 | Batch 000/017 | Cost: 2.6068\n",
      "Iter: 421/1000 | Batch 002/017 | Cost: 2.5938\n",
      "Train Loss: 0.929 | Val Loss: 0.528\n",
      "Iter: 422/1000 | Batch 000/017 | Cost: 1.8457\n",
      "Iter: 422/1000 | Batch 002/017 | Cost: 3.4937\n",
      "Train Loss: 0.468 | Val Loss: 0.336\n",
      "Iter: 423/1000 | Batch 000/017 | Cost: 3.6854\n",
      "Iter: 423/1000 | Batch 002/017 | Cost: 3.1179\n",
      "Train Loss: 1.004 | Val Loss: 0.931\n",
      "Iter: 424/1000 | Batch 000/017 | Cost: 1.4881\n",
      "Iter: 424/1000 | Batch 002/017 | Cost: 2.9993\n",
      "Train Loss: 0.363 | Val Loss: 0.437\n",
      "Iter: 425/1000 | Batch 000/017 | Cost: 2.2340\n",
      "Iter: 425/1000 | Batch 002/017 | Cost: 1.6645\n",
      "Train Loss: 0.717 | Val Loss: 0.682\n",
      "Iter: 426/1000 | Batch 000/017 | Cost: 2.5694\n",
      "Iter: 426/1000 | Batch 002/017 | Cost: 3.1221\n",
      "Train Loss: 0.463 | Val Loss: 0.435\n",
      "Iter: 427/1000 | Batch 000/017 | Cost: 5.2122\n",
      "Iter: 427/1000 | Batch 002/017 | Cost: 4.9361\n",
      "Train Loss: 0.266 | Val Loss: 0.638\n",
      "Iter: 428/1000 | Batch 000/017 | Cost: 2.9913\n",
      "Iter: 428/1000 | Batch 002/017 | Cost: 1.8094\n",
      "Train Loss: 0.577 | Val Loss: 0.637\n",
      "Iter: 429/1000 | Batch 000/017 | Cost: 3.1800\n",
      "Iter: 429/1000 | Batch 002/017 | Cost: 1.8057\n",
      "Train Loss: 0.322 | Val Loss: 0.429\n",
      "Iter: 430/1000 | Batch 000/017 | Cost: 2.1057\n",
      "Iter: 430/1000 | Batch 002/017 | Cost: 2.4970\n",
      "Train Loss: 0.775 | Val Loss: 0.428\n",
      "Iter: 431/1000 | Batch 000/017 | Cost: 3.6095\n",
      "Iter: 431/1000 | Batch 002/017 | Cost: 2.0362\n",
      "Train Loss: 0.743 | Val Loss: 0.669\n",
      "Iter: 432/1000 | Batch 000/017 | Cost: 2.2402\n",
      "Iter: 432/1000 | Batch 002/017 | Cost: 3.5146\n",
      "Train Loss: 0.360 | Val Loss: 0.235\n",
      "Iter: 433/1000 | Batch 000/017 | Cost: 2.5704\n",
      "Iter: 433/1000 | Batch 002/017 | Cost: 2.6785\n",
      "Train Loss: 1.155 | Val Loss: 0.705\n",
      "Iter: 434/1000 | Batch 000/017 | Cost: 3.0976\n",
      "Iter: 434/1000 | Batch 002/017 | Cost: 2.7671\n",
      "Train Loss: 0.523 | Val Loss: 0.355\n",
      "Iter: 435/1000 | Batch 000/017 | Cost: 3.8905\n",
      "Iter: 435/1000 | Batch 002/017 | Cost: 4.3396\n",
      "Train Loss: 0.587 | Val Loss: 0.567\n",
      "Iter: 436/1000 | Batch 000/017 | Cost: 3.4156\n",
      "Iter: 436/1000 | Batch 002/017 | Cost: 4.2136\n",
      "Train Loss: 0.932 | Val Loss: 0.650\n",
      "Iter: 437/1000 | Batch 000/017 | Cost: 3.0646\n",
      "Iter: 437/1000 | Batch 002/017 | Cost: 2.9834\n",
      "Train Loss: 0.226 | Val Loss: 1.072\n",
      "Iter: 438/1000 | Batch 000/017 | Cost: 2.5361\n",
      "Iter: 438/1000 | Batch 002/017 | Cost: 2.3817\n",
      "Train Loss: 0.784 | Val Loss: 0.667\n",
      "Iter: 439/1000 | Batch 000/017 | Cost: 1.2337\n",
      "Iter: 439/1000 | Batch 002/017 | Cost: 1.3412\n",
      "Train Loss: 0.568 | Val Loss: 0.463\n",
      "Iter: 440/1000 | Batch 000/017 | Cost: 2.2245\n",
      "Iter: 440/1000 | Batch 002/017 | Cost: 2.9007\n",
      "Train Loss: 1.093 | Val Loss: 0.583\n",
      "Iter: 441/1000 | Batch 000/017 | Cost: 1.9417\n",
      "Iter: 441/1000 | Batch 002/017 | Cost: 3.3944\n",
      "Train Loss: 0.370 | Val Loss: 0.744\n",
      "Iter: 442/1000 | Batch 000/017 | Cost: 3.1066\n",
      "Iter: 442/1000 | Batch 002/017 | Cost: 2.1565\n",
      "Train Loss: 0.452 | Val Loss: 0.306\n",
      "Iter: 443/1000 | Batch 000/017 | Cost: 1.4132\n",
      "Iter: 443/1000 | Batch 002/017 | Cost: 2.2713\n",
      "Train Loss: 0.558 | Val Loss: 0.428\n",
      "Iter: 444/1000 | Batch 000/017 | Cost: 2.9686\n",
      "Iter: 444/1000 | Batch 002/017 | Cost: 2.8613\n",
      "Train Loss: 0.277 | Val Loss: 0.138\n",
      "Iter: 445/1000 | Batch 000/017 | Cost: 1.5545\n",
      "Iter: 445/1000 | Batch 002/017 | Cost: 1.8739\n",
      "Train Loss: 0.420 | Val Loss: 0.694\n",
      "Iter: 446/1000 | Batch 000/017 | Cost: 2.1425\n",
      "Iter: 446/1000 | Batch 002/017 | Cost: 2.7095\n",
      "Train Loss: 0.848 | Val Loss: 0.423\n",
      "Iter: 447/1000 | Batch 000/017 | Cost: 3.7989\n",
      "Iter: 447/1000 | Batch 002/017 | Cost: 3.8006\n",
      "Train Loss: 0.729 | Val Loss: 0.210\n",
      "Iter: 448/1000 | Batch 000/017 | Cost: 2.5758\n",
      "Iter: 448/1000 | Batch 002/017 | Cost: 3.2780\n",
      "Train Loss: 0.727 | Val Loss: 0.360\n",
      "Iter: 449/1000 | Batch 000/017 | Cost: 1.6867\n",
      "Iter: 449/1000 | Batch 002/017 | Cost: 2.1681\n",
      "Train Loss: 0.794 | Val Loss: 0.474\n",
      "Iter: 450/1000 | Batch 000/017 | Cost: 1.2212\n",
      "Iter: 450/1000 | Batch 002/017 | Cost: 1.4846\n",
      "Train Loss: 0.413 | Val Loss: 0.593\n",
      "Iter: 451/1000 | Batch 000/017 | Cost: 1.9317\n",
      "Iter: 451/1000 | Batch 002/017 | Cost: 2.3623\n",
      "Train Loss: 0.326 | Val Loss: 0.632\n",
      "Iter: 452/1000 | Batch 000/017 | Cost: 2.3662\n",
      "Iter: 452/1000 | Batch 002/017 | Cost: 1.6859\n",
      "Train Loss: 0.323 | Val Loss: 0.514\n",
      "Iter: 453/1000 | Batch 000/017 | Cost: 3.0991\n",
      "Iter: 453/1000 | Batch 002/017 | Cost: 2.5516\n",
      "Train Loss: 0.636 | Val Loss: 0.166\n",
      "Iter: 454/1000 | Batch 000/017 | Cost: 2.2523\n",
      "Iter: 454/1000 | Batch 002/017 | Cost: 1.7375\n",
      "Train Loss: 0.590 | Val Loss: 0.180\n",
      "Iter: 455/1000 | Batch 000/017 | Cost: 1.6055\n",
      "Iter: 455/1000 | Batch 002/017 | Cost: 0.6892\n",
      "Train Loss: 0.247 | Val Loss: 0.525\n",
      "Iter: 456/1000 | Batch 000/017 | Cost: 2.5826\n",
      "Iter: 456/1000 | Batch 002/017 | Cost: 2.5189\n",
      "Train Loss: 0.328 | Val Loss: 0.551\n",
      "Iter: 457/1000 | Batch 000/017 | Cost: 2.3938\n",
      "Iter: 457/1000 | Batch 002/017 | Cost: 1.3078\n",
      "Train Loss: 0.457 | Val Loss: 0.372\n",
      "Iter: 458/1000 | Batch 000/017 | Cost: 2.4615\n",
      "Iter: 458/1000 | Batch 002/017 | Cost: 3.1358\n",
      "Train Loss: 0.657 | Val Loss: 0.124\n",
      "Iter: 459/1000 | Batch 000/017 | Cost: 2.1895\n",
      "Iter: 459/1000 | Batch 002/017 | Cost: 1.9882\n",
      "Train Loss: 0.517 | Val Loss: 0.600\n",
      "Iter: 460/1000 | Batch 000/017 | Cost: 2.1532\n",
      "Iter: 460/1000 | Batch 002/017 | Cost: 1.0367\n",
      "Train Loss: 0.492 | Val Loss: 0.315\n",
      "Iter: 461/1000 | Batch 000/017 | Cost: 0.8924\n",
      "Iter: 461/1000 | Batch 002/017 | Cost: 1.1112\n",
      "Train Loss: 0.826 | Val Loss: 0.538\n",
      "Iter: 462/1000 | Batch 000/017 | Cost: 1.2151\n",
      "Iter: 462/1000 | Batch 002/017 | Cost: 2.2635\n",
      "Train Loss: 0.549 | Val Loss: 0.462\n",
      "Iter: 463/1000 | Batch 000/017 | Cost: 3.2262\n",
      "Iter: 463/1000 | Batch 002/017 | Cost: 3.6423\n",
      "Train Loss: 0.458 | Val Loss: 0.381\n",
      "Iter: 464/1000 | Batch 000/017 | Cost: 3.9065\n",
      "Iter: 464/1000 | Batch 002/017 | Cost: 5.0396\n",
      "Train Loss: 0.153 | Val Loss: 0.424\n",
      "Iter: 465/1000 | Batch 000/017 | Cost: 3.2714\n",
      "Iter: 465/1000 | Batch 002/017 | Cost: 3.2108\n",
      "Train Loss: 0.306 | Val Loss: 0.701\n",
      "Iter: 466/1000 | Batch 000/017 | Cost: 3.3060\n",
      "Iter: 466/1000 | Batch 002/017 | Cost: 1.9782\n",
      "Train Loss: 0.655 | Val Loss: 0.529\n",
      "Iter: 467/1000 | Batch 000/017 | Cost: 3.4547\n",
      "Iter: 467/1000 | Batch 002/017 | Cost: 2.0944\n",
      "Train Loss: 0.672 | Val Loss: 0.605\n",
      "Iter: 468/1000 | Batch 000/017 | Cost: 2.7025\n",
      "Iter: 468/1000 | Batch 002/017 | Cost: 1.4628\n",
      "Train Loss: 0.216 | Val Loss: 0.220\n",
      "Iter: 469/1000 | Batch 000/017 | Cost: 0.7060\n",
      "Iter: 469/1000 | Batch 002/017 | Cost: 1.3551\n",
      "Train Loss: 0.173 | Val Loss: 0.702\n",
      "Iter: 470/1000 | Batch 000/017 | Cost: 4.5596\n",
      "Iter: 470/1000 | Batch 002/017 | Cost: 3.6569\n",
      "Train Loss: 0.318 | Val Loss: 1.079\n",
      "Iter: 471/1000 | Batch 000/017 | Cost: 4.2368\n",
      "Iter: 471/1000 | Batch 002/017 | Cost: 2.8184\n",
      "Train Loss: 0.970 | Val Loss: 0.565\n",
      "Iter: 472/1000 | Batch 000/017 | Cost: 2.4618\n",
      "Iter: 472/1000 | Batch 002/017 | Cost: 2.7394\n",
      "Train Loss: 0.474 | Val Loss: 0.374\n",
      "Iter: 473/1000 | Batch 000/017 | Cost: 2.0454\n",
      "Iter: 473/1000 | Batch 002/017 | Cost: 2.6453\n",
      "Train Loss: 0.336 | Val Loss: 0.511\n",
      "Iter: 474/1000 | Batch 000/017 | Cost: 3.0837\n",
      "Iter: 474/1000 | Batch 002/017 | Cost: 2.5662\n",
      "Train Loss: 0.518 | Val Loss: 0.193\n",
      "Iter: 475/1000 | Batch 000/017 | Cost: 2.2336\n",
      "Iter: 475/1000 | Batch 002/017 | Cost: 2.1816\n",
      "Train Loss: 0.609 | Val Loss: 0.540\n",
      "Iter: 476/1000 | Batch 000/017 | Cost: 3.4891\n",
      "Iter: 476/1000 | Batch 002/017 | Cost: 3.4865\n",
      "Train Loss: 0.491 | Val Loss: 0.443\n",
      "Iter: 477/1000 | Batch 000/017 | Cost: 2.6338\n",
      "Iter: 477/1000 | Batch 002/017 | Cost: 1.6594\n",
      "Train Loss: 0.504 | Val Loss: 0.461\n",
      "Iter: 478/1000 | Batch 000/017 | Cost: 1.3032\n",
      "Iter: 478/1000 | Batch 002/017 | Cost: 1.3079\n",
      "Train Loss: 0.484 | Val Loss: 0.507\n",
      "Iter: 479/1000 | Batch 000/017 | Cost: 3.6662\n",
      "Iter: 479/1000 | Batch 002/017 | Cost: 3.1775\n",
      "Train Loss: 0.462 | Val Loss: 0.359\n",
      "Iter: 480/1000 | Batch 000/017 | Cost: 3.7169\n",
      "Iter: 480/1000 | Batch 002/017 | Cost: 2.6443\n",
      "Train Loss: 0.610 | Val Loss: 0.589\n",
      "Iter: 481/1000 | Batch 000/017 | Cost: 3.6820\n",
      "Iter: 481/1000 | Batch 002/017 | Cost: 3.4878\n",
      "Train Loss: 0.397 | Val Loss: 0.316\n",
      "Iter: 482/1000 | Batch 000/017 | Cost: 1.2120\n",
      "Iter: 482/1000 | Batch 002/017 | Cost: 2.7718\n",
      "Train Loss: 0.390 | Val Loss: 0.725\n",
      "Iter: 483/1000 | Batch 000/017 | Cost: 0.7567\n",
      "Iter: 483/1000 | Batch 002/017 | Cost: 1.5030\n",
      "Train Loss: 0.531 | Val Loss: 0.762\n",
      "Iter: 484/1000 | Batch 000/017 | Cost: 2.0322\n",
      "Iter: 484/1000 | Batch 002/017 | Cost: 2.4369\n",
      "Train Loss: 0.684 | Val Loss: 0.423\n",
      "Iter: 485/1000 | Batch 000/017 | Cost: 4.8209\n",
      "Iter: 485/1000 | Batch 002/017 | Cost: 5.2015\n",
      "Train Loss: 0.406 | Val Loss: 0.217\n",
      "Iter: 486/1000 | Batch 000/017 | Cost: 2.3482\n",
      "Iter: 486/1000 | Batch 002/017 | Cost: 1.8633\n",
      "Train Loss: 0.225 | Val Loss: 0.454\n",
      "Iter: 487/1000 | Batch 000/017 | Cost: 1.8838\n",
      "Iter: 487/1000 | Batch 002/017 | Cost: 1.4843\n",
      "Train Loss: 0.783 | Val Loss: 0.735\n",
      "Iter: 488/1000 | Batch 000/017 | Cost: 2.0799\n",
      "Iter: 488/1000 | Batch 002/017 | Cost: 2.5801\n",
      "Train Loss: 0.471 | Val Loss: 0.452\n",
      "Iter: 489/1000 | Batch 000/017 | Cost: 1.4874\n",
      "Iter: 489/1000 | Batch 002/017 | Cost: 1.2298\n",
      "Train Loss: 0.729 | Val Loss: 0.448\n",
      "Iter: 490/1000 | Batch 000/017 | Cost: 2.0795\n",
      "Iter: 490/1000 | Batch 002/017 | Cost: 3.1817\n",
      "Train Loss: 0.678 | Val Loss: 0.764\n",
      "Iter: 491/1000 | Batch 000/017 | Cost: 2.5160\n",
      "Iter: 491/1000 | Batch 002/017 | Cost: 2.1980\n",
      "Train Loss: 0.401 | Val Loss: 0.507\n",
      "Iter: 492/1000 | Batch 000/017 | Cost: 2.8846\n",
      "Iter: 492/1000 | Batch 002/017 | Cost: 2.9556\n",
      "Train Loss: 0.730 | Val Loss: 0.591\n",
      "Iter: 493/1000 | Batch 000/017 | Cost: 2.2085\n",
      "Iter: 493/1000 | Batch 002/017 | Cost: 2.0062\n",
      "Train Loss: 0.534 | Val Loss: 0.476\n",
      "Iter: 494/1000 | Batch 000/017 | Cost: 2.2684\n",
      "Iter: 494/1000 | Batch 002/017 | Cost: 2.1021\n",
      "Train Loss: 0.551 | Val Loss: 0.361\n",
      "Iter: 495/1000 | Batch 000/017 | Cost: 3.2016\n",
      "Iter: 495/1000 | Batch 002/017 | Cost: 1.8495\n",
      "Train Loss: 0.592 | Val Loss: 0.486\n",
      "Iter: 496/1000 | Batch 000/017 | Cost: 1.5739\n",
      "Iter: 496/1000 | Batch 002/017 | Cost: 2.4437\n",
      "Train Loss: 0.983 | Val Loss: 0.493\n",
      "Iter: 497/1000 | Batch 000/017 | Cost: 2.5885\n",
      "Iter: 497/1000 | Batch 002/017 | Cost: 2.8851\n",
      "Train Loss: 0.492 | Val Loss: 0.365\n",
      "Iter: 498/1000 | Batch 000/017 | Cost: 2.5646\n",
      "Iter: 498/1000 | Batch 002/017 | Cost: 1.1766\n",
      "Train Loss: 0.710 | Val Loss: 0.492\n",
      "Iter: 499/1000 | Batch 000/017 | Cost: 3.2739\n",
      "Iter: 499/1000 | Batch 002/017 | Cost: 2.9410\n",
      "Train Loss: 0.511 | Val Loss: 0.853\n",
      "Iter: 500/1000 | Batch 000/017 | Cost: 1.5624\n",
      "Iter: 500/1000 | Batch 002/017 | Cost: 2.6216\n",
      "Train Loss: 0.422 | Val Loss: 0.820\n",
      "Iter: 501/1000 | Batch 000/017 | Cost: 2.9264\n",
      "Iter: 501/1000 | Batch 002/017 | Cost: 4.0768\n",
      "Train Loss: 0.326 | Val Loss: 0.572\n",
      "Iter: 502/1000 | Batch 000/017 | Cost: 4.8496\n",
      "Iter: 502/1000 | Batch 002/017 | Cost: 4.1350\n",
      "Train Loss: 0.494 | Val Loss: 0.792\n",
      "Iter: 503/1000 | Batch 000/017 | Cost: 4.0914\n",
      "Iter: 503/1000 | Batch 002/017 | Cost: 3.3496\n",
      "Train Loss: 0.726 | Val Loss: 0.702\n",
      "Iter: 504/1000 | Batch 000/017 | Cost: 1.9399\n",
      "Iter: 504/1000 | Batch 002/017 | Cost: 2.5664\n",
      "Train Loss: 0.836 | Val Loss: 0.624\n",
      "Iter: 505/1000 | Batch 000/017 | Cost: 1.9264\n",
      "Iter: 505/1000 | Batch 002/017 | Cost: 0.6157\n",
      "Train Loss: 0.375 | Val Loss: 0.216\n",
      "Iter: 506/1000 | Batch 000/017 | Cost: 5.5347\n",
      "Iter: 506/1000 | Batch 002/017 | Cost: 5.5617\n",
      "Train Loss: 0.354 | Val Loss: 0.374\n",
      "Iter: 507/1000 | Batch 000/017 | Cost: 3.4239\n",
      "Iter: 507/1000 | Batch 002/017 | Cost: 2.8676\n",
      "Train Loss: 0.532 | Val Loss: 0.973\n",
      "Iter: 508/1000 | Batch 000/017 | Cost: 3.1412\n",
      "Iter: 508/1000 | Batch 002/017 | Cost: 3.5299\n",
      "Train Loss: 0.548 | Val Loss: 0.459\n",
      "Iter: 509/1000 | Batch 000/017 | Cost: 2.5992\n",
      "Iter: 509/1000 | Batch 002/017 | Cost: 2.1151\n",
      "Train Loss: 0.917 | Val Loss: 0.733\n",
      "Iter: 510/1000 | Batch 000/017 | Cost: 2.0102\n",
      "Iter: 510/1000 | Batch 002/017 | Cost: 3.4304\n",
      "Train Loss: 0.541 | Val Loss: 0.602\n",
      "Iter: 511/1000 | Batch 000/017 | Cost: 3.2445\n",
      "Iter: 511/1000 | Batch 002/017 | Cost: 3.0622\n",
      "Train Loss: 0.546 | Val Loss: 0.329\n",
      "Iter: 512/1000 | Batch 000/017 | Cost: 2.7536\n",
      "Iter: 512/1000 | Batch 002/017 | Cost: 1.9485\n",
      "Train Loss: 0.334 | Val Loss: 0.311\n",
      "Iter: 513/1000 | Batch 000/017 | Cost: 5.5198\n",
      "Iter: 513/1000 | Batch 002/017 | Cost: 4.6911\n",
      "Train Loss: 0.408 | Val Loss: 0.528\n",
      "Iter: 514/1000 | Batch 000/017 | Cost: 4.3168\n",
      "Iter: 514/1000 | Batch 002/017 | Cost: 2.6567\n",
      "Train Loss: 0.801 | Val Loss: 0.521\n",
      "Iter: 515/1000 | Batch 000/017 | Cost: 3.1227\n",
      "Iter: 515/1000 | Batch 002/017 | Cost: 2.0406\n",
      "Train Loss: 0.293 | Val Loss: 0.444\n",
      "Iter: 516/1000 | Batch 000/017 | Cost: 3.4177\n",
      "Iter: 516/1000 | Batch 002/017 | Cost: 1.6141\n",
      "Train Loss: 1.031 | Val Loss: 0.855\n",
      "Iter: 517/1000 | Batch 000/017 | Cost: 4.6111\n",
      "Iter: 517/1000 | Batch 002/017 | Cost: 3.9716\n",
      "Train Loss: 0.303 | Val Loss: 0.815\n",
      "Iter: 518/1000 | Batch 000/017 | Cost: 2.7379\n",
      "Iter: 518/1000 | Batch 002/017 | Cost: 4.2589\n",
      "Train Loss: 0.342 | Val Loss: 0.263\n",
      "Iter: 519/1000 | Batch 000/017 | Cost: 2.5187\n",
      "Iter: 519/1000 | Batch 002/017 | Cost: 1.6683\n",
      "Train Loss: 0.496 | Val Loss: 0.531\n",
      "Iter: 520/1000 | Batch 000/017 | Cost: 2.6151\n",
      "Iter: 520/1000 | Batch 002/017 | Cost: 3.4497\n",
      "Train Loss: 0.550 | Val Loss: 0.328\n",
      "Iter: 521/1000 | Batch 000/017 | Cost: 2.4797\n",
      "Iter: 521/1000 | Batch 002/017 | Cost: 2.0213\n",
      "Train Loss: 0.241 | Val Loss: 1.060\n",
      "Iter: 522/1000 | Batch 000/017 | Cost: 3.5849\n",
      "Iter: 522/1000 | Batch 002/017 | Cost: 3.4417\n",
      "Train Loss: 0.398 | Val Loss: 0.510\n",
      "Iter: 523/1000 | Batch 000/017 | Cost: 1.5147\n",
      "Iter: 523/1000 | Batch 002/017 | Cost: 2.3373\n",
      "Train Loss: 0.429 | Val Loss: 0.805\n",
      "Iter: 524/1000 | Batch 000/017 | Cost: 4.1431\n",
      "Iter: 524/1000 | Batch 002/017 | Cost: 4.5388\n",
      "Train Loss: 0.885 | Val Loss: 0.379\n",
      "Iter: 525/1000 | Batch 000/017 | Cost: 2.8264\n",
      "Iter: 525/1000 | Batch 002/017 | Cost: 2.0003\n",
      "Train Loss: 0.570 | Val Loss: 0.466\n",
      "Iter: 526/1000 | Batch 000/017 | Cost: 2.3726\n",
      "Iter: 526/1000 | Batch 002/017 | Cost: 2.1501\n",
      "Train Loss: 0.359 | Val Loss: 0.775\n",
      "Iter: 527/1000 | Batch 000/017 | Cost: 2.0914\n",
      "Iter: 527/1000 | Batch 002/017 | Cost: 2.1910\n",
      "Train Loss: 0.730 | Val Loss: 0.704\n",
      "Iter: 528/1000 | Batch 000/017 | Cost: 3.1748\n",
      "Iter: 528/1000 | Batch 002/017 | Cost: 3.6186\n",
      "Train Loss: 0.776 | Val Loss: 0.583\n",
      "Iter: 529/1000 | Batch 000/017 | Cost: 2.8281\n",
      "Iter: 529/1000 | Batch 002/017 | Cost: 3.1291\n",
      "Train Loss: 0.428 | Val Loss: 0.570\n",
      "Iter: 530/1000 | Batch 000/017 | Cost: 1.5608\n",
      "Iter: 530/1000 | Batch 002/017 | Cost: 1.7710\n",
      "Train Loss: 0.494 | Val Loss: 0.325\n",
      "Iter: 531/1000 | Batch 000/017 | Cost: 4.4020\n",
      "Iter: 531/1000 | Batch 002/017 | Cost: 3.9053\n",
      "Train Loss: 0.563 | Val Loss: 0.317\n",
      "Iter: 532/1000 | Batch 000/017 | Cost: 0.8009\n",
      "Iter: 532/1000 | Batch 002/017 | Cost: 1.1190\n",
      "Train Loss: 0.246 | Val Loss: 0.143\n",
      "Iter: 533/1000 | Batch 000/017 | Cost: 2.6558\n",
      "Iter: 533/1000 | Batch 002/017 | Cost: 1.3512\n",
      "Train Loss: 0.469 | Val Loss: 0.244\n",
      "Iter: 534/1000 | Batch 000/017 | Cost: 1.5804\n",
      "Iter: 534/1000 | Batch 002/017 | Cost: 2.0769\n",
      "Train Loss: 0.520 | Val Loss: 0.169\n",
      "Iter: 535/1000 | Batch 000/017 | Cost: 0.8224\n",
      "Iter: 535/1000 | Batch 002/017 | Cost: 1.9107\n",
      "Train Loss: 0.566 | Val Loss: 0.559\n",
      "Iter: 536/1000 | Batch 000/017 | Cost: 2.5646\n",
      "Iter: 536/1000 | Batch 002/017 | Cost: 3.5038\n",
      "Train Loss: 0.636 | Val Loss: 0.319\n",
      "Iter: 537/1000 | Batch 000/017 | Cost: 2.6621\n",
      "Iter: 537/1000 | Batch 002/017 | Cost: 2.4722\n",
      "Train Loss: 0.736 | Val Loss: 0.443\n",
      "Iter: 538/1000 | Batch 000/017 | Cost: 3.1047\n",
      "Iter: 538/1000 | Batch 002/017 | Cost: 2.9465\n",
      "Train Loss: 0.383 | Val Loss: 0.334\n",
      "Iter: 539/1000 | Batch 000/017 | Cost: 3.6506\n",
      "Iter: 539/1000 | Batch 002/017 | Cost: 3.8643\n",
      "Train Loss: 0.673 | Val Loss: 0.124\n",
      "Iter: 540/1000 | Batch 000/017 | Cost: 2.0726\n",
      "Iter: 540/1000 | Batch 002/017 | Cost: 2.0973\n",
      "Train Loss: 0.684 | Val Loss: 0.475\n",
      "Iter: 541/1000 | Batch 000/017 | Cost: 3.0291\n",
      "Iter: 541/1000 | Batch 002/017 | Cost: 2.2877\n",
      "Train Loss: 1.016 | Val Loss: 0.189\n",
      "Iter: 542/1000 | Batch 000/017 | Cost: 1.7124\n",
      "Iter: 542/1000 | Batch 002/017 | Cost: 2.2896\n",
      "Train Loss: 0.661 | Val Loss: 0.451\n",
      "Iter: 543/1000 | Batch 000/017 | Cost: 3.3176\n",
      "Iter: 543/1000 | Batch 002/017 | Cost: 3.3487\n",
      "Train Loss: 0.420 | Val Loss: 0.357\n",
      "Iter: 544/1000 | Batch 000/017 | Cost: 1.4047\n",
      "Iter: 544/1000 | Batch 002/017 | Cost: 1.2664\n",
      "Train Loss: 0.685 | Val Loss: 0.375\n",
      "Iter: 545/1000 | Batch 000/017 | Cost: 3.1705\n",
      "Iter: 545/1000 | Batch 002/017 | Cost: 2.7512\n",
      "Train Loss: 0.315 | Val Loss: 0.361\n",
      "Iter: 546/1000 | Batch 000/017 | Cost: 3.1826\n",
      "Iter: 546/1000 | Batch 002/017 | Cost: 2.8661\n",
      "Train Loss: 0.505 | Val Loss: 0.657\n",
      "Iter: 547/1000 | Batch 000/017 | Cost: 2.8999\n",
      "Iter: 547/1000 | Batch 002/017 | Cost: 2.7718\n",
      "Train Loss: 0.508 | Val Loss: 0.397\n",
      "Iter: 548/1000 | Batch 000/017 | Cost: 3.2261\n",
      "Iter: 548/1000 | Batch 002/017 | Cost: 2.9164\n",
      "Train Loss: 0.353 | Val Loss: 0.648\n",
      "Iter: 549/1000 | Batch 000/017 | Cost: 2.1961\n",
      "Iter: 549/1000 | Batch 002/017 | Cost: 2.1128\n",
      "Train Loss: 0.604 | Val Loss: 0.579\n",
      "Iter: 550/1000 | Batch 000/017 | Cost: 2.5367\n",
      "Iter: 550/1000 | Batch 002/017 | Cost: 3.3925\n",
      "Train Loss: 0.434 | Val Loss: 0.363\n",
      "Iter: 551/1000 | Batch 000/017 | Cost: 2.8932\n",
      "Iter: 551/1000 | Batch 002/017 | Cost: 2.3547\n",
      "Train Loss: 0.899 | Val Loss: 0.653\n",
      "Iter: 552/1000 | Batch 000/017 | Cost: 5.2978\n",
      "Iter: 552/1000 | Batch 002/017 | Cost: 5.3158\n",
      "Train Loss: 0.401 | Val Loss: 0.472\n",
      "Iter: 553/1000 | Batch 000/017 | Cost: 3.4939\n",
      "Iter: 553/1000 | Batch 002/017 | Cost: 2.9745\n",
      "Train Loss: 0.764 | Val Loss: 0.479\n",
      "Iter: 554/1000 | Batch 000/017 | Cost: 2.8807\n",
      "Iter: 554/1000 | Batch 002/017 | Cost: 1.9267\n",
      "Train Loss: 0.764 | Val Loss: 0.443\n",
      "Iter: 555/1000 | Batch 000/017 | Cost: 2.2626\n",
      "Iter: 555/1000 | Batch 002/017 | Cost: 1.7144\n",
      "Train Loss: 1.066 | Val Loss: 0.467\n",
      "Iter: 556/1000 | Batch 000/017 | Cost: 2.4446\n",
      "Iter: 556/1000 | Batch 002/017 | Cost: 3.1247\n",
      "Train Loss: 0.908 | Val Loss: 1.216\n",
      "Iter: 557/1000 | Batch 000/017 | Cost: 3.2399\n",
      "Iter: 557/1000 | Batch 002/017 | Cost: 2.6296\n",
      "Train Loss: 0.818 | Val Loss: 0.634\n",
      "Iter: 558/1000 | Batch 000/017 | Cost: 4.2516\n",
      "Iter: 558/1000 | Batch 002/017 | Cost: 4.3070\n",
      "Train Loss: 0.475 | Val Loss: 1.141\n",
      "Iter: 559/1000 | Batch 000/017 | Cost: 3.2985\n",
      "Iter: 559/1000 | Batch 002/017 | Cost: 1.2992\n",
      "Train Loss: 0.598 | Val Loss: 0.473\n",
      "Iter: 560/1000 | Batch 000/017 | Cost: 1.5682\n",
      "Iter: 560/1000 | Batch 002/017 | Cost: 1.5991\n",
      "Train Loss: 0.599 | Val Loss: 0.228\n",
      "Iter: 561/1000 | Batch 000/017 | Cost: 1.7360\n",
      "Iter: 561/1000 | Batch 002/017 | Cost: 1.3514\n",
      "Train Loss: 0.874 | Val Loss: 0.453\n",
      "Iter: 562/1000 | Batch 000/017 | Cost: 1.5787\n",
      "Iter: 562/1000 | Batch 002/017 | Cost: 2.7880\n",
      "Train Loss: 1.063 | Val Loss: 0.387\n",
      "Iter: 563/1000 | Batch 000/017 | Cost: 2.1669\n",
      "Iter: 563/1000 | Batch 002/017 | Cost: 2.7443\n",
      "Train Loss: 0.439 | Val Loss: 1.015\n",
      "Iter: 564/1000 | Batch 000/017 | Cost: 1.8849\n",
      "Iter: 564/1000 | Batch 002/017 | Cost: 1.6462\n",
      "Train Loss: 0.901 | Val Loss: 0.369\n",
      "Iter: 565/1000 | Batch 000/017 | Cost: 1.6887\n",
      "Iter: 565/1000 | Batch 002/017 | Cost: 2.6526\n",
      "Train Loss: 0.780 | Val Loss: 0.983\n",
      "Iter: 566/1000 | Batch 000/017 | Cost: 1.3784\n",
      "Iter: 566/1000 | Batch 002/017 | Cost: 1.6851\n",
      "Train Loss: 0.777 | Val Loss: 0.710\n",
      "Iter: 567/1000 | Batch 000/017 | Cost: 3.3583\n",
      "Iter: 567/1000 | Batch 002/017 | Cost: 2.4027\n",
      "Train Loss: 0.873 | Val Loss: 0.813\n",
      "Iter: 568/1000 | Batch 000/017 | Cost: 2.9392\n",
      "Iter: 568/1000 | Batch 002/017 | Cost: 3.0202\n",
      "Train Loss: 0.470 | Val Loss: 0.237\n",
      "Iter: 569/1000 | Batch 000/017 | Cost: 3.3063\n",
      "Iter: 569/1000 | Batch 002/017 | Cost: 3.9051\n",
      "Train Loss: 0.903 | Val Loss: 0.535\n",
      "Iter: 570/1000 | Batch 000/017 | Cost: 1.5570\n",
      "Iter: 570/1000 | Batch 002/017 | Cost: 1.8294\n",
      "Train Loss: 1.017 | Val Loss: 0.398\n",
      "Iter: 571/1000 | Batch 000/017 | Cost: 2.1397\n",
      "Iter: 571/1000 | Batch 002/017 | Cost: 2.3290\n",
      "Train Loss: 0.614 | Val Loss: 0.378\n",
      "Iter: 572/1000 | Batch 000/017 | Cost: 2.4770\n",
      "Iter: 572/1000 | Batch 002/017 | Cost: 3.2286\n",
      "Train Loss: 0.995 | Val Loss: 0.472\n",
      "Iter: 573/1000 | Batch 000/017 | Cost: 1.8677\n",
      "Iter: 573/1000 | Batch 002/017 | Cost: 1.4565\n",
      "Train Loss: 0.706 | Val Loss: 0.425\n",
      "Iter: 574/1000 | Batch 000/017 | Cost: 2.0820\n",
      "Iter: 574/1000 | Batch 002/017 | Cost: 2.8218\n",
      "Train Loss: 1.021 | Val Loss: 0.188\n",
      "Iter: 575/1000 | Batch 000/017 | Cost: 0.5101\n",
      "Iter: 575/1000 | Batch 002/017 | Cost: 0.9597\n",
      "Train Loss: 0.313 | Val Loss: 0.670\n",
      "Iter: 576/1000 | Batch 000/017 | Cost: 2.7619\n",
      "Iter: 576/1000 | Batch 002/017 | Cost: 3.2547\n",
      "Train Loss: 0.712 | Val Loss: 0.332\n",
      "Iter: 577/1000 | Batch 000/017 | Cost: 1.7554\n",
      "Iter: 577/1000 | Batch 002/017 | Cost: 2.1722\n",
      "Train Loss: 1.004 | Val Loss: 1.099\n",
      "Iter: 578/1000 | Batch 000/017 | Cost: 3.7459\n",
      "Iter: 578/1000 | Batch 002/017 | Cost: 4.3104\n",
      "Train Loss: 0.508 | Val Loss: 0.555\n",
      "Iter: 579/1000 | Batch 000/017 | Cost: 2.8166\n",
      "Iter: 579/1000 | Batch 002/017 | Cost: 2.6129\n",
      "Train Loss: 0.700 | Val Loss: 0.555\n",
      "Iter: 580/1000 | Batch 000/017 | Cost: 3.2205\n",
      "Iter: 580/1000 | Batch 002/017 | Cost: 2.3285\n",
      "Train Loss: 0.636 | Val Loss: 0.803\n",
      "Iter: 581/1000 | Batch 000/017 | Cost: 3.8547\n",
      "Iter: 581/1000 | Batch 002/017 | Cost: 4.5663\n",
      "Train Loss: 0.760 | Val Loss: 0.558\n",
      "Iter: 582/1000 | Batch 000/017 | Cost: 2.2777\n",
      "Iter: 582/1000 | Batch 002/017 | Cost: 2.2178\n",
      "Train Loss: 0.363 | Val Loss: 0.525\n",
      "Iter: 583/1000 | Batch 000/017 | Cost: 4.1264\n",
      "Iter: 583/1000 | Batch 002/017 | Cost: 3.4494\n",
      "Train Loss: 0.550 | Val Loss: 0.644\n",
      "Iter: 584/1000 | Batch 000/017 | Cost: 3.3217\n",
      "Iter: 584/1000 | Batch 002/017 | Cost: 3.6437\n",
      "Train Loss: 0.684 | Val Loss: 0.875\n",
      "Iter: 585/1000 | Batch 000/017 | Cost: 3.0388\n",
      "Iter: 585/1000 | Batch 002/017 | Cost: 3.7400\n",
      "Train Loss: 0.906 | Val Loss: 0.645\n",
      "Iter: 586/1000 | Batch 000/017 | Cost: 3.5907\n",
      "Iter: 586/1000 | Batch 002/017 | Cost: 2.5649\n",
      "Train Loss: 0.459 | Val Loss: 0.454\n",
      "Iter: 587/1000 | Batch 000/017 | Cost: 1.4542\n",
      "Iter: 587/1000 | Batch 002/017 | Cost: 2.2250\n",
      "Train Loss: 0.884 | Val Loss: 0.860\n",
      "Iter: 588/1000 | Batch 000/017 | Cost: 1.9826\n",
      "Iter: 588/1000 | Batch 002/017 | Cost: 3.0765\n",
      "Train Loss: 0.890 | Val Loss: 0.682\n",
      "Iter: 589/1000 | Batch 000/017 | Cost: 3.8013\n",
      "Iter: 589/1000 | Batch 002/017 | Cost: 2.9536\n",
      "Train Loss: 0.968 | Val Loss: 0.319\n",
      "Iter: 590/1000 | Batch 000/017 | Cost: 2.5869\n",
      "Iter: 590/1000 | Batch 002/017 | Cost: 3.3640\n",
      "Train Loss: 0.707 | Val Loss: 0.361\n",
      "Iter: 591/1000 | Batch 000/017 | Cost: 2.1932\n",
      "Iter: 591/1000 | Batch 002/017 | Cost: 2.4844\n",
      "Train Loss: 0.455 | Val Loss: 0.457\n",
      "Iter: 592/1000 | Batch 000/017 | Cost: 2.1712\n",
      "Iter: 592/1000 | Batch 002/017 | Cost: 2.3531\n",
      "Train Loss: 0.239 | Val Loss: 0.266\n",
      "Iter: 593/1000 | Batch 000/017 | Cost: 2.8483\n",
      "Iter: 593/1000 | Batch 002/017 | Cost: 2.9451\n",
      "Train Loss: 0.483 | Val Loss: 0.158\n",
      "Iter: 594/1000 | Batch 000/017 | Cost: 3.3316\n",
      "Iter: 594/1000 | Batch 002/017 | Cost: 3.1205\n",
      "Train Loss: 0.375 | Val Loss: 0.580\n",
      "Iter: 595/1000 | Batch 000/017 | Cost: 4.0148\n",
      "Iter: 595/1000 | Batch 002/017 | Cost: 4.4748\n",
      "Train Loss: 0.252 | Val Loss: 0.262\n",
      "Iter: 596/1000 | Batch 000/017 | Cost: 3.6893\n",
      "Iter: 596/1000 | Batch 002/017 | Cost: 3.4839\n",
      "Train Loss: 1.065 | Val Loss: 0.478\n",
      "Iter: 597/1000 | Batch 000/017 | Cost: 0.6985\n",
      "Iter: 597/1000 | Batch 002/017 | Cost: 1.2944\n",
      "Train Loss: 0.648 | Val Loss: 0.591\n",
      "Iter: 598/1000 | Batch 000/017 | Cost: 2.5367\n",
      "Iter: 598/1000 | Batch 002/017 | Cost: 2.1904\n",
      "Train Loss: 0.733 | Val Loss: 0.491\n",
      "Iter: 599/1000 | Batch 000/017 | Cost: 2.5086\n",
      "Iter: 599/1000 | Batch 002/017 | Cost: 2.2512\n",
      "Train Loss: 1.139 | Val Loss: 0.296\n",
      "Iter: 600/1000 | Batch 000/017 | Cost: 2.9515\n",
      "Iter: 600/1000 | Batch 002/017 | Cost: 3.5230\n",
      "Train Loss: 0.627 | Val Loss: 0.765\n",
      "Iter: 601/1000 | Batch 000/017 | Cost: 2.4315\n",
      "Iter: 601/1000 | Batch 002/017 | Cost: 1.8426\n",
      "Train Loss: 0.682 | Val Loss: 0.522\n",
      "Iter: 602/1000 | Batch 000/017 | Cost: 2.1954\n",
      "Iter: 602/1000 | Batch 002/017 | Cost: 1.4006\n",
      "Train Loss: 0.579 | Val Loss: 0.290\n",
      "Iter: 603/1000 | Batch 000/017 | Cost: 1.8161\n",
      "Iter: 603/1000 | Batch 002/017 | Cost: 2.6333\n",
      "Train Loss: 0.974 | Val Loss: 0.817\n",
      "Iter: 604/1000 | Batch 000/017 | Cost: 5.1479\n",
      "Iter: 604/1000 | Batch 002/017 | Cost: 4.2324\n",
      "Train Loss: 0.362 | Val Loss: 0.311\n",
      "Iter: 605/1000 | Batch 000/017 | Cost: 2.9786\n",
      "Iter: 605/1000 | Batch 002/017 | Cost: 2.0035\n",
      "Train Loss: 0.441 | Val Loss: 0.716\n",
      "Iter: 606/1000 | Batch 000/017 | Cost: 4.3910\n",
      "Iter: 606/1000 | Batch 002/017 | Cost: 4.5704\n",
      "Train Loss: 0.308 | Val Loss: 0.691\n",
      "Iter: 607/1000 | Batch 000/017 | Cost: 3.2425\n",
      "Iter: 607/1000 | Batch 002/017 | Cost: 2.4234\n",
      "Train Loss: 0.545 | Val Loss: 0.676\n",
      "Iter: 608/1000 | Batch 000/017 | Cost: 4.0866\n",
      "Iter: 608/1000 | Batch 002/017 | Cost: 3.6550\n",
      "Train Loss: 0.641 | Val Loss: 1.000\n",
      "Iter: 609/1000 | Batch 000/017 | Cost: 1.1693\n",
      "Iter: 609/1000 | Batch 002/017 | Cost: 2.4273\n",
      "Train Loss: 0.419 | Val Loss: 1.161\n",
      "Iter: 610/1000 | Batch 000/017 | Cost: 2.7791\n",
      "Iter: 610/1000 | Batch 002/017 | Cost: 1.8898\n",
      "Train Loss: 0.500 | Val Loss: 0.678\n",
      "Iter: 611/1000 | Batch 000/017 | Cost: 1.0505\n",
      "Iter: 611/1000 | Batch 002/017 | Cost: 3.1333\n",
      "Train Loss: 0.571 | Val Loss: 0.394\n",
      "Iter: 612/1000 | Batch 000/017 | Cost: 2.9294\n",
      "Iter: 612/1000 | Batch 002/017 | Cost: 2.2266\n",
      "Train Loss: 0.559 | Val Loss: 0.605\n",
      "Iter: 613/1000 | Batch 000/017 | Cost: 1.7410\n",
      "Iter: 613/1000 | Batch 002/017 | Cost: 2.7148\n",
      "Train Loss: 0.540 | Val Loss: 0.368\n",
      "Iter: 614/1000 | Batch 000/017 | Cost: 3.8824\n",
      "Iter: 614/1000 | Batch 002/017 | Cost: 2.6006\n",
      "Train Loss: 0.630 | Val Loss: 0.239\n",
      "Iter: 615/1000 | Batch 000/017 | Cost: 2.6940\n",
      "Iter: 615/1000 | Batch 002/017 | Cost: 2.0751\n",
      "Train Loss: 0.298 | Val Loss: 0.342\n",
      "Iter: 616/1000 | Batch 000/017 | Cost: 3.5405\n",
      "Iter: 616/1000 | Batch 002/017 | Cost: 3.5403\n",
      "Train Loss: 0.322 | Val Loss: 0.385\n",
      "Iter: 617/1000 | Batch 000/017 | Cost: 2.3956\n",
      "Iter: 617/1000 | Batch 002/017 | Cost: 2.5953\n",
      "Train Loss: 0.560 | Val Loss: 0.558\n",
      "Iter: 618/1000 | Batch 000/017 | Cost: 2.8247\n",
      "Iter: 618/1000 | Batch 002/017 | Cost: 2.7318\n",
      "Train Loss: 0.376 | Val Loss: 0.672\n",
      "Iter: 619/1000 | Batch 000/017 | Cost: 2.2380\n",
      "Iter: 619/1000 | Batch 002/017 | Cost: 1.8940\n",
      "Train Loss: 0.687 | Val Loss: 0.826\n",
      "Iter: 620/1000 | Batch 000/017 | Cost: 4.7892\n",
      "Iter: 620/1000 | Batch 002/017 | Cost: 4.6672\n",
      "Train Loss: 0.327 | Val Loss: 0.484\n",
      "Iter: 621/1000 | Batch 000/017 | Cost: 2.7291\n",
      "Iter: 621/1000 | Batch 002/017 | Cost: 2.4841\n",
      "Train Loss: 0.273 | Val Loss: 0.456\n",
      "Iter: 622/1000 | Batch 000/017 | Cost: 2.1064\n",
      "Iter: 622/1000 | Batch 002/017 | Cost: 2.0271\n",
      "Train Loss: 0.525 | Val Loss: 0.720\n",
      "Iter: 623/1000 | Batch 000/017 | Cost: 3.8927\n",
      "Iter: 623/1000 | Batch 002/017 | Cost: 3.9837\n",
      "Train Loss: 0.938 | Val Loss: 0.495\n",
      "Iter: 624/1000 | Batch 000/017 | Cost: 1.7586\n",
      "Iter: 624/1000 | Batch 002/017 | Cost: 2.1142\n",
      "Train Loss: 0.483 | Val Loss: 0.439\n",
      "Iter: 625/1000 | Batch 000/017 | Cost: 3.1451\n",
      "Iter: 625/1000 | Batch 002/017 | Cost: 3.0766\n",
      "Train Loss: 0.997 | Val Loss: 0.658\n",
      "Iter: 626/1000 | Batch 000/017 | Cost: 3.2671\n",
      "Iter: 626/1000 | Batch 002/017 | Cost: 3.6588\n",
      "Train Loss: 0.398 | Val Loss: 0.733\n",
      "Iter: 627/1000 | Batch 000/017 | Cost: 2.4360\n",
      "Iter: 627/1000 | Batch 002/017 | Cost: 2.7784\n",
      "Train Loss: 0.590 | Val Loss: 0.764\n",
      "Iter: 628/1000 | Batch 000/017 | Cost: 1.2051\n",
      "Iter: 628/1000 | Batch 002/017 | Cost: 1.2432\n",
      "Train Loss: 0.619 | Val Loss: 0.618\n",
      "Iter: 629/1000 | Batch 000/017 | Cost: 3.1504\n",
      "Iter: 629/1000 | Batch 002/017 | Cost: 3.0327\n",
      "Train Loss: 0.583 | Val Loss: 0.433\n",
      "Iter: 630/1000 | Batch 000/017 | Cost: 4.0306\n",
      "Iter: 630/1000 | Batch 002/017 | Cost: 4.5550\n",
      "Train Loss: 0.812 | Val Loss: 0.691\n",
      "Iter: 631/1000 | Batch 000/017 | Cost: 2.8394\n",
      "Iter: 631/1000 | Batch 002/017 | Cost: 3.6779\n",
      "Train Loss: 1.046 | Val Loss: 0.562\n",
      "Iter: 632/1000 | Batch 000/017 | Cost: 5.0997\n",
      "Iter: 632/1000 | Batch 002/017 | Cost: 4.5739\n",
      "Train Loss: 0.618 | Val Loss: 0.378\n",
      "Iter: 633/1000 | Batch 000/017 | Cost: 1.9395\n",
      "Iter: 633/1000 | Batch 002/017 | Cost: 1.5932\n",
      "Train Loss: 0.613 | Val Loss: 0.443\n",
      "Iter: 634/1000 | Batch 000/017 | Cost: 1.0816\n",
      "Iter: 634/1000 | Batch 002/017 | Cost: 1.6069\n",
      "Train Loss: 0.637 | Val Loss: 0.436\n",
      "Iter: 635/1000 | Batch 000/017 | Cost: 2.5990\n",
      "Iter: 635/1000 | Batch 002/017 | Cost: 1.9111\n",
      "Train Loss: 0.649 | Val Loss: 0.392\n",
      "Iter: 636/1000 | Batch 000/017 | Cost: 1.9779\n",
      "Iter: 636/1000 | Batch 002/017 | Cost: 1.4886\n",
      "Train Loss: 0.536 | Val Loss: 0.499\n",
      "Iter: 637/1000 | Batch 000/017 | Cost: 2.8564\n",
      "Iter: 637/1000 | Batch 002/017 | Cost: 1.9667\n",
      "Train Loss: 0.361 | Val Loss: 0.432\n",
      "Iter: 638/1000 | Batch 000/017 | Cost: 3.2740\n",
      "Iter: 638/1000 | Batch 002/017 | Cost: 4.0504\n",
      "Train Loss: 0.356 | Val Loss: 0.781\n",
      "Iter: 639/1000 | Batch 000/017 | Cost: 2.3134\n",
      "Iter: 639/1000 | Batch 002/017 | Cost: 1.7447\n",
      "Train Loss: 0.892 | Val Loss: 0.220\n",
      "Iter: 640/1000 | Batch 000/017 | Cost: 3.8495\n",
      "Iter: 640/1000 | Batch 002/017 | Cost: 3.9121\n",
      "Train Loss: 0.309 | Val Loss: 0.549\n",
      "Iter: 641/1000 | Batch 000/017 | Cost: 1.7820\n",
      "Iter: 641/1000 | Batch 002/017 | Cost: 2.1745\n",
      "Train Loss: 0.602 | Val Loss: 0.326\n",
      "Iter: 642/1000 | Batch 000/017 | Cost: 3.6531\n",
      "Iter: 642/1000 | Batch 002/017 | Cost: 2.6515\n",
      "Train Loss: 0.943 | Val Loss: 0.548\n",
      "Iter: 643/1000 | Batch 000/017 | Cost: 2.7979\n",
      "Iter: 643/1000 | Batch 002/017 | Cost: 2.3238\n",
      "Train Loss: 0.642 | Val Loss: 0.260\n",
      "Iter: 644/1000 | Batch 000/017 | Cost: 4.1161\n",
      "Iter: 644/1000 | Batch 002/017 | Cost: 3.7666\n",
      "Train Loss: 0.502 | Val Loss: 0.859\n",
      "Iter: 645/1000 | Batch 000/017 | Cost: 1.8552\n",
      "Iter: 645/1000 | Batch 002/017 | Cost: 1.0367\n",
      "Train Loss: 0.430 | Val Loss: 0.564\n",
      "Iter: 646/1000 | Batch 000/017 | Cost: 2.0668\n",
      "Iter: 646/1000 | Batch 002/017 | Cost: 2.3728\n",
      "Train Loss: 0.539 | Val Loss: 0.432\n",
      "Iter: 647/1000 | Batch 000/017 | Cost: 1.4443\n",
      "Iter: 647/1000 | Batch 002/017 | Cost: 2.1826\n",
      "Train Loss: 0.627 | Val Loss: 0.612\n",
      "Iter: 648/1000 | Batch 000/017 | Cost: 3.1126\n",
      "Iter: 648/1000 | Batch 002/017 | Cost: 2.5594\n",
      "Train Loss: 0.975 | Val Loss: 0.837\n",
      "Iter: 649/1000 | Batch 000/017 | Cost: 2.5397\n",
      "Iter: 649/1000 | Batch 002/017 | Cost: 2.0734\n",
      "Train Loss: 0.856 | Val Loss: 0.161\n",
      "Iter: 650/1000 | Batch 000/017 | Cost: 1.7539\n",
      "Iter: 650/1000 | Batch 002/017 | Cost: 1.2333\n",
      "Train Loss: 0.636 | Val Loss: 0.620\n",
      "Iter: 651/1000 | Batch 000/017 | Cost: 2.3512\n",
      "Iter: 651/1000 | Batch 002/017 | Cost: 2.4449\n",
      "Train Loss: 0.632 | Val Loss: 0.764\n",
      "Iter: 652/1000 | Batch 000/017 | Cost: 1.8637\n",
      "Iter: 652/1000 | Batch 002/017 | Cost: 1.7884\n",
      "Train Loss: 0.728 | Val Loss: 0.952\n",
      "Iter: 653/1000 | Batch 000/017 | Cost: 1.4595\n",
      "Iter: 653/1000 | Batch 002/017 | Cost: 1.3581\n",
      "Train Loss: 0.736 | Val Loss: 0.666\n",
      "Iter: 654/1000 | Batch 000/017 | Cost: 1.4632\n",
      "Iter: 654/1000 | Batch 002/017 | Cost: 1.4796\n",
      "Train Loss: 0.647 | Val Loss: 0.511\n",
      "Iter: 655/1000 | Batch 000/017 | Cost: 2.1440\n",
      "Iter: 655/1000 | Batch 002/017 | Cost: 2.4459\n",
      "Train Loss: 0.485 | Val Loss: 0.813\n",
      "Iter: 656/1000 | Batch 000/017 | Cost: 2.3157\n",
      "Iter: 656/1000 | Batch 002/017 | Cost: 2.3405\n",
      "Train Loss: 0.670 | Val Loss: 1.074\n",
      "Iter: 657/1000 | Batch 000/017 | Cost: 2.2178\n",
      "Iter: 657/1000 | Batch 002/017 | Cost: 1.3074\n",
      "Train Loss: 0.490 | Val Loss: 0.501\n",
      "Iter: 658/1000 | Batch 000/017 | Cost: 1.8204\n",
      "Iter: 658/1000 | Batch 002/017 | Cost: 2.7397\n",
      "Train Loss: 0.547 | Val Loss: 1.095\n",
      "Iter: 659/1000 | Batch 000/017 | Cost: 3.5342\n",
      "Iter: 659/1000 | Batch 002/017 | Cost: 4.9905\n",
      "Train Loss: 0.712 | Val Loss: 0.302\n",
      "Iter: 660/1000 | Batch 000/017 | Cost: 3.7958\n",
      "Iter: 660/1000 | Batch 002/017 | Cost: 4.8867\n",
      "Train Loss: 0.988 | Val Loss: 0.450\n",
      "Iter: 661/1000 | Batch 000/017 | Cost: 0.8186\n",
      "Iter: 661/1000 | Batch 002/017 | Cost: 1.3391\n",
      "Train Loss: 0.309 | Val Loss: 0.578\n",
      "Iter: 662/1000 | Batch 000/017 | Cost: 2.9266\n",
      "Iter: 662/1000 | Batch 002/017 | Cost: 2.3621\n",
      "Train Loss: 0.836 | Val Loss: 0.399\n",
      "Iter: 663/1000 | Batch 000/017 | Cost: 1.2185\n",
      "Iter: 663/1000 | Batch 002/017 | Cost: 1.7040\n",
      "Train Loss: 0.666 | Val Loss: 0.379\n",
      "Iter: 664/1000 | Batch 000/017 | Cost: 3.8979\n",
      "Iter: 664/1000 | Batch 002/017 | Cost: 4.3681\n",
      "Train Loss: 0.936 | Val Loss: 0.557\n",
      "Iter: 665/1000 | Batch 000/017 | Cost: 4.5644\n",
      "Iter: 665/1000 | Batch 002/017 | Cost: 4.5511\n",
      "Train Loss: 0.230 | Val Loss: 0.553\n",
      "Iter: 666/1000 | Batch 000/017 | Cost: 4.4792\n",
      "Iter: 666/1000 | Batch 002/017 | Cost: 4.8098\n",
      "Train Loss: 0.566 | Val Loss: 0.678\n",
      "Iter: 667/1000 | Batch 000/017 | Cost: 1.1270\n",
      "Iter: 667/1000 | Batch 002/017 | Cost: 0.9830\n",
      "Train Loss: 0.790 | Val Loss: 0.446\n",
      "Iter: 668/1000 | Batch 000/017 | Cost: 2.2695\n",
      "Iter: 668/1000 | Batch 002/017 | Cost: 1.5061\n",
      "Train Loss: 0.308 | Val Loss: 0.473\n",
      "Iter: 669/1000 | Batch 000/017 | Cost: 3.9848\n",
      "Iter: 669/1000 | Batch 002/017 | Cost: 4.7794\n",
      "Train Loss: 0.262 | Val Loss: 0.833\n",
      "Iter: 670/1000 | Batch 000/017 | Cost: 1.9800\n",
      "Iter: 670/1000 | Batch 002/017 | Cost: 1.7772\n",
      "Train Loss: 0.436 | Val Loss: 0.267\n",
      "Iter: 671/1000 | Batch 000/017 | Cost: 3.3361\n",
      "Iter: 671/1000 | Batch 002/017 | Cost: 1.6364\n",
      "Train Loss: 0.891 | Val Loss: 0.269\n",
      "Iter: 672/1000 | Batch 000/017 | Cost: 3.5681\n",
      "Iter: 672/1000 | Batch 002/017 | Cost: 2.6027\n",
      "Train Loss: 0.768 | Val Loss: 0.397\n",
      "Iter: 673/1000 | Batch 000/017 | Cost: 1.5629\n",
      "Iter: 673/1000 | Batch 002/017 | Cost: 1.9225\n",
      "Train Loss: 0.668 | Val Loss: 0.428\n",
      "Iter: 674/1000 | Batch 000/017 | Cost: 1.5138\n",
      "Iter: 674/1000 | Batch 002/017 | Cost: 2.9044\n",
      "Train Loss: 0.468 | Val Loss: 0.326\n",
      "Iter: 675/1000 | Batch 000/017 | Cost: 4.1678\n",
      "Iter: 675/1000 | Batch 002/017 | Cost: 4.5214\n",
      "Train Loss: 0.988 | Val Loss: 0.527\n",
      "Iter: 676/1000 | Batch 000/017 | Cost: 3.6923\n",
      "Iter: 676/1000 | Batch 002/017 | Cost: 2.9189\n",
      "Train Loss: 0.511 | Val Loss: 0.868\n",
      "Iter: 677/1000 | Batch 000/017 | Cost: 2.4934\n",
      "Iter: 677/1000 | Batch 002/017 | Cost: 1.4319\n",
      "Train Loss: 0.620 | Val Loss: 0.343\n",
      "Iter: 678/1000 | Batch 000/017 | Cost: 4.2270\n",
      "Iter: 678/1000 | Batch 002/017 | Cost: 3.6032\n",
      "Train Loss: 0.321 | Val Loss: 0.527\n",
      "Iter: 679/1000 | Batch 000/017 | Cost: 1.7297\n",
      "Iter: 679/1000 | Batch 002/017 | Cost: 1.8021\n",
      "Train Loss: 0.397 | Val Loss: 0.307\n",
      "Iter: 680/1000 | Batch 000/017 | Cost: 3.5923\n",
      "Iter: 680/1000 | Batch 002/017 | Cost: 2.7384\n",
      "Train Loss: 0.477 | Val Loss: 0.569\n",
      "Iter: 681/1000 | Batch 000/017 | Cost: 3.4989\n",
      "Iter: 681/1000 | Batch 002/017 | Cost: 3.4192\n",
      "Train Loss: 0.440 | Val Loss: 0.801\n",
      "Iter: 682/1000 | Batch 000/017 | Cost: 2.8171\n",
      "Iter: 682/1000 | Batch 002/017 | Cost: 1.9876\n",
      "Train Loss: 0.429 | Val Loss: 0.478\n",
      "Iter: 683/1000 | Batch 000/017 | Cost: 1.3302\n",
      "Iter: 683/1000 | Batch 002/017 | Cost: 1.7641\n",
      "Train Loss: 0.567 | Val Loss: 0.702\n",
      "Iter: 684/1000 | Batch 000/017 | Cost: 3.1766\n",
      "Iter: 684/1000 | Batch 002/017 | Cost: 2.8318\n",
      "Train Loss: 0.441 | Val Loss: 0.675\n",
      "Iter: 685/1000 | Batch 000/017 | Cost: 3.8480\n",
      "Iter: 685/1000 | Batch 002/017 | Cost: 4.0800\n",
      "Train Loss: 0.913 | Val Loss: 0.423\n",
      "Iter: 686/1000 | Batch 000/017 | Cost: 2.3258\n",
      "Iter: 686/1000 | Batch 002/017 | Cost: 2.3990\n",
      "Train Loss: 0.450 | Val Loss: 0.285\n",
      "Iter: 687/1000 | Batch 000/017 | Cost: 2.1267\n",
      "Iter: 687/1000 | Batch 002/017 | Cost: 0.8827\n",
      "Train Loss: 0.780 | Val Loss: 0.619\n",
      "Iter: 688/1000 | Batch 000/017 | Cost: 3.5230\n",
      "Iter: 688/1000 | Batch 002/017 | Cost: 3.7166\n",
      "Train Loss: 0.414 | Val Loss: 0.218\n",
      "Iter: 689/1000 | Batch 000/017 | Cost: 3.6939\n",
      "Iter: 689/1000 | Batch 002/017 | Cost: 3.9221\n",
      "Train Loss: 0.583 | Val Loss: 0.749\n",
      "Iter: 690/1000 | Batch 000/017 | Cost: 2.5692\n",
      "Iter: 690/1000 | Batch 002/017 | Cost: 3.0194\n",
      "Train Loss: 0.900 | Val Loss: 0.229\n",
      "Iter: 691/1000 | Batch 000/017 | Cost: 2.2986\n",
      "Iter: 691/1000 | Batch 002/017 | Cost: 1.5210\n",
      "Train Loss: 0.351 | Val Loss: 0.317\n",
      "Iter: 692/1000 | Batch 000/017 | Cost: 3.0402\n",
      "Iter: 692/1000 | Batch 002/017 | Cost: 2.7683\n",
      "Train Loss: 0.488 | Val Loss: 0.977\n",
      "Iter: 693/1000 | Batch 000/017 | Cost: 4.4324\n",
      "Iter: 693/1000 | Batch 002/017 | Cost: 3.9296\n",
      "Train Loss: 0.518 | Val Loss: 0.547\n",
      "Iter: 694/1000 | Batch 000/017 | Cost: 2.5443\n",
      "Iter: 694/1000 | Batch 002/017 | Cost: 3.8232\n",
      "Train Loss: 0.673 | Val Loss: 0.624\n",
      "Iter: 695/1000 | Batch 000/017 | Cost: 5.5707\n",
      "Iter: 695/1000 | Batch 002/017 | Cost: 4.0907\n",
      "Train Loss: 0.259 | Val Loss: 0.640\n",
      "Iter: 696/1000 | Batch 000/017 | Cost: 3.0955\n",
      "Iter: 696/1000 | Batch 002/017 | Cost: 2.3924\n",
      "Train Loss: 0.734 | Val Loss: 0.212\n",
      "Iter: 697/1000 | Batch 000/017 | Cost: 2.3072\n",
      "Iter: 697/1000 | Batch 002/017 | Cost: 1.9768\n",
      "Train Loss: 0.488 | Val Loss: 0.341\n",
      "Iter: 698/1000 | Batch 000/017 | Cost: 3.2399\n",
      "Iter: 698/1000 | Batch 002/017 | Cost: 3.4572\n",
      "Train Loss: 0.616 | Val Loss: 0.634\n",
      "Iter: 699/1000 | Batch 000/017 | Cost: 2.3473\n",
      "Iter: 699/1000 | Batch 002/017 | Cost: 3.1409\n",
      "Train Loss: 0.568 | Val Loss: 1.105\n",
      "Iter: 700/1000 | Batch 000/017 | Cost: 1.9084\n",
      "Iter: 700/1000 | Batch 002/017 | Cost: 2.5193\n",
      "Train Loss: 0.491 | Val Loss: 0.587\n",
      "Iter: 701/1000 | Batch 000/017 | Cost: 2.2701\n",
      "Iter: 701/1000 | Batch 002/017 | Cost: 2.5599\n",
      "Train Loss: 0.352 | Val Loss: 1.182\n",
      "Iter: 702/1000 | Batch 000/017 | Cost: 2.0373\n",
      "Iter: 702/1000 | Batch 002/017 | Cost: 1.7946\n",
      "Train Loss: 0.845 | Val Loss: 0.947\n",
      "Iter: 703/1000 | Batch 000/017 | Cost: 4.4742\n",
      "Iter: 703/1000 | Batch 002/017 | Cost: 3.6964\n",
      "Train Loss: 0.462 | Val Loss: 0.253\n",
      "Iter: 704/1000 | Batch 000/017 | Cost: 1.6176\n",
      "Iter: 704/1000 | Batch 002/017 | Cost: 2.1592\n",
      "Train Loss: 0.453 | Val Loss: 1.044\n",
      "Iter: 705/1000 | Batch 000/017 | Cost: 1.6715\n",
      "Iter: 705/1000 | Batch 002/017 | Cost: 2.0246\n",
      "Train Loss: 0.313 | Val Loss: 0.720\n",
      "Iter: 706/1000 | Batch 000/017 | Cost: 2.7144\n",
      "Iter: 706/1000 | Batch 002/017 | Cost: 2.8905\n",
      "Train Loss: 0.610 | Val Loss: 0.413\n",
      "Iter: 707/1000 | Batch 000/017 | Cost: 2.2487\n",
      "Iter: 707/1000 | Batch 002/017 | Cost: 2.4866\n",
      "Train Loss: 0.373 | Val Loss: 0.452\n",
      "Iter: 708/1000 | Batch 000/017 | Cost: 2.8454\n",
      "Iter: 708/1000 | Batch 002/017 | Cost: 2.4454\n",
      "Train Loss: 0.582 | Val Loss: 0.962\n",
      "Iter: 709/1000 | Batch 000/017 | Cost: 1.9501\n",
      "Iter: 709/1000 | Batch 002/017 | Cost: 0.5698\n",
      "Train Loss: 0.466 | Val Loss: 0.612\n",
      "Iter: 710/1000 | Batch 000/017 | Cost: 2.0952\n",
      "Iter: 710/1000 | Batch 002/017 | Cost: 2.2372\n",
      "Train Loss: 0.342 | Val Loss: 0.742\n",
      "Iter: 711/1000 | Batch 000/017 | Cost: 3.3610\n",
      "Iter: 711/1000 | Batch 002/017 | Cost: 3.5027\n",
      "Train Loss: 0.366 | Val Loss: 0.297\n",
      "Iter: 712/1000 | Batch 000/017 | Cost: 2.6987\n",
      "Iter: 712/1000 | Batch 002/017 | Cost: 2.2555\n",
      "Train Loss: 0.640 | Val Loss: 0.322\n",
      "Iter: 713/1000 | Batch 000/017 | Cost: 2.7608\n",
      "Iter: 713/1000 | Batch 002/017 | Cost: 2.6351\n",
      "Train Loss: 0.336 | Val Loss: 0.711\n",
      "Iter: 714/1000 | Batch 000/017 | Cost: 1.9587\n",
      "Iter: 714/1000 | Batch 002/017 | Cost: 2.2614\n",
      "Train Loss: 0.680 | Val Loss: 0.640\n",
      "Iter: 715/1000 | Batch 000/017 | Cost: 4.6635\n",
      "Iter: 715/1000 | Batch 002/017 | Cost: 3.4378\n",
      "Train Loss: 0.469 | Val Loss: 0.811\n",
      "Iter: 716/1000 | Batch 000/017 | Cost: 2.0879\n",
      "Iter: 716/1000 | Batch 002/017 | Cost: 1.5686\n",
      "Train Loss: 0.429 | Val Loss: 0.329\n",
      "Iter: 717/1000 | Batch 000/017 | Cost: 2.6138\n",
      "Iter: 717/1000 | Batch 002/017 | Cost: 3.5276\n",
      "Train Loss: 0.862 | Val Loss: 0.458\n",
      "Iter: 718/1000 | Batch 000/017 | Cost: 3.0696\n",
      "Iter: 718/1000 | Batch 002/017 | Cost: 3.4470\n",
      "Train Loss: 0.379 | Val Loss: 0.439\n",
      "Iter: 719/1000 | Batch 000/017 | Cost: 3.2845\n",
      "Iter: 719/1000 | Batch 002/017 | Cost: 3.3947\n",
      "Train Loss: 0.827 | Val Loss: 0.316\n",
      "Iter: 720/1000 | Batch 000/017 | Cost: 4.0134\n",
      "Iter: 720/1000 | Batch 002/017 | Cost: 3.3319\n",
      "Train Loss: 0.617 | Val Loss: 0.656\n",
      "Iter: 721/1000 | Batch 000/017 | Cost: 2.2296\n",
      "Iter: 721/1000 | Batch 002/017 | Cost: 2.0305\n",
      "Train Loss: 0.470 | Val Loss: 0.626\n",
      "Iter: 722/1000 | Batch 000/017 | Cost: 1.9603\n",
      "Iter: 722/1000 | Batch 002/017 | Cost: 2.5581\n",
      "Train Loss: 0.558 | Val Loss: 0.464\n",
      "Iter: 723/1000 | Batch 000/017 | Cost: 2.0345\n",
      "Iter: 723/1000 | Batch 002/017 | Cost: 1.8176\n",
      "Train Loss: 0.722 | Val Loss: 0.468\n",
      "Iter: 724/1000 | Batch 000/017 | Cost: 3.7070\n",
      "Iter: 724/1000 | Batch 002/017 | Cost: 3.9546\n",
      "Train Loss: 0.543 | Val Loss: 0.155\n",
      "Iter: 725/1000 | Batch 000/017 | Cost: 2.8931\n",
      "Iter: 725/1000 | Batch 002/017 | Cost: 2.5705\n",
      "Train Loss: 0.213 | Val Loss: 0.307\n",
      "Iter: 726/1000 | Batch 000/017 | Cost: 1.0014\n",
      "Iter: 726/1000 | Batch 002/017 | Cost: 0.9976\n",
      "Train Loss: 0.438 | Val Loss: 0.180\n",
      "Iter: 727/1000 | Batch 000/017 | Cost: 1.2373\n",
      "Iter: 727/1000 | Batch 002/017 | Cost: 1.5292\n",
      "Train Loss: 0.637 | Val Loss: 0.371\n",
      "Iter: 728/1000 | Batch 000/017 | Cost: 2.1637\n",
      "Iter: 728/1000 | Batch 002/017 | Cost: 2.1990\n",
      "Train Loss: 0.658 | Val Loss: 0.844\n",
      "Iter: 729/1000 | Batch 000/017 | Cost: 1.9927\n",
      "Iter: 729/1000 | Batch 002/017 | Cost: 2.6365\n",
      "Train Loss: 0.833 | Val Loss: 0.292\n",
      "Iter: 730/1000 | Batch 000/017 | Cost: 4.5184\n",
      "Iter: 730/1000 | Batch 002/017 | Cost: 4.1429\n",
      "Train Loss: 0.371 | Val Loss: 0.652\n",
      "Iter: 731/1000 | Batch 000/017 | Cost: 3.8870\n",
      "Iter: 731/1000 | Batch 002/017 | Cost: 3.6119\n",
      "Train Loss: 0.908 | Val Loss: 0.309\n",
      "Iter: 732/1000 | Batch 000/017 | Cost: 1.0759\n",
      "Iter: 732/1000 | Batch 002/017 | Cost: 1.2208\n",
      "Train Loss: 0.515 | Val Loss: 0.258\n",
      "Iter: 733/1000 | Batch 000/017 | Cost: 2.7120\n",
      "Iter: 733/1000 | Batch 002/017 | Cost: 3.0157\n",
      "Train Loss: 0.449 | Val Loss: 0.648\n",
      "Iter: 734/1000 | Batch 000/017 | Cost: 4.5805\n",
      "Iter: 734/1000 | Batch 002/017 | Cost: 4.3462\n",
      "Train Loss: 0.567 | Val Loss: 0.605\n",
      "Iter: 735/1000 | Batch 000/017 | Cost: 1.8923\n",
      "Iter: 735/1000 | Batch 002/017 | Cost: 2.3717\n",
      "Train Loss: 0.424 | Val Loss: 0.643\n",
      "Iter: 736/1000 | Batch 000/017 | Cost: 4.0537\n",
      "Iter: 736/1000 | Batch 002/017 | Cost: 3.2776\n",
      "Train Loss: 0.488 | Val Loss: 0.400\n",
      "Iter: 737/1000 | Batch 000/017 | Cost: 3.9707\n",
      "Iter: 737/1000 | Batch 002/017 | Cost: 3.5276\n",
      "Train Loss: 0.315 | Val Loss: 0.505\n",
      "Iter: 738/1000 | Batch 000/017 | Cost: 1.7327\n",
      "Iter: 738/1000 | Batch 002/017 | Cost: 2.0561\n",
      "Train Loss: 0.863 | Val Loss: 0.625\n",
      "Iter: 739/1000 | Batch 000/017 | Cost: 4.9587\n",
      "Iter: 739/1000 | Batch 002/017 | Cost: 4.8055\n",
      "Train Loss: 0.782 | Val Loss: 0.283\n",
      "Iter: 740/1000 | Batch 000/017 | Cost: 2.4108\n",
      "Iter: 740/1000 | Batch 002/017 | Cost: 3.1570\n",
      "Train Loss: 0.334 | Val Loss: 0.453\n",
      "Iter: 741/1000 | Batch 000/017 | Cost: 2.3505\n",
      "Iter: 741/1000 | Batch 002/017 | Cost: 2.0493\n",
      "Train Loss: 0.270 | Val Loss: 0.548\n",
      "Iter: 742/1000 | Batch 000/017 | Cost: 1.0229\n",
      "Iter: 742/1000 | Batch 002/017 | Cost: 1.4975\n",
      "Train Loss: 0.636 | Val Loss: 0.326\n",
      "Iter: 743/1000 | Batch 000/017 | Cost: 2.4863\n",
      "Iter: 743/1000 | Batch 002/017 | Cost: 2.2805\n",
      "Train Loss: 0.538 | Val Loss: 0.418\n",
      "Iter: 744/1000 | Batch 000/017 | Cost: 1.6447\n",
      "Iter: 744/1000 | Batch 002/017 | Cost: 1.5344\n",
      "Train Loss: 0.502 | Val Loss: 0.513\n",
      "Iter: 745/1000 | Batch 000/017 | Cost: 3.5373\n",
      "Iter: 745/1000 | Batch 002/017 | Cost: 3.3246\n",
      "Train Loss: 0.211 | Val Loss: 0.359\n",
      "Iter: 746/1000 | Batch 000/017 | Cost: 3.4385\n",
      "Iter: 746/1000 | Batch 002/017 | Cost: 2.6994\n",
      "Train Loss: 0.376 | Val Loss: 0.299\n",
      "Iter: 747/1000 | Batch 000/017 | Cost: 3.4937\n",
      "Iter: 747/1000 | Batch 002/017 | Cost: 2.3184\n",
      "Train Loss: 0.582 | Val Loss: 0.411\n",
      "Iter: 748/1000 | Batch 000/017 | Cost: 2.2863\n",
      "Iter: 748/1000 | Batch 002/017 | Cost: 2.1678\n",
      "Train Loss: 0.778 | Val Loss: 0.557\n",
      "Iter: 749/1000 | Batch 000/017 | Cost: 0.7105\n",
      "Iter: 749/1000 | Batch 002/017 | Cost: 0.5526\n",
      "Train Loss: 0.546 | Val Loss: 0.549\n",
      "Iter: 750/1000 | Batch 000/017 | Cost: 2.9275\n",
      "Iter: 750/1000 | Batch 002/017 | Cost: 2.2028\n",
      "Train Loss: 0.370 | Val Loss: 0.837\n",
      "Iter: 751/1000 | Batch 000/017 | Cost: 3.0803\n",
      "Iter: 751/1000 | Batch 002/017 | Cost: 3.0748\n",
      "Train Loss: 0.341 | Val Loss: 0.730\n",
      "Iter: 752/1000 | Batch 000/017 | Cost: 3.8606\n",
      "Iter: 752/1000 | Batch 002/017 | Cost: 3.4310\n",
      "Train Loss: 0.441 | Val Loss: 0.637\n",
      "Iter: 753/1000 | Batch 000/017 | Cost: 2.9627\n",
      "Iter: 753/1000 | Batch 002/017 | Cost: 2.6415\n",
      "Train Loss: 1.224 | Val Loss: 0.387\n",
      "Iter: 754/1000 | Batch 000/017 | Cost: 4.1442\n",
      "Iter: 754/1000 | Batch 002/017 | Cost: 3.9888\n",
      "Train Loss: 0.998 | Val Loss: 0.759\n",
      "Iter: 755/1000 | Batch 000/017 | Cost: 3.4007\n",
      "Iter: 755/1000 | Batch 002/017 | Cost: 2.8379\n",
      "Train Loss: 0.955 | Val Loss: 0.329\n",
      "Iter: 756/1000 | Batch 000/017 | Cost: 3.1831\n",
      "Iter: 756/1000 | Batch 002/017 | Cost: 2.7949\n",
      "Train Loss: 0.441 | Val Loss: 0.810\n",
      "Iter: 757/1000 | Batch 000/017 | Cost: 4.1356\n",
      "Iter: 757/1000 | Batch 002/017 | Cost: 4.2744\n",
      "Train Loss: 0.327 | Val Loss: 0.965\n",
      "Iter: 758/1000 | Batch 000/017 | Cost: 2.7262\n",
      "Iter: 758/1000 | Batch 002/017 | Cost: 3.2791\n",
      "Train Loss: 0.634 | Val Loss: 0.179\n",
      "Iter: 759/1000 | Batch 000/017 | Cost: 2.7025\n",
      "Iter: 759/1000 | Batch 002/017 | Cost: 2.3009\n",
      "Train Loss: 0.409 | Val Loss: 0.614\n",
      "Iter: 760/1000 | Batch 000/017 | Cost: 2.6441\n",
      "Iter: 760/1000 | Batch 002/017 | Cost: 2.6652\n",
      "Train Loss: 0.603 | Val Loss: 0.554\n",
      "Iter: 761/1000 | Batch 000/017 | Cost: 1.9919\n",
      "Iter: 761/1000 | Batch 002/017 | Cost: 2.5689\n",
      "Train Loss: 0.815 | Val Loss: 0.620\n",
      "Iter: 762/1000 | Batch 000/017 | Cost: 3.5759\n",
      "Iter: 762/1000 | Batch 002/017 | Cost: 3.8629\n",
      "Train Loss: 0.587 | Val Loss: 0.550\n",
      "Iter: 763/1000 | Batch 000/017 | Cost: 2.1048\n",
      "Iter: 763/1000 | Batch 002/017 | Cost: 2.4442\n",
      "Train Loss: 0.452 | Val Loss: 0.279\n",
      "Iter: 764/1000 | Batch 000/017 | Cost: 1.9037\n",
      "Iter: 764/1000 | Batch 002/017 | Cost: 2.0025\n",
      "Train Loss: 0.987 | Val Loss: 0.761\n",
      "Iter: 765/1000 | Batch 000/017 | Cost: 2.3700\n",
      "Iter: 765/1000 | Batch 002/017 | Cost: 3.7789\n",
      "Train Loss: 0.424 | Val Loss: 0.852\n",
      "Iter: 766/1000 | Batch 000/017 | Cost: 2.5120\n",
      "Iter: 766/1000 | Batch 002/017 | Cost: 2.5654\n",
      "Train Loss: 0.887 | Val Loss: 0.693\n",
      "Iter: 767/1000 | Batch 000/017 | Cost: 1.7473\n",
      "Iter: 767/1000 | Batch 002/017 | Cost: 2.1247\n",
      "Train Loss: 0.654 | Val Loss: 0.393\n",
      "Iter: 768/1000 | Batch 000/017 | Cost: 2.8779\n",
      "Iter: 768/1000 | Batch 002/017 | Cost: 2.0513\n",
      "Train Loss: 0.356 | Val Loss: 0.690\n",
      "Iter: 769/1000 | Batch 000/017 | Cost: 3.3125\n",
      "Iter: 769/1000 | Batch 002/017 | Cost: 2.7134\n",
      "Train Loss: 0.980 | Val Loss: 0.359\n",
      "Iter: 770/1000 | Batch 000/017 | Cost: 1.8280\n",
      "Iter: 770/1000 | Batch 002/017 | Cost: 2.6902\n",
      "Train Loss: 0.680 | Val Loss: 0.373\n",
      "Iter: 771/1000 | Batch 000/017 | Cost: 1.5986\n",
      "Iter: 771/1000 | Batch 002/017 | Cost: 1.8614\n",
      "Train Loss: 0.511 | Val Loss: 0.525\n",
      "Iter: 772/1000 | Batch 000/017 | Cost: 1.1475\n",
      "Iter: 772/1000 | Batch 002/017 | Cost: 1.2990\n",
      "Train Loss: 0.748 | Val Loss: 0.385\n",
      "Iter: 773/1000 | Batch 000/017 | Cost: 3.3203\n",
      "Iter: 773/1000 | Batch 002/017 | Cost: 3.7324\n",
      "Train Loss: 0.732 | Val Loss: 0.467\n",
      "Iter: 774/1000 | Batch 000/017 | Cost: 2.1316\n",
      "Iter: 774/1000 | Batch 002/017 | Cost: 1.5644\n",
      "Train Loss: 0.297 | Val Loss: 0.489\n",
      "Iter: 775/1000 | Batch 000/017 | Cost: 2.1462\n",
      "Iter: 775/1000 | Batch 002/017 | Cost: 1.8935\n",
      "Train Loss: 0.395 | Val Loss: 0.333\n",
      "Iter: 776/1000 | Batch 000/017 | Cost: 1.9906\n",
      "Iter: 776/1000 | Batch 002/017 | Cost: 2.0728\n",
      "Train Loss: 0.560 | Val Loss: 0.494\n",
      "Iter: 777/1000 | Batch 000/017 | Cost: 1.5192\n",
      "Iter: 777/1000 | Batch 002/017 | Cost: 0.9765\n",
      "Train Loss: 0.839 | Val Loss: 0.539\n",
      "Iter: 778/1000 | Batch 000/017 | Cost: 1.9293\n",
      "Iter: 778/1000 | Batch 002/017 | Cost: 1.4791\n",
      "Train Loss: 0.437 | Val Loss: 0.556\n",
      "Iter: 779/1000 | Batch 000/017 | Cost: 0.9252\n",
      "Iter: 779/1000 | Batch 002/017 | Cost: 1.2795\n",
      "Train Loss: 0.305 | Val Loss: 0.450\n",
      "Iter: 780/1000 | Batch 000/017 | Cost: 1.7981\n",
      "Iter: 780/1000 | Batch 002/017 | Cost: 1.8266\n",
      "Train Loss: 0.531 | Val Loss: 0.690\n",
      "Iter: 781/1000 | Batch 000/017 | Cost: 2.7117\n",
      "Iter: 781/1000 | Batch 002/017 | Cost: 1.8054\n",
      "Train Loss: 0.816 | Val Loss: 1.139\n",
      "Iter: 782/1000 | Batch 000/017 | Cost: 1.9603\n",
      "Iter: 782/1000 | Batch 002/017 | Cost: 1.2854\n",
      "Train Loss: 0.624 | Val Loss: 0.656\n",
      "Iter: 783/1000 | Batch 000/017 | Cost: 1.7966\n",
      "Iter: 783/1000 | Batch 002/017 | Cost: 1.0077\n",
      "Train Loss: 0.638 | Val Loss: 0.609\n",
      "Iter: 784/1000 | Batch 000/017 | Cost: 2.5155\n",
      "Iter: 784/1000 | Batch 002/017 | Cost: 2.4326\n",
      "Train Loss: 0.538 | Val Loss: 0.871\n",
      "Iter: 785/1000 | Batch 000/017 | Cost: 1.9654\n",
      "Iter: 785/1000 | Batch 002/017 | Cost: 2.2956\n",
      "Train Loss: 0.594 | Val Loss: 0.440\n",
      "Iter: 786/1000 | Batch 000/017 | Cost: 1.9632\n",
      "Iter: 786/1000 | Batch 002/017 | Cost: 1.4992\n",
      "Train Loss: 0.519 | Val Loss: 0.501\n",
      "Iter: 787/1000 | Batch 000/017 | Cost: 1.5766\n",
      "Iter: 787/1000 | Batch 002/017 | Cost: 2.5080\n",
      "Train Loss: 0.878 | Val Loss: 0.532\n",
      "Iter: 788/1000 | Batch 000/017 | Cost: 1.6120\n",
      "Iter: 788/1000 | Batch 002/017 | Cost: 1.0144\n",
      "Train Loss: 0.908 | Val Loss: 0.696\n",
      "Iter: 789/1000 | Batch 000/017 | Cost: 2.4980\n",
      "Iter: 789/1000 | Batch 002/017 | Cost: 2.7871\n",
      "Train Loss: 0.520 | Val Loss: 0.670\n",
      "Iter: 790/1000 | Batch 000/017 | Cost: 0.9728\n",
      "Iter: 790/1000 | Batch 002/017 | Cost: 1.0620\n",
      "Train Loss: 0.903 | Val Loss: 0.646\n",
      "Iter: 791/1000 | Batch 000/017 | Cost: 4.4994\n",
      "Iter: 791/1000 | Batch 002/017 | Cost: 3.9146\n",
      "Train Loss: 0.589 | Val Loss: 0.383\n",
      "Iter: 792/1000 | Batch 000/017 | Cost: 1.2399\n",
      "Iter: 792/1000 | Batch 002/017 | Cost: 1.3170\n",
      "Train Loss: 0.565 | Val Loss: 0.522\n",
      "Iter: 793/1000 | Batch 000/017 | Cost: 1.5341\n",
      "Iter: 793/1000 | Batch 002/017 | Cost: 1.8241\n",
      "Train Loss: 0.473 | Val Loss: 0.341\n",
      "Iter: 794/1000 | Batch 000/017 | Cost: 3.6792\n",
      "Iter: 794/1000 | Batch 002/017 | Cost: 4.0000\n",
      "Train Loss: 0.682 | Val Loss: 0.388\n",
      "Iter: 795/1000 | Batch 000/017 | Cost: 2.7101\n",
      "Iter: 795/1000 | Batch 002/017 | Cost: 2.7244\n",
      "Train Loss: 0.835 | Val Loss: 0.750\n",
      "Iter: 796/1000 | Batch 000/017 | Cost: 3.2089\n",
      "Iter: 796/1000 | Batch 002/017 | Cost: 2.9964\n",
      "Train Loss: 0.768 | Val Loss: 0.822\n",
      "Iter: 797/1000 | Batch 000/017 | Cost: 1.5591\n",
      "Iter: 797/1000 | Batch 002/017 | Cost: 1.8421\n",
      "Train Loss: 0.679 | Val Loss: 0.529\n",
      "Iter: 798/1000 | Batch 000/017 | Cost: 2.4088\n",
      "Iter: 798/1000 | Batch 002/017 | Cost: 3.4312\n",
      "Train Loss: 0.680 | Val Loss: 0.658\n",
      "Iter: 799/1000 | Batch 000/017 | Cost: 4.9845\n",
      "Iter: 799/1000 | Batch 002/017 | Cost: 5.0764\n",
      "Train Loss: 1.003 | Val Loss: 0.479\n",
      "Iter: 800/1000 | Batch 000/017 | Cost: 2.6492\n",
      "Iter: 800/1000 | Batch 002/017 | Cost: 2.5812\n",
      "Train Loss: 0.349 | Val Loss: 0.795\n",
      "Iter: 801/1000 | Batch 000/017 | Cost: 2.9399\n",
      "Iter: 801/1000 | Batch 002/017 | Cost: 2.9061\n",
      "Train Loss: 0.553 | Val Loss: 0.590\n",
      "Iter: 802/1000 | Batch 000/017 | Cost: 2.2464\n",
      "Iter: 802/1000 | Batch 002/017 | Cost: 1.1397\n",
      "Train Loss: 0.327 | Val Loss: 0.920\n",
      "Iter: 803/1000 | Batch 000/017 | Cost: 3.3923\n",
      "Iter: 803/1000 | Batch 002/017 | Cost: 3.6919\n",
      "Train Loss: 0.363 | Val Loss: 0.894\n",
      "Iter: 804/1000 | Batch 000/017 | Cost: 2.9260\n",
      "Iter: 804/1000 | Batch 002/017 | Cost: 3.4004\n",
      "Train Loss: 0.863 | Val Loss: 0.447\n",
      "Iter: 805/1000 | Batch 000/017 | Cost: 2.3725\n",
      "Iter: 805/1000 | Batch 002/017 | Cost: 2.5530\n",
      "Train Loss: 0.756 | Val Loss: 0.750\n",
      "Iter: 806/1000 | Batch 000/017 | Cost: 3.7988\n",
      "Iter: 806/1000 | Batch 002/017 | Cost: 3.5075\n",
      "Train Loss: 0.784 | Val Loss: 0.188\n",
      "Iter: 807/1000 | Batch 000/017 | Cost: 1.1503\n",
      "Iter: 807/1000 | Batch 002/017 | Cost: 1.0440\n",
      "Train Loss: 0.463 | Val Loss: 1.239\n",
      "Iter: 808/1000 | Batch 000/017 | Cost: 3.2737\n",
      "Iter: 808/1000 | Batch 002/017 | Cost: 2.6185\n",
      "Train Loss: 0.395 | Val Loss: 0.543\n",
      "Iter: 809/1000 | Batch 000/017 | Cost: 2.0868\n",
      "Iter: 809/1000 | Batch 002/017 | Cost: 2.2776\n",
      "Train Loss: 0.449 | Val Loss: 0.195\n",
      "Iter: 810/1000 | Batch 000/017 | Cost: 3.6731\n",
      "Iter: 810/1000 | Batch 002/017 | Cost: 3.5942\n",
      "Train Loss: 0.743 | Val Loss: 0.846\n",
      "Iter: 811/1000 | Batch 000/017 | Cost: 1.5293\n",
      "Iter: 811/1000 | Batch 002/017 | Cost: 1.0996\n",
      "Train Loss: 0.352 | Val Loss: 0.655\n",
      "Iter: 812/1000 | Batch 000/017 | Cost: 1.5836\n",
      "Iter: 812/1000 | Batch 002/017 | Cost: 2.1887\n",
      "Train Loss: 0.856 | Val Loss: 0.720\n",
      "Iter: 813/1000 | Batch 000/017 | Cost: 3.1568\n",
      "Iter: 813/1000 | Batch 002/017 | Cost: 2.5267\n",
      "Train Loss: 0.785 | Val Loss: 0.693\n",
      "Iter: 814/1000 | Batch 000/017 | Cost: 4.2083\n",
      "Iter: 814/1000 | Batch 002/017 | Cost: 2.8199\n",
      "Train Loss: 0.490 | Val Loss: 0.477\n",
      "Iter: 815/1000 | Batch 000/017 | Cost: 2.2718\n",
      "Iter: 815/1000 | Batch 002/017 | Cost: 2.0889\n",
      "Train Loss: 0.599 | Val Loss: 0.784\n",
      "Iter: 816/1000 | Batch 000/017 | Cost: 1.7522\n",
      "Iter: 816/1000 | Batch 002/017 | Cost: 1.1864\n",
      "Train Loss: 0.722 | Val Loss: 0.496\n",
      "Iter: 817/1000 | Batch 000/017 | Cost: 2.6117\n",
      "Iter: 817/1000 | Batch 002/017 | Cost: 3.1965\n",
      "Train Loss: 0.687 | Val Loss: 0.671\n",
      "Iter: 818/1000 | Batch 000/017 | Cost: 4.3666\n",
      "Iter: 818/1000 | Batch 002/017 | Cost: 3.4777\n",
      "Train Loss: 0.802 | Val Loss: 0.776\n",
      "Iter: 819/1000 | Batch 000/017 | Cost: 3.9489\n",
      "Iter: 819/1000 | Batch 002/017 | Cost: 3.9465\n",
      "Train Loss: 0.618 | Val Loss: 1.077\n",
      "Iter: 820/1000 | Batch 000/017 | Cost: 4.3160\n",
      "Iter: 820/1000 | Batch 002/017 | Cost: 3.0366\n",
      "Train Loss: 0.603 | Val Loss: 0.622\n",
      "Iter: 821/1000 | Batch 000/017 | Cost: 1.1007\n",
      "Iter: 821/1000 | Batch 002/017 | Cost: 1.3080\n",
      "Train Loss: 0.676 | Val Loss: 0.823\n",
      "Iter: 822/1000 | Batch 000/017 | Cost: 2.9510\n",
      "Iter: 822/1000 | Batch 002/017 | Cost: 2.3826\n",
      "Train Loss: 0.723 | Val Loss: 0.496\n",
      "Iter: 823/1000 | Batch 000/017 | Cost: 3.8356\n",
      "Iter: 823/1000 | Batch 002/017 | Cost: 3.6349\n",
      "Train Loss: 0.385 | Val Loss: 0.382\n",
      "Iter: 824/1000 | Batch 000/017 | Cost: 1.9367\n",
      "Iter: 824/1000 | Batch 002/017 | Cost: 2.5433\n",
      "Train Loss: 0.868 | Val Loss: 0.287\n",
      "Iter: 825/1000 | Batch 000/017 | Cost: 2.4149\n",
      "Iter: 825/1000 | Batch 002/017 | Cost: 2.2117\n",
      "Train Loss: 0.836 | Val Loss: 0.617\n",
      "Iter: 826/1000 | Batch 000/017 | Cost: 2.4329\n",
      "Iter: 826/1000 | Batch 002/017 | Cost: 2.3317\n",
      "Train Loss: 0.388 | Val Loss: 0.388\n",
      "Iter: 827/1000 | Batch 000/017 | Cost: 3.4876\n",
      "Iter: 827/1000 | Batch 002/017 | Cost: 3.4373\n",
      "Train Loss: 0.848 | Val Loss: 0.564\n",
      "Iter: 828/1000 | Batch 000/017 | Cost: 1.5936\n",
      "Iter: 828/1000 | Batch 002/017 | Cost: 2.1995\n",
      "Train Loss: 0.635 | Val Loss: 0.342\n",
      "Iter: 829/1000 | Batch 000/017 | Cost: 3.3405\n",
      "Iter: 829/1000 | Batch 002/017 | Cost: 2.3738\n",
      "Train Loss: 0.261 | Val Loss: 0.306\n",
      "Iter: 830/1000 | Batch 000/017 | Cost: 1.5633\n",
      "Iter: 830/1000 | Batch 002/017 | Cost: 1.5712\n",
      "Train Loss: 0.835 | Val Loss: 0.354\n",
      "Iter: 831/1000 | Batch 000/017 | Cost: 3.3693\n",
      "Iter: 831/1000 | Batch 002/017 | Cost: 4.7009\n",
      "Train Loss: 0.792 | Val Loss: 0.876\n",
      "Iter: 832/1000 | Batch 000/017 | Cost: 3.5741\n",
      "Iter: 832/1000 | Batch 002/017 | Cost: 3.2893\n",
      "Train Loss: 0.422 | Val Loss: 0.350\n",
      "Iter: 833/1000 | Batch 000/017 | Cost: 4.2741\n",
      "Iter: 833/1000 | Batch 002/017 | Cost: 3.2438\n",
      "Train Loss: 0.394 | Val Loss: 0.534\n",
      "Iter: 834/1000 | Batch 000/017 | Cost: 2.2845\n",
      "Iter: 834/1000 | Batch 002/017 | Cost: 0.9152\n",
      "Train Loss: 0.597 | Val Loss: 0.303\n",
      "Iter: 835/1000 | Batch 000/017 | Cost: 3.0567\n",
      "Iter: 835/1000 | Batch 002/017 | Cost: 2.7048\n",
      "Train Loss: 0.328 | Val Loss: 0.709\n",
      "Iter: 836/1000 | Batch 000/017 | Cost: 3.6750\n",
      "Iter: 836/1000 | Batch 002/017 | Cost: 4.0726\n",
      "Train Loss: 0.774 | Val Loss: 0.622\n",
      "Iter: 837/1000 | Batch 000/017 | Cost: 2.2186\n",
      "Iter: 837/1000 | Batch 002/017 | Cost: 3.3116\n",
      "Train Loss: 1.120 | Val Loss: 0.327\n",
      "Iter: 838/1000 | Batch 000/017 | Cost: 1.7695\n",
      "Iter: 838/1000 | Batch 002/017 | Cost: 1.3702\n",
      "Train Loss: 0.471 | Val Loss: 0.482\n",
      "Iter: 839/1000 | Batch 000/017 | Cost: 1.0516\n",
      "Iter: 839/1000 | Batch 002/017 | Cost: 2.0125\n",
      "Train Loss: 0.859 | Val Loss: 0.569\n",
      "Iter: 840/1000 | Batch 000/017 | Cost: 3.5126\n",
      "Iter: 840/1000 | Batch 002/017 | Cost: 3.5644\n",
      "Train Loss: 0.514 | Val Loss: 0.871\n",
      "Iter: 841/1000 | Batch 000/017 | Cost: 3.2803\n",
      "Iter: 841/1000 | Batch 002/017 | Cost: 4.0806\n",
      "Train Loss: 0.768 | Val Loss: 0.532\n",
      "Iter: 842/1000 | Batch 000/017 | Cost: 4.0582\n",
      "Iter: 842/1000 | Batch 002/017 | Cost: 3.7054\n",
      "Train Loss: 0.322 | Val Loss: 0.561\n",
      "Iter: 843/1000 | Batch 000/017 | Cost: 3.4047\n",
      "Iter: 843/1000 | Batch 002/017 | Cost: 2.9458\n",
      "Train Loss: 0.782 | Val Loss: 0.607\n",
      "Iter: 844/1000 | Batch 000/017 | Cost: 3.2518\n",
      "Iter: 844/1000 | Batch 002/017 | Cost: 3.2271\n",
      "Train Loss: 0.387 | Val Loss: 1.073\n",
      "Iter: 845/1000 | Batch 000/017 | Cost: 1.6713\n",
      "Iter: 845/1000 | Batch 002/017 | Cost: 2.3469\n",
      "Train Loss: 1.113 | Val Loss: 0.448\n",
      "Iter: 846/1000 | Batch 000/017 | Cost: 3.4617\n",
      "Iter: 846/1000 | Batch 002/017 | Cost: 3.4564\n",
      "Train Loss: 0.846 | Val Loss: 0.701\n",
      "Iter: 847/1000 | Batch 000/017 | Cost: 1.6518\n",
      "Iter: 847/1000 | Batch 002/017 | Cost: 1.7843\n",
      "Train Loss: 0.487 | Val Loss: 0.353\n",
      "Iter: 848/1000 | Batch 000/017 | Cost: 4.0616\n",
      "Iter: 848/1000 | Batch 002/017 | Cost: 4.2352\n",
      "Train Loss: 0.569 | Val Loss: 0.814\n",
      "Iter: 849/1000 | Batch 000/017 | Cost: 1.6654\n",
      "Iter: 849/1000 | Batch 002/017 | Cost: 2.5863\n",
      "Train Loss: 0.793 | Val Loss: 0.667\n",
      "Iter: 850/1000 | Batch 000/017 | Cost: 3.4725\n",
      "Iter: 850/1000 | Batch 002/017 | Cost: 2.6953\n",
      "Train Loss: 0.670 | Val Loss: 0.850\n",
      "Iter: 851/1000 | Batch 000/017 | Cost: 2.5606\n",
      "Iter: 851/1000 | Batch 002/017 | Cost: 2.7955\n",
      "Train Loss: 0.418 | Val Loss: 1.009\n",
      "Iter: 852/1000 | Batch 000/017 | Cost: 2.3069\n",
      "Iter: 852/1000 | Batch 002/017 | Cost: 2.0168\n",
      "Train Loss: 0.683 | Val Loss: 0.436\n",
      "Iter: 853/1000 | Batch 000/017 | Cost: 1.8973\n",
      "Iter: 853/1000 | Batch 002/017 | Cost: 2.1954\n",
      "Train Loss: 0.365 | Val Loss: 0.416\n",
      "Iter: 854/1000 | Batch 000/017 | Cost: 2.5228\n",
      "Iter: 854/1000 | Batch 002/017 | Cost: 0.9400\n",
      "Train Loss: 0.665 | Val Loss: 0.704\n",
      "Iter: 855/1000 | Batch 000/017 | Cost: 2.1340\n",
      "Iter: 855/1000 | Batch 002/017 | Cost: 1.6362\n",
      "Train Loss: 0.363 | Val Loss: 0.781\n",
      "Iter: 856/1000 | Batch 000/017 | Cost: 2.1817\n",
      "Iter: 856/1000 | Batch 002/017 | Cost: 1.5345\n",
      "Train Loss: 0.156 | Val Loss: 0.641\n",
      "Iter: 857/1000 | Batch 000/017 | Cost: 1.1240\n",
      "Iter: 857/1000 | Batch 002/017 | Cost: 1.7280\n",
      "Train Loss: 0.417 | Val Loss: 1.053\n",
      "Iter: 858/1000 | Batch 000/017 | Cost: 2.9983\n",
      "Iter: 858/1000 | Batch 002/017 | Cost: 3.7354\n",
      "Train Loss: 0.588 | Val Loss: 1.103\n",
      "Iter: 859/1000 | Batch 000/017 | Cost: 4.5975\n",
      "Iter: 859/1000 | Batch 002/017 | Cost: 2.7906\n",
      "Train Loss: 0.345 | Val Loss: 0.803\n",
      "Iter: 860/1000 | Batch 000/017 | Cost: 3.1258\n",
      "Iter: 860/1000 | Batch 002/017 | Cost: 3.3479\n",
      "Train Loss: 1.087 | Val Loss: 0.827\n",
      "Iter: 861/1000 | Batch 000/017 | Cost: 1.5830\n",
      "Iter: 861/1000 | Batch 002/017 | Cost: 1.6839\n",
      "Train Loss: 0.754 | Val Loss: 0.430\n",
      "Iter: 862/1000 | Batch 000/017 | Cost: 4.6523\n",
      "Iter: 862/1000 | Batch 002/017 | Cost: 3.6867\n",
      "Train Loss: 0.405 | Val Loss: 0.338\n",
      "Iter: 863/1000 | Batch 000/017 | Cost: 1.8629\n",
      "Iter: 863/1000 | Batch 002/017 | Cost: 1.5003\n",
      "Train Loss: 0.669 | Val Loss: 0.302\n",
      "Iter: 864/1000 | Batch 000/017 | Cost: 0.9590\n",
      "Iter: 864/1000 | Batch 002/017 | Cost: 1.3627\n",
      "Train Loss: 0.383 | Val Loss: 0.712\n",
      "Iter: 865/1000 | Batch 000/017 | Cost: 1.3875\n",
      "Iter: 865/1000 | Batch 002/017 | Cost: 0.8418\n",
      "Train Loss: 0.285 | Val Loss: 0.742\n",
      "Iter: 866/1000 | Batch 000/017 | Cost: 3.0350\n",
      "Iter: 866/1000 | Batch 002/017 | Cost: 1.6774\n",
      "Train Loss: 0.633 | Val Loss: 0.364\n",
      "Iter: 867/1000 | Batch 000/017 | Cost: 3.1772\n",
      "Iter: 867/1000 | Batch 002/017 | Cost: 2.4246\n",
      "Train Loss: 0.476 | Val Loss: 0.928\n",
      "Iter: 868/1000 | Batch 000/017 | Cost: 4.3975\n",
      "Iter: 868/1000 | Batch 002/017 | Cost: 3.8050\n",
      "Train Loss: 0.433 | Val Loss: 0.368\n",
      "Iter: 869/1000 | Batch 000/017 | Cost: 4.3388\n",
      "Iter: 869/1000 | Batch 002/017 | Cost: 4.1284\n",
      "Train Loss: 0.873 | Val Loss: 0.739\n",
      "Iter: 870/1000 | Batch 000/017 | Cost: 1.3642\n",
      "Iter: 870/1000 | Batch 002/017 | Cost: 1.1550\n",
      "Train Loss: 0.524 | Val Loss: 0.490\n",
      "Iter: 871/1000 | Batch 000/017 | Cost: 2.8562\n",
      "Iter: 871/1000 | Batch 002/017 | Cost: 2.8733\n",
      "Train Loss: 0.566 | Val Loss: 1.251\n",
      "Iter: 872/1000 | Batch 000/017 | Cost: 1.9463\n",
      "Iter: 872/1000 | Batch 002/017 | Cost: 1.8039\n",
      "Train Loss: 0.548 | Val Loss: 0.917\n",
      "Iter: 873/1000 | Batch 000/017 | Cost: 2.9268\n",
      "Iter: 873/1000 | Batch 002/017 | Cost: 4.3974\n",
      "Train Loss: 0.525 | Val Loss: 0.358\n",
      "Iter: 874/1000 | Batch 000/017 | Cost: 1.8208\n",
      "Iter: 874/1000 | Batch 002/017 | Cost: 2.0967\n",
      "Train Loss: 0.296 | Val Loss: 0.284\n",
      "Iter: 875/1000 | Batch 000/017 | Cost: 2.8258\n",
      "Iter: 875/1000 | Batch 002/017 | Cost: 3.7525\n",
      "Train Loss: 0.882 | Val Loss: 0.313\n",
      "Iter: 876/1000 | Batch 000/017 | Cost: 3.3723\n",
      "Iter: 876/1000 | Batch 002/017 | Cost: 3.8198\n",
      "Train Loss: 0.272 | Val Loss: 0.861\n",
      "Iter: 877/1000 | Batch 000/017 | Cost: 1.8084\n",
      "Iter: 877/1000 | Batch 002/017 | Cost: 2.2941\n",
      "Train Loss: 0.330 | Val Loss: 0.460\n",
      "Iter: 878/1000 | Batch 000/017 | Cost: 4.7911\n",
      "Iter: 878/1000 | Batch 002/017 | Cost: 4.5128\n",
      "Train Loss: 0.308 | Val Loss: 0.535\n",
      "Iter: 879/1000 | Batch 000/017 | Cost: 2.4646\n",
      "Iter: 879/1000 | Batch 002/017 | Cost: 2.4404\n",
      "Train Loss: 0.222 | Val Loss: 1.125\n",
      "Iter: 880/1000 | Batch 000/017 | Cost: 3.7092\n",
      "Iter: 880/1000 | Batch 002/017 | Cost: 2.6628\n",
      "Train Loss: 0.592 | Val Loss: 0.520\n",
      "Iter: 881/1000 | Batch 000/017 | Cost: 3.8144\n",
      "Iter: 881/1000 | Batch 002/017 | Cost: 4.2715\n",
      "Train Loss: 0.390 | Val Loss: 0.304\n",
      "Iter: 882/1000 | Batch 000/017 | Cost: 2.3603\n",
      "Iter: 882/1000 | Batch 002/017 | Cost: 2.4304\n",
      "Train Loss: 0.602 | Val Loss: 0.774\n",
      "Iter: 883/1000 | Batch 000/017 | Cost: 4.5377\n",
      "Iter: 883/1000 | Batch 002/017 | Cost: 3.4048\n",
      "Train Loss: 0.449 | Val Loss: 0.647\n",
      "Iter: 884/1000 | Batch 000/017 | Cost: 2.1372\n",
      "Iter: 884/1000 | Batch 002/017 | Cost: 1.9269\n",
      "Train Loss: 0.529 | Val Loss: 0.462\n",
      "Iter: 885/1000 | Batch 000/017 | Cost: 1.7090\n",
      "Iter: 885/1000 | Batch 002/017 | Cost: 1.6972\n",
      "Train Loss: 0.301 | Val Loss: 1.166\n",
      "Iter: 886/1000 | Batch 000/017 | Cost: 1.5783\n",
      "Iter: 886/1000 | Batch 002/017 | Cost: 1.2818\n",
      "Train Loss: 0.366 | Val Loss: 0.377\n",
      "Iter: 887/1000 | Batch 000/017 | Cost: 3.5099\n",
      "Iter: 887/1000 | Batch 002/017 | Cost: 1.9585\n",
      "Train Loss: 0.644 | Val Loss: 0.483\n",
      "Iter: 888/1000 | Batch 000/017 | Cost: 1.8480\n",
      "Iter: 888/1000 | Batch 002/017 | Cost: 1.7504\n",
      "Train Loss: 0.398 | Val Loss: 0.626\n",
      "Iter: 889/1000 | Batch 000/017 | Cost: 1.9625\n",
      "Iter: 889/1000 | Batch 002/017 | Cost: 1.2405\n",
      "Train Loss: 0.529 | Val Loss: 0.537\n",
      "Iter: 890/1000 | Batch 000/017 | Cost: 3.0213\n",
      "Iter: 890/1000 | Batch 002/017 | Cost: 2.6865\n",
      "Train Loss: 0.623 | Val Loss: 0.835\n",
      "Iter: 891/1000 | Batch 000/017 | Cost: 2.0816\n",
      "Iter: 891/1000 | Batch 002/017 | Cost: 2.6579\n",
      "Train Loss: 0.253 | Val Loss: 0.183\n",
      "Iter: 892/1000 | Batch 000/017 | Cost: 1.6849\n",
      "Iter: 892/1000 | Batch 002/017 | Cost: 1.9611\n",
      "Train Loss: 0.628 | Val Loss: 0.449\n",
      "Iter: 893/1000 | Batch 000/017 | Cost: 3.5380\n",
      "Iter: 893/1000 | Batch 002/017 | Cost: 3.3693\n",
      "Train Loss: 0.507 | Val Loss: 0.453\n",
      "Iter: 894/1000 | Batch 000/017 | Cost: 1.4189\n",
      "Iter: 894/1000 | Batch 002/017 | Cost: 1.5604\n",
      "Train Loss: 0.855 | Val Loss: 0.602\n",
      "Iter: 895/1000 | Batch 000/017 | Cost: 4.7596\n",
      "Iter: 895/1000 | Batch 002/017 | Cost: 5.2472\n",
      "Train Loss: 0.726 | Val Loss: 0.598\n",
      "Iter: 896/1000 | Batch 000/017 | Cost: 2.4452\n",
      "Iter: 896/1000 | Batch 002/017 | Cost: 1.8163\n",
      "Train Loss: 0.635 | Val Loss: 0.392\n",
      "Iter: 897/1000 | Batch 000/017 | Cost: 2.0481\n",
      "Iter: 897/1000 | Batch 002/017 | Cost: 1.5778\n",
      "Train Loss: 0.508 | Val Loss: 0.467\n",
      "Iter: 898/1000 | Batch 000/017 | Cost: 1.5600\n",
      "Iter: 898/1000 | Batch 002/017 | Cost: 2.0647\n",
      "Train Loss: 0.576 | Val Loss: 0.824\n",
      "Iter: 899/1000 | Batch 000/017 | Cost: 3.7001\n",
      "Iter: 899/1000 | Batch 002/017 | Cost: 3.2004\n",
      "Train Loss: 0.443 | Val Loss: 0.633\n",
      "Iter: 900/1000 | Batch 000/017 | Cost: 2.4193\n",
      "Iter: 900/1000 | Batch 002/017 | Cost: 2.6346\n",
      "Train Loss: 0.559 | Val Loss: 0.595\n",
      "Iter: 901/1000 | Batch 000/017 | Cost: 2.9697\n",
      "Iter: 901/1000 | Batch 002/017 | Cost: 1.5490\n",
      "Train Loss: 0.635 | Val Loss: 0.846\n",
      "Iter: 902/1000 | Batch 000/017 | Cost: 3.5306\n",
      "Iter: 902/1000 | Batch 002/017 | Cost: 2.2457\n",
      "Train Loss: 0.293 | Val Loss: 0.379\n",
      "Iter: 903/1000 | Batch 000/017 | Cost: 3.1006\n",
      "Iter: 903/1000 | Batch 002/017 | Cost: 3.0756\n",
      "Train Loss: 0.681 | Val Loss: 0.681\n",
      "Iter: 904/1000 | Batch 000/017 | Cost: 4.1107\n",
      "Iter: 904/1000 | Batch 002/017 | Cost: 3.8917\n",
      "Train Loss: 0.368 | Val Loss: 1.098\n",
      "Iter: 905/1000 | Batch 000/017 | Cost: 3.5998\n",
      "Iter: 905/1000 | Batch 002/017 | Cost: 3.5746\n",
      "Train Loss: 0.459 | Val Loss: 0.429\n",
      "Iter: 906/1000 | Batch 000/017 | Cost: 3.7769\n",
      "Iter: 906/1000 | Batch 002/017 | Cost: 2.5923\n",
      "Train Loss: 0.469 | Val Loss: 0.567\n",
      "Iter: 907/1000 | Batch 000/017 | Cost: 4.1816\n",
      "Iter: 907/1000 | Batch 002/017 | Cost: 3.5908\n",
      "Train Loss: 0.321 | Val Loss: 0.736\n",
      "Iter: 908/1000 | Batch 000/017 | Cost: 3.0113\n",
      "Iter: 908/1000 | Batch 002/017 | Cost: 4.1096\n",
      "Train Loss: 0.490 | Val Loss: 0.754\n",
      "Iter: 909/1000 | Batch 000/017 | Cost: 2.0157\n",
      "Iter: 909/1000 | Batch 002/017 | Cost: 2.5548\n",
      "Train Loss: 0.426 | Val Loss: 0.816\n",
      "Iter: 910/1000 | Batch 000/017 | Cost: 2.2940\n",
      "Iter: 910/1000 | Batch 002/017 | Cost: 2.5519\n",
      "Train Loss: 0.519 | Val Loss: 0.439\n",
      "Iter: 911/1000 | Batch 000/017 | Cost: 2.4783\n",
      "Iter: 911/1000 | Batch 002/017 | Cost: 2.5734\n",
      "Train Loss: 0.479 | Val Loss: 0.547\n",
      "Iter: 912/1000 | Batch 000/017 | Cost: 1.0759\n",
      "Iter: 912/1000 | Batch 002/017 | Cost: 1.1711\n",
      "Train Loss: 0.276 | Val Loss: 0.195\n",
      "Iter: 913/1000 | Batch 000/017 | Cost: 2.2444\n",
      "Iter: 913/1000 | Batch 002/017 | Cost: 3.3263\n",
      "Train Loss: 0.866 | Val Loss: 0.407\n",
      "Iter: 914/1000 | Batch 000/017 | Cost: 3.8433\n",
      "Iter: 914/1000 | Batch 002/017 | Cost: 3.0464\n",
      "Train Loss: 0.226 | Val Loss: 0.275\n",
      "Iter: 915/1000 | Batch 000/017 | Cost: 2.3628\n",
      "Iter: 915/1000 | Batch 002/017 | Cost: 2.0459\n",
      "Train Loss: 0.511 | Val Loss: 0.765\n",
      "Iter: 916/1000 | Batch 000/017 | Cost: 0.9014\n",
      "Iter: 916/1000 | Batch 002/017 | Cost: 0.9171\n",
      "Train Loss: 0.607 | Val Loss: 0.304\n",
      "Iter: 917/1000 | Batch 000/017 | Cost: 3.7006\n",
      "Iter: 917/1000 | Batch 002/017 | Cost: 4.5466\n",
      "Train Loss: 0.097 | Val Loss: 0.766\n",
      "Iter: 918/1000 | Batch 000/017 | Cost: 3.5590\n",
      "Iter: 918/1000 | Batch 002/017 | Cost: 2.6887\n",
      "Train Loss: 0.573 | Val Loss: 0.435\n",
      "Iter: 919/1000 | Batch 000/017 | Cost: 1.6063\n",
      "Iter: 919/1000 | Batch 002/017 | Cost: 1.7339\n",
      "Train Loss: 0.765 | Val Loss: 0.808\n",
      "Iter: 920/1000 | Batch 000/017 | Cost: 2.6335\n",
      "Iter: 920/1000 | Batch 002/017 | Cost: 2.3269\n",
      "Train Loss: 0.429 | Val Loss: 0.863\n",
      "Iter: 921/1000 | Batch 000/017 | Cost: 3.1977\n",
      "Iter: 921/1000 | Batch 002/017 | Cost: 2.6079\n",
      "Train Loss: 0.679 | Val Loss: 0.668\n",
      "Iter: 922/1000 | Batch 000/017 | Cost: 3.8470\n",
      "Iter: 922/1000 | Batch 002/017 | Cost: 5.2801\n",
      "Train Loss: 0.486 | Val Loss: 0.506\n",
      "Iter: 923/1000 | Batch 000/017 | Cost: 0.4679\n",
      "Iter: 923/1000 | Batch 002/017 | Cost: 2.1906\n",
      "Train Loss: 0.588 | Val Loss: 0.625\n",
      "Iter: 924/1000 | Batch 000/017 | Cost: 2.7170\n",
      "Iter: 924/1000 | Batch 002/017 | Cost: 1.8503\n",
      "Train Loss: 0.253 | Val Loss: 0.497\n",
      "Iter: 925/1000 | Batch 000/017 | Cost: 3.2605\n",
      "Iter: 925/1000 | Batch 002/017 | Cost: 4.2926\n",
      "Train Loss: 1.191 | Val Loss: 0.319\n",
      "Iter: 926/1000 | Batch 000/017 | Cost: 1.8992\n",
      "Iter: 926/1000 | Batch 002/017 | Cost: 0.9743\n",
      "Train Loss: 0.363 | Val Loss: 0.929\n",
      "Iter: 927/1000 | Batch 000/017 | Cost: 2.1775\n",
      "Iter: 927/1000 | Batch 002/017 | Cost: 1.8020\n",
      "Train Loss: 0.370 | Val Loss: 0.750\n",
      "Iter: 928/1000 | Batch 000/017 | Cost: 2.6265\n",
      "Iter: 928/1000 | Batch 002/017 | Cost: 2.7869\n",
      "Train Loss: 0.719 | Val Loss: 0.361\n",
      "Iter: 929/1000 | Batch 000/017 | Cost: 3.5883\n",
      "Iter: 929/1000 | Batch 002/017 | Cost: 4.0818\n",
      "Train Loss: 0.312 | Val Loss: 0.316\n",
      "Iter: 930/1000 | Batch 000/017 | Cost: 3.3057\n",
      "Iter: 930/1000 | Batch 002/017 | Cost: 3.1532\n",
      "Train Loss: 0.256 | Val Loss: 0.881\n",
      "Iter: 931/1000 | Batch 000/017 | Cost: 2.7154\n",
      "Iter: 931/1000 | Batch 002/017 | Cost: 3.7095\n",
      "Train Loss: 0.885 | Val Loss: 0.498\n",
      "Iter: 932/1000 | Batch 000/017 | Cost: 2.1153\n",
      "Iter: 932/1000 | Batch 002/017 | Cost: 2.6194\n",
      "Train Loss: 0.346 | Val Loss: 0.245\n",
      "Iter: 933/1000 | Batch 000/017 | Cost: 1.3302\n",
      "Iter: 933/1000 | Batch 002/017 | Cost: 3.0552\n",
      "Train Loss: 0.377 | Val Loss: 0.486\n",
      "Iter: 934/1000 | Batch 000/017 | Cost: 3.0209\n",
      "Iter: 934/1000 | Batch 002/017 | Cost: 2.9007\n",
      "Train Loss: 0.762 | Val Loss: 0.831\n",
      "Iter: 935/1000 | Batch 000/017 | Cost: 3.0923\n",
      "Iter: 935/1000 | Batch 002/017 | Cost: 2.6368\n",
      "Train Loss: 0.307 | Val Loss: 0.565\n",
      "Iter: 936/1000 | Batch 000/017 | Cost: 2.7356\n",
      "Iter: 936/1000 | Batch 002/017 | Cost: 2.4330\n",
      "Train Loss: 0.798 | Val Loss: 0.651\n",
      "Iter: 937/1000 | Batch 000/017 | Cost: 3.7219\n",
      "Iter: 937/1000 | Batch 002/017 | Cost: 3.5526\n",
      "Train Loss: 0.473 | Val Loss: 0.622\n",
      "Iter: 938/1000 | Batch 000/017 | Cost: 3.2691\n",
      "Iter: 938/1000 | Batch 002/017 | Cost: 2.3408\n",
      "Train Loss: 0.841 | Val Loss: 0.438\n",
      "Iter: 939/1000 | Batch 000/017 | Cost: 2.8780\n",
      "Iter: 939/1000 | Batch 002/017 | Cost: 3.0661\n",
      "Train Loss: 0.951 | Val Loss: 0.251\n",
      "Iter: 940/1000 | Batch 000/017 | Cost: 3.8931\n",
      "Iter: 940/1000 | Batch 002/017 | Cost: 3.4352\n",
      "Train Loss: 0.550 | Val Loss: 0.424\n",
      "Iter: 941/1000 | Batch 000/017 | Cost: 1.3253\n",
      "Iter: 941/1000 | Batch 002/017 | Cost: 2.2564\n",
      "Train Loss: 0.838 | Val Loss: 0.428\n",
      "Iter: 942/1000 | Batch 000/017 | Cost: 3.7123\n",
      "Iter: 942/1000 | Batch 002/017 | Cost: 3.3983\n",
      "Train Loss: 0.612 | Val Loss: 0.593\n",
      "Iter: 943/1000 | Batch 000/017 | Cost: 3.1919\n",
      "Iter: 943/1000 | Batch 002/017 | Cost: 3.2164\n",
      "Train Loss: 0.447 | Val Loss: 0.722\n",
      "Iter: 944/1000 | Batch 000/017 | Cost: 1.3153\n",
      "Iter: 944/1000 | Batch 002/017 | Cost: 1.5428\n",
      "Train Loss: 0.509 | Val Loss: 0.659\n",
      "Iter: 945/1000 | Batch 000/017 | Cost: 1.9127\n",
      "Iter: 945/1000 | Batch 002/017 | Cost: 2.6301\n",
      "Train Loss: 0.374 | Val Loss: 0.409\n",
      "Iter: 946/1000 | Batch 000/017 | Cost: 2.0673\n",
      "Iter: 946/1000 | Batch 002/017 | Cost: 1.5391\n",
      "Train Loss: 0.992 | Val Loss: 0.523\n",
      "Iter: 947/1000 | Batch 000/017 | Cost: 2.5383\n",
      "Iter: 947/1000 | Batch 002/017 | Cost: 2.3186\n",
      "Train Loss: 0.607 | Val Loss: 0.548\n",
      "Iter: 948/1000 | Batch 000/017 | Cost: 2.6667\n",
      "Iter: 948/1000 | Batch 002/017 | Cost: 2.4031\n",
      "Train Loss: 0.341 | Val Loss: 0.826\n",
      "Iter: 949/1000 | Batch 000/017 | Cost: 1.5680\n",
      "Iter: 949/1000 | Batch 002/017 | Cost: 0.9123\n",
      "Train Loss: 0.479 | Val Loss: 0.477\n",
      "Iter: 950/1000 | Batch 000/017 | Cost: 2.8879\n",
      "Iter: 950/1000 | Batch 002/017 | Cost: 2.3054\n",
      "Train Loss: 0.343 | Val Loss: 0.481\n",
      "Iter: 951/1000 | Batch 000/017 | Cost: 2.0185\n",
      "Iter: 951/1000 | Batch 002/017 | Cost: 3.0630\n",
      "Train Loss: 0.483 | Val Loss: 0.918\n",
      "Iter: 952/1000 | Batch 000/017 | Cost: 2.2531\n",
      "Iter: 952/1000 | Batch 002/017 | Cost: 1.5323\n",
      "Train Loss: 0.444 | Val Loss: 0.322\n",
      "Iter: 953/1000 | Batch 000/017 | Cost: 4.7123\n",
      "Iter: 953/1000 | Batch 002/017 | Cost: 3.4845\n",
      "Train Loss: 0.546 | Val Loss: 0.269\n",
      "Iter: 954/1000 | Batch 000/017 | Cost: 1.6488\n",
      "Iter: 954/1000 | Batch 002/017 | Cost: 1.3812\n",
      "Train Loss: 0.558 | Val Loss: 0.613\n",
      "Iter: 955/1000 | Batch 000/017 | Cost: 2.9455\n",
      "Iter: 955/1000 | Batch 002/017 | Cost: 3.5952\n",
      "Train Loss: 0.283 | Val Loss: 0.705\n",
      "Iter: 956/1000 | Batch 000/017 | Cost: 3.8608\n",
      "Iter: 956/1000 | Batch 002/017 | Cost: 2.3762\n",
      "Train Loss: 0.729 | Val Loss: 0.947\n",
      "Iter: 957/1000 | Batch 000/017 | Cost: 4.6464\n",
      "Iter: 957/1000 | Batch 002/017 | Cost: 4.2902\n",
      "Train Loss: 0.425 | Val Loss: 0.543\n",
      "Iter: 958/1000 | Batch 000/017 | Cost: 1.9515\n",
      "Iter: 958/1000 | Batch 002/017 | Cost: 1.3772\n",
      "Train Loss: 0.335 | Val Loss: 0.509\n",
      "Iter: 959/1000 | Batch 000/017 | Cost: 3.7286\n",
      "Iter: 959/1000 | Batch 002/017 | Cost: 3.5437\n",
      "Train Loss: 0.498 | Val Loss: 0.490\n",
      "Iter: 960/1000 | Batch 000/017 | Cost: 4.0713\n",
      "Iter: 960/1000 | Batch 002/017 | Cost: 3.8306\n",
      "Train Loss: 0.692 | Val Loss: 0.796\n",
      "Iter: 961/1000 | Batch 000/017 | Cost: 3.6180\n",
      "Iter: 961/1000 | Batch 002/017 | Cost: 3.3521\n",
      "Train Loss: 0.308 | Val Loss: 0.515\n",
      "Iter: 962/1000 | Batch 000/017 | Cost: 2.3101\n",
      "Iter: 962/1000 | Batch 002/017 | Cost: 2.8979\n",
      "Train Loss: 0.655 | Val Loss: 0.384\n",
      "Iter: 963/1000 | Batch 000/017 | Cost: 2.1553\n",
      "Iter: 963/1000 | Batch 002/017 | Cost: 2.0554\n",
      "Train Loss: 0.546 | Val Loss: 0.196\n",
      "Iter: 964/1000 | Batch 000/017 | Cost: 2.7660\n",
      "Iter: 964/1000 | Batch 002/017 | Cost: 2.8744\n",
      "Train Loss: 0.406 | Val Loss: 0.401\n",
      "Iter: 965/1000 | Batch 000/017 | Cost: 0.9625\n",
      "Iter: 965/1000 | Batch 002/017 | Cost: 1.1648\n",
      "Train Loss: 0.577 | Val Loss: 0.516\n",
      "Iter: 966/1000 | Batch 000/017 | Cost: 1.1026\n",
      "Iter: 966/1000 | Batch 002/017 | Cost: 1.1010\n",
      "Train Loss: 0.817 | Val Loss: 0.972\n",
      "Iter: 967/1000 | Batch 000/017 | Cost: 1.4042\n",
      "Iter: 967/1000 | Batch 002/017 | Cost: 1.4867\n",
      "Train Loss: 0.214 | Val Loss: 0.283\n",
      "Iter: 968/1000 | Batch 000/017 | Cost: 3.3365\n",
      "Iter: 968/1000 | Batch 002/017 | Cost: 3.7227\n",
      "Train Loss: 0.490 | Val Loss: 0.491\n",
      "Iter: 969/1000 | Batch 000/017 | Cost: 2.2294\n",
      "Iter: 969/1000 | Batch 002/017 | Cost: 1.3565\n",
      "Train Loss: 0.495 | Val Loss: 0.757\n",
      "Iter: 970/1000 | Batch 000/017 | Cost: 2.3431\n",
      "Iter: 970/1000 | Batch 002/017 | Cost: 3.1555\n",
      "Train Loss: 0.494 | Val Loss: 0.532\n",
      "Iter: 971/1000 | Batch 000/017 | Cost: 1.7435\n",
      "Iter: 971/1000 | Batch 002/017 | Cost: 1.7583\n",
      "Train Loss: 0.471 | Val Loss: 0.584\n",
      "Iter: 972/1000 | Batch 000/017 | Cost: 2.7309\n",
      "Iter: 972/1000 | Batch 002/017 | Cost: 2.6275\n",
      "Train Loss: 0.402 | Val Loss: 0.406\n",
      "Iter: 973/1000 | Batch 000/017 | Cost: 2.1475\n",
      "Iter: 973/1000 | Batch 002/017 | Cost: 1.8880\n",
      "Train Loss: 0.471 | Val Loss: 0.587\n",
      "Iter: 974/1000 | Batch 000/017 | Cost: 4.0763\n",
      "Iter: 974/1000 | Batch 002/017 | Cost: 4.0771\n",
      "Train Loss: 1.072 | Val Loss: 0.613\n",
      "Iter: 975/1000 | Batch 000/017 | Cost: 3.7093\n",
      "Iter: 975/1000 | Batch 002/017 | Cost: 3.5470\n",
      "Train Loss: 0.452 | Val Loss: 0.356\n",
      "Iter: 976/1000 | Batch 000/017 | Cost: 1.6650\n",
      "Iter: 976/1000 | Batch 002/017 | Cost: 2.4090\n",
      "Train Loss: 0.643 | Val Loss: 0.828\n",
      "Iter: 977/1000 | Batch 000/017 | Cost: 2.9489\n",
      "Iter: 977/1000 | Batch 002/017 | Cost: 2.1838\n",
      "Train Loss: 0.608 | Val Loss: 0.684\n",
      "Iter: 978/1000 | Batch 000/017 | Cost: 1.6579\n",
      "Iter: 978/1000 | Batch 002/017 | Cost: 1.7930\n",
      "Train Loss: 0.507 | Val Loss: 0.440\n",
      "Iter: 979/1000 | Batch 000/017 | Cost: 2.9681\n",
      "Iter: 979/1000 | Batch 002/017 | Cost: 2.7359\n",
      "Train Loss: 0.362 | Val Loss: 0.547\n",
      "Iter: 980/1000 | Batch 000/017 | Cost: 2.6441\n",
      "Iter: 980/1000 | Batch 002/017 | Cost: 2.3069\n",
      "Train Loss: 0.609 | Val Loss: 0.547\n",
      "Iter: 981/1000 | Batch 000/017 | Cost: 2.0052\n",
      "Iter: 981/1000 | Batch 002/017 | Cost: 2.1121\n",
      "Train Loss: 0.829 | Val Loss: 0.447\n",
      "Iter: 982/1000 | Batch 000/017 | Cost: 3.8617\n",
      "Iter: 982/1000 | Batch 002/017 | Cost: 3.1109\n",
      "Train Loss: 0.372 | Val Loss: 0.609\n",
      "Iter: 983/1000 | Batch 000/017 | Cost: 3.2597\n",
      "Iter: 983/1000 | Batch 002/017 | Cost: 2.8605\n",
      "Train Loss: 0.666 | Val Loss: 0.416\n",
      "Iter: 984/1000 | Batch 000/017 | Cost: 3.6261\n",
      "Iter: 984/1000 | Batch 002/017 | Cost: 3.7632\n",
      "Train Loss: 0.686 | Val Loss: 0.736\n",
      "Iter: 985/1000 | Batch 000/017 | Cost: 2.4120\n",
      "Iter: 985/1000 | Batch 002/017 | Cost: 2.5518\n",
      "Train Loss: 0.802 | Val Loss: 0.619\n",
      "Iter: 986/1000 | Batch 000/017 | Cost: 2.4751\n",
      "Iter: 986/1000 | Batch 002/017 | Cost: 2.5679\n",
      "Train Loss: 0.741 | Val Loss: 0.466\n",
      "Iter: 987/1000 | Batch 000/017 | Cost: 3.8615\n",
      "Iter: 987/1000 | Batch 002/017 | Cost: 3.4352\n",
      "Train Loss: 0.910 | Val Loss: 0.560\n",
      "Iter: 988/1000 | Batch 000/017 | Cost: 2.4781\n",
      "Iter: 988/1000 | Batch 002/017 | Cost: 2.2292\n",
      "Train Loss: 0.449 | Val Loss: 0.437\n",
      "Iter: 989/1000 | Batch 000/017 | Cost: 2.7061\n",
      "Iter: 989/1000 | Batch 002/017 | Cost: 1.8872\n",
      "Train Loss: 0.838 | Val Loss: 0.427\n",
      "Iter: 990/1000 | Batch 000/017 | Cost: 1.2136\n",
      "Iter: 990/1000 | Batch 002/017 | Cost: 0.9162\n",
      "Train Loss: 0.665 | Val Loss: 0.943\n",
      "Iter: 991/1000 | Batch 000/017 | Cost: 2.2441\n",
      "Iter: 991/1000 | Batch 002/017 | Cost: 2.8227\n",
      "Train Loss: 0.427 | Val Loss: 0.740\n",
      "Iter: 992/1000 | Batch 000/017 | Cost: 2.4187\n",
      "Iter: 992/1000 | Batch 002/017 | Cost: 2.0813\n",
      "Train Loss: 0.514 | Val Loss: 0.863\n",
      "Iter: 993/1000 | Batch 000/017 | Cost: 3.8326\n",
      "Iter: 993/1000 | Batch 002/017 | Cost: 3.4766\n",
      "Train Loss: 0.512 | Val Loss: 0.938\n",
      "Iter: 994/1000 | Batch 000/017 | Cost: 2.7984\n",
      "Iter: 994/1000 | Batch 002/017 | Cost: 2.7867\n",
      "Train Loss: 0.722 | Val Loss: 0.950\n",
      "Iter: 995/1000 | Batch 000/017 | Cost: 0.6303\n",
      "Iter: 995/1000 | Batch 002/017 | Cost: 1.0492\n",
      "Train Loss: 0.398 | Val Loss: 0.590\n",
      "Iter: 996/1000 | Batch 000/017 | Cost: 5.0493\n",
      "Iter: 996/1000 | Batch 002/017 | Cost: 3.4147\n",
      "Train Loss: 0.604 | Val Loss: 0.511\n",
      "Iter: 997/1000 | Batch 000/017 | Cost: 1.9164\n",
      "Iter: 997/1000 | Batch 002/017 | Cost: 2.0340\n",
      "Train Loss: 0.428 | Val Loss: 0.402\n",
      "Iter: 998/1000 | Batch 000/017 | Cost: 2.7592\n",
      "Iter: 998/1000 | Batch 002/017 | Cost: 3.6677\n",
      "Train Loss: 0.491 | Val Loss: 0.527\n",
      "Iter: 999/1000 | Batch 000/017 | Cost: 2.3489\n",
      "Iter: 999/1000 | Batch 002/017 | Cost: 2.7356\n",
      "Train Loss: 0.961 | Val Loss: 0.287\n",
      "Iter: 1000/1000 | Batch 000/017 | Cost: 3.3946\n",
      "Iter: 1000/1000 | Batch 002/017 | Cost: 2.0039\n",
      "Train Loss: 0.741 | Val Loss: 0.715\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train() # dropout behaves differently in train and eval mode\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size = 4,\n",
    "                              shuffle=True, \n",
    "                              collate_fn = lambda batch: collate_fn_wl3(batch, device))\n",
    "    \n",
    "    val_dataloader = DataLoader(val_dataset, \n",
    "                              batch_size = 4,\n",
    "                              shuffle=True, \n",
    "                              collate_fn = lambda batch: collate_fn_wl3(batch, device))\n",
    "    \n",
    "    all_batch_dicts = get_next_batch(inf_generator(train_dataloader))\n",
    "    for bidx, bd in enumerate(all_batch_dicts):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        imgs = bd[\"observed_data\"].to(device)\n",
    "        true_arrs = bd[\"target\"].reshape(-1, 1).to(device)\n",
    "\n",
    "        pred_arrs = model(imgs)\n",
    "        pred_err = torch.sum(torch.abs(true_arrs - pred_arrs))\n",
    "\n",
    "        pred_err.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if bidx % 2 == 0:\n",
    "            print(\"Iter: %03d/%03d | Batch %03d/%03d | Cost: %.4f\"\n",
    "                  % (epoch+1, N_EPOCHS, bidx,\n",
    "                 len(train_dataloader), pred_err))\n",
    "            \n",
    "            \n",
    "    model.eval()\n",
    "    ep_loss = get_sae(model, train_dataloader)\n",
    "    val_loss = get_sae(model, val_dataloader)\n",
    "    \n",
    "    train_losses.append(ep_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print('Train Loss: %.3f | Val Loss: %.3f'\n",
    "              % (ep_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed2cc2f1-7c00-4048-956a-adaac423ab11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WLCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Conv2d(8, 16, kernel_size=(5, 5), stride=(3, 3), padding=(3, 3))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=16, bias=True)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): ELU(alpha=1.0)\n",
       "    (8): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22981332-e88e-46cb-a11f-4198dc116e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1513b170eb30>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00a4908d-1773-4f4a-95f6-60bcc53d0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d8bc3ef-bafb-4882-841d-5ae483e76658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24177"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ad52435-736b-4a11-baff-920e182d1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dicts = get_next_batch(inf_generator(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2cf2e9c4-17d2-462c-bbf1-cb13200c75b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_gen_train \u001b[38;5;241m=\u001b[39m inf_generator(train_dataloader)\n\u001b[0;32m----> 2\u001b[0m batches_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_gen_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "data_gen_train = inf_generator(train_dataloader)\n",
    "batches_per_epoch = len(data_gen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29909b61-ae73-4c18-9c81-2952d8800d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7c54dd0-46de-4a8f-b10f-7081bc37c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_arrs = test_data_dicts[0][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d37062d5-80ea-4ef8-be06-95f00ddabcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8642],\n",
       "        [ 1.3074],\n",
       "        [-1.2955],\n",
       "        [-1.0729],\n",
       "        [-0.5739],\n",
       "        [ 0.3093],\n",
       "        [-0.0616],\n",
       "        [-0.1415],\n",
       "        [ 0.7623],\n",
       "        [ 1.4990],\n",
       "        [-0.2234],\n",
       "        [-0.1415],\n",
       "        [ 0.9929],\n",
       "        [ 0.9929],\n",
       "        [ 1.5449],\n",
       "        [ 1.1545],\n",
       "        [ 1.2574],\n",
       "        [ 0.0921],\n",
       "        [ 0.1662],\n",
       "        [ 1.3566],\n",
       "        [-0.0616],\n",
       "        [ 0.0162]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91d81398-5096-4be5-8da7-5916bf5aa828",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_arrs_t = pt.inverse_transform(true_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "030d1ee1-7d5f-463a-bb4f-92b1c6afdbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42.00000017],\n",
       "       [73.00000063],\n",
       "       [37.99999988],\n",
       "       [40.00000032],\n",
       "       [44.99999986],\n",
       "       [56.00000001],\n",
       "       [51.00000002],\n",
       "       [50.00000005],\n",
       "       [63.00000009],\n",
       "       [76.9999998 ],\n",
       "       [49.00000008],\n",
       "       [50.00000005],\n",
       "       [66.99999983],\n",
       "       [66.99999983],\n",
       "       [78.0000003 ],\n",
       "       [70.00000053],\n",
       "       [71.99999997],\n",
       "       [52.99999998],\n",
       "       [54.00000007],\n",
       "       [74.00000097],\n",
       "       [51.00000002],\n",
       "       [52.        ]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_arrs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "586549cc-bb54-451c-9fa6-8f0d8b53825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds1 = model(test_data_dicts[0][\"observed_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f440ffd5-e5a9-4782-9db1-aefdff2b0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_t1 = pt.inverse_transform(test_preds1.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "91001b20-42e9-47ed-b5ef-428212ef7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds2 = model(test_data_dicts[1][\"observed_data\"])\n",
    "test_preds_t2 = pt.inverse_transform(test_preds2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "559d489f-a345-4f3d-9eca-0ea07e0da876",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds3 = model(test_data_dicts[2][\"observed_data\"])\n",
    "test_preds_t3 = pt.inverse_transform(test_preds3.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01df2e75-8bb6-490d-94bb-89d730e75f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted arrival of sim (hrs)')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(true_arrs_t, test_preds_t1)\n",
    "plt.scatter(true_arrs_t, test_preds_t2)\n",
    "plt.scatter(true_arrs_t, test_preds_t3)\n",
    "plt.plot(np.linspace(35, 80, 40), np.linspace(35, 80, 40), '--')\n",
    "plt.xlabel(\"True arrival of sim (hrs)\")\n",
    "plt.ylabel(\"Predicted arrival of sim (hrs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "74401937-4f30-42a4-9137-e9695adf412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7017538909701525\n",
      "0.7055397411844198\n",
      "0.6954690908984974\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(true_arrs_t.T, test_preds_t1.T)[1][0])\n",
    "print(np.corrcoef(true_arrs_t.T, test_preds_t2.T)[1][0])\n",
    "print(np.corrcoef(true_arrs_t.T, test_preds_t3.T)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba59ab-0bef-4cbc-802f-30a820977607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
